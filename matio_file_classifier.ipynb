{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import csv\n",
    "import multiprocessing as mp\n",
    "from materials_io.utils.interface import get_available_parsers, get_parser\n",
    "\n",
    "\n",
    "def run_parser(file_parser):\n",
    "    \"\"\"Helper function for multiprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "    file_parser (parser, file): 2-tuple containing a parser name\n",
    "    to use for parsing and a file(s) to parse.\n",
    "    \n",
    "    Returns:\n",
    "    metadata {file_name: {parser_name: metadata_dict}}: Returns\n",
    "    dictionary of metadata with parser and file names included.\n",
    "    \"\"\"\n",
    "    parser = get_parser(file_parser[0])\n",
    "    file = file_parser[1]\n",
    "    \n",
    "    try:\n",
    "        metadata = {file: {file_parser[0]: parser.parse(file)}}\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def run_all_parsers_mp(directory, exclude_parsers=None):\n",
    "    \"\"\"Runs all parsers on a directory but uses multiprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (str): Directory to run parsers on.\n",
    "    exclude_parsers (list): List of parsers to not run.\n",
    "    \n",
    "    Returns:\n",
    "    file_metadata (file_name: {parser_name: metadata_dict}}):\n",
    "    List of dictionaries of metadata as returned by run_parser.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    parsers = get_available_parsers()\n",
    "    file_metadata = []\n",
    "    task_queue = []\n",
    "    \n",
    "    if exclude_parsers is not None:\n",
    "        parsers = list(set(parsers.keys()).difference(exclude_parsers))\n",
    "    \n",
    "    print(\"starting...\")\n",
    "    \n",
    "    for parser in parsers:\n",
    "        parser_obj = get_parser(parser)\n",
    "        \n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            # Generate the full paths\n",
    "            dirs = [os.path.join(root, d) for d in dirs]\n",
    "            files = [os.path.join(root, f) for f in files]\n",
    "            \n",
    "            for group in parser_obj.group(files,dirs):\n",
    "                task_queue.append((parser, group))\n",
    "    \n",
    "    print(\"It took {} seconds to generate the queue. {} jobs in queue\".format(time.time() - start_time,\n",
    "                                                                              len(task_queue)))\n",
    "    print(\"starting job processing...\")\n",
    "    \n",
    "    pools = mp.Pool()\n",
    "    \n",
    "    for metadata in pools.imap_unordered(run_parser, task_queue):\n",
    "        file_metadata.append(metadata)\n",
    "        if (len(file_metadata) % 100) == 0:\n",
    "            print(\"{} out of {} files processed\".format(len(file_metadata), len(task_queue)))\n",
    "            print(\"{} seconds have passed\\n\".format(time.time() - start_time))\n",
    "    \n",
    "    pools.close()\n",
    "    pools.join()\n",
    "    \n",
    "    file_metadata = [metadata for metadata in file_metadata if metadata is not None]\n",
    "    \n",
    "    print(\"Finished in {} seconds\".format(time.time() - start_time))\n",
    "    print(\"{} number of metadata\".format(len(file_metadata)))\n",
    "    return file_metadata\n",
    "\n",
    "def matio_label_gen(directory, label_file=None, exclude_parsers=None):\n",
    "    \"\"\"Generates file metadata using run_all_parsers_mp\n",
    "    and then writes file names and parser names to a .csv for\n",
    "    successfully extracted metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (str): Directory of files to write labels for.\n",
    "    label_file (str): Name of .csv to write labels to.\n",
    "    exclude_parsers (list): List of parsers to not run.\n",
    "    \"\"\"\n",
    "    file_row = []\n",
    "    \n",
    "    if label_file is None:\n",
    "        label_file = os.path.basename(directory) + \".csv\"\n",
    "    \n",
    "    file_metadata = run_all_parsers_mp(directory, exclude_parsers=exclude_parsers)\n",
    "    \n",
    "    for metadata in file_metadata:\n",
    "        file_path = list(metadata.keys())[0]\n",
    "        file_label = list(metadata[file_path].keys())[0]\n",
    "        \n",
    "        if isinstance(file_path, list):\n",
    "            for path in file_path:\n",
    "                file_row.append([path, os.path.getsize(path), file_label])\n",
    "        else:\n",
    "            file_row.append([file_path[0], os.path.getsize(file_path[0]), file_label])\n",
    "    \n",
    "    with open(label_file, 'w', newline='') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow([\"path\", \"size\", \"file_label\"])\n",
    "        \n",
    "        for row in file_row:\n",
    "            csv_writer.writerow(row)\n",
    "    \n",
    "    print(\"Done writing labels\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:stevedore.extension:Could not load 'csv': No module named 'tableschema'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "It took 0.07443904876708984 seconds to generate the queue. 2442 jobs in queue\n",
      "starting job processing...\n",
      "100 out of 2442 files processed\n",
      "3.2220511436462402 seconds have passed\n",
      "\n",
      "200 out of 2442 files processed\n",
      "3.4292941093444824 seconds have passed\n",
      "\n",
      "300 out of 2442 files processed\n",
      "3.571004867553711 seconds have passed\n",
      "\n",
      "400 out of 2442 files processed\n",
      "3.6650571823120117 seconds have passed\n",
      "\n",
      "500 out of 2442 files processed\n",
      "3.7619121074676514 seconds have passed\n",
      "\n",
      "600 out of 2442 files processed\n",
      "3.8659861087799072 seconds have passed\n",
      "\n",
      "700 out of 2442 files processed\n",
      "3.964473247528076 seconds have passed\n",
      "\n",
      "800 out of 2442 files processed\n",
      "5.084309101104736 seconds have passed\n",
      "\n",
      "900 out of 2442 files processed\n",
      "9.18298602104187 seconds have passed\n",
      "\n",
      "1000 out of 2442 files processed\n",
      "249.69861793518066 seconds have passed\n",
      "\n",
      "1100 out of 2442 files processed\n",
      "250.99192810058594 seconds have passed\n",
      "\n",
      "1200 out of 2442 files processed\n",
      "254.94179010391235 seconds have passed\n",
      "\n",
      "1300 out of 2442 files processed\n",
      "510.44212198257446 seconds have passed\n",
      "\n",
      "1400 out of 2442 files processed\n",
      "512.5471930503845 seconds have passed\n",
      "\n",
      "1500 out of 2442 files processed\n",
      "533.7483489513397 seconds have passed\n",
      "\n",
      "1600 out of 2442 files processed\n",
      "762.0110459327698 seconds have passed\n",
      "\n",
      "1700 out of 2442 files processed\n",
      "763.0008509159088 seconds have passed\n",
      "\n",
      "1800 out of 2442 files processed\n",
      "910.5383281707764 seconds have passed\n",
      "\n",
      "1900 out of 2442 files processed\n",
      "912.9264042377472 seconds have passed\n",
      "\n",
      "2000 out of 2442 files processed\n",
      "912.9754700660706 seconds have passed\n",
      "\n",
      "2100 out of 2442 files processed\n",
      "913.0295870304108 seconds have passed\n",
      "\n",
      "2200 out of 2442 files processed\n",
      "913.0744788646698 seconds have passed\n",
      "\n",
      "2300 out of 2442 files processed\n",
      "913.1044449806213 seconds have passed\n",
      "\n",
      "2400 out of 2442 files processed\n",
      "913.1303730010986 seconds have passed\n",
      "\n",
      "Finished in 966.517492055893 seconds\n",
      "268 number of metadata\n",
      "Done writing labels\n"
     ]
    }
   ],
   "source": [
    "matio_label_gen('/Users/ryan/Documents/CS/CDAC/xtract_autoencoder/datasets/nist_dataset'\n",
    "                                , exclude_parsers=['generic', 'noop', 'csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
