{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to grab features from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ktb3NKMBoSTv",
    "outputId": "253ee7d9-48e3-4dd7-e871-7734e573deaa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def feature_from_file(file_path, feature_type=\"head\", byte_num=512): # will add more feature_type later\n",
    "    \"\"\"Retreives features from a file.\n",
    "  \n",
    "    Parameters:\n",
    "    feature_type (str): \"head\" to get bytes from head of the file.\n",
    "    byte_num (int): Number of bytes to grab.\n",
    "    file_path (str): File path of file to get features from.\n",
    "    \n",
    "    Returns:\n",
    "    List of bytes from file_path. \n",
    "    \"\"\"\n",
    "    if feature_type == \"head\":\n",
    "        with open(file_path, 'rb') as f:\n",
    "            byte = f.read(1)\n",
    "            index = 1\n",
    "            features = []\n",
    "            \n",
    "            while byte and index <= byte_num:\n",
    "                features.append(byte)\n",
    "                index += 1\n",
    "                byte = f.read(1)\n",
    "            \n",
    "            if len(features) < byte_num:\n",
    "                features.extend([b'' for i in range(byte_num - len(features))])\n",
    "                \n",
    "            assert len(features) == byte_num\n",
    "            return features\n",
    "    else:\n",
    "        print(\"Invalid feature type\")\n",
    "\n",
    "def feature_from_dir(dir_path, feature_type=\"head\", byte_num=512):\n",
    "    \"\"\"Takes a directory and grabs features from each file.\n",
    "    \n",
    "    Parameters:\n",
    "    dir_path (str): Path of directory to take features from.\n",
    "    feature_type (str): Type of features to get.\n",
    "    byte_num (str): Number of features to take\n",
    "    \n",
    "    Return:\n",
    "    features (list): 2D list of byte_num bytes from each fie in dir_path.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(dir_path):\n",
    "        for filename in filenames:\n",
    "            features.append(feature_from_file(os.path.join(dirpath, filename), feature_type, byte_num))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def translate_bytes(dir_features):\n",
    "    \"\"\"Translates bytes into integers.\n",
    "    \n",
    "    Parameter:\n",
    "    dir_features (list): 2D list of bytes.\n",
    "    \n",
    "    Return:\n",
    "    translated_features (numpy array): dir_features with bytes translated to integers.\n",
    "    \"\"\"\n",
    "    translated_features = np.zeros((len(dir_features), len(dir_features[0])))\n",
    "    \n",
    "    for idx, file_features in enumerate(dir_features):\n",
    "        x = np.array([int.from_bytes(c, byteorder=\"big\") for c in file_features])\n",
    "        translated_features[idx] = x\n",
    "    \n",
    "    return translated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'S', b'O', b'C', b'A', b'T', b' ', b'd', b'a', b't', b'a', b' ', b'r', b'e', b'p', b'o', b'r', b't', b' ', b'c', b'r', b'e', b'a', b't', b'e', b'd', b':', b' ', b'2', b'0', b'1', b'5', b'-', b'1', b'0', b'-', b'2', b'8', b' ', b'1', b'9', b':', b'1', b'4', b' ', b'+', b'0', b'0', b'0', b'0', b'\\n', b'D', b'O', b'I', b' ', b'o', b'f', b' ', b't', b'h', b'e', b' ', b'e', b'n', b't', b'i', b'r', b'e', b' ', b'S', b'O', b'C', b'A', b'T', b' ', b'c', b'o', b'l', b'l', b'e', b'c', b't', b'i', b'o', b'n', b':', b' ', b'1', b'0', b'.', b'1', b'5', b'9', b'4', b'/', b'P', b'A', b'N', b'G', b'A', b'E', b'A', b'.', b'8', b'4', b'9', b'7', b'7', b'0', b'\\n', b' ', b' ', b' ', b' ', b'o', b'r', b' ', b's', b'e', b'e', b':', b' ', b'h', b't', b't', b'p', b':', b'/', b'/', b'd', b'o', b'i', b'.', b'p', b'a', b'n', b'g', b'a', b'e', b'a', b'.', b'd', b'e', b'/', b'1', b'0', b'.', b'1', b'5', b'9', b'4', b'/', b'P', b'A', b'N', b'G', b'A', b'E', b'A', b'.', b'8', b'4', b'9', b'7', b'7', b'0', b'\\n', b'S', b'O', b'C', b'A', b'T', b' ', b'c', b'r', b'u', b'i', b's', b'e', b' ', b'd', b'a', b't', b'a', b' ', b'i', b'n', b' ', b'S', b'O', b'C', b'A', b'T', b' ', b'r', b'e', b'g', b'i', b'o', b'n', b' ', b'\"', b'T', b'r', b'o', b'p', b'i', b'c', b'a', b'l', b' ', b'P', b'a', b'c', b'i', b'f', b'i', b'c', b'\"', b' ', b'f', b'o', b'r', b' ', b't', b'h', b'e', b' ', b'f', b'o', b'l', b'l', b'o', b'w', b'i', b'n', b'g', b' ', b'c', b'r', b'u', b'i', b's', b'e', b's', b':', b'\\n', b'E', b'x', b'p', b'o', b'c', b'o', b'd', b'e', b'\\t', b'v', b'e', b'r', b's', b'i', b'o', b'n', b'\\t', b'C', b'r', b'u', b'i', b's', b'e', b'/', b'D', b'a', b't', b'a', b's', b'e', b't', b' ', b'N', b'a', b'm', b'e', b'\\t', b'S', b'h', b'i', b'p', b'/', b'V', b'e', b's', b's', b'e', b'l', b' ', b'N', b'a', b'm', b'e', b'\\t', b'P', b'I', b'(', b's', b')', b'\\t', b'O', b'r', b'i', b'g', b'i', b'n', b'a', b'l', b' ', b'D', b'a', b't', b'a', b' ', b'D', b'O', b'I', b'\\t', b'O', b'r', b'i', b'g', b'i', b'n', b'a', b'l', b' ', b'D', b'a', b't', b'a', b' ', b'R', b'e', b'f', b'e', b'r', b'e', b'n', b'c', b'e', b'\\t', b'S', b'O', b'C', b'A', b'T', b' ', b'D', b'O', b'I', b'\\t', b'S', b'O', b'C', b'A', b'T', b' ', b'R', b'e', b'f', b'e', b'r', b'e', b'n', b'c', b'e', b'\\t', b'W', b'e', b's', b't', b'm', b'o', b's', b't', b' ', b'L', b'o', b'n', b'g', b'i', b't', b'u', b'd', b'e', b'\\t', b'E', b'a', b's', b't', b'm', b'o', b's', b't', b' ', b'L', b'o', b'n', b'g', b'i', b't', b'u', b'd', b'e', b'\\t', b'S', b'o', b'u', b't', b'h', b'm', b'o', b's', b't', b' ', b'L', b'a', b't', b'i', b't', b'u', b'd', b'e', b'\\t', b'N', b'o', b'r', b't', b'h', b'm', b'o', b's', b't', b' ', b'L', b'a', b't', b'i', b't', b'u', b'd', b'e', b'\\t', b'S', b't', b'a', b'r', b't', b' ', b'T', b'i', b'm', b'e', b'\\t', b'E', b'n', b'd', b' ', b'T', b'i', b'm', b'e', b'\\t', b'Q', b'C', b' ', b'F', b'l', b'a', b'g', b'\\t', b'A', b'd', b'd', b'i', b't', b'i', b'o', b'n', b'a', b'l', b' ', b'M', b'e', b't', b'a', b'd', b'a', b't', b'a', b' ', b'D', b'o', b'c', b'u', b'm', b'e', b'n', b't', b'(', b's', b')', b'\\n', b'0', b'6']\n",
      "[ 83.  79.  67.  65.  84.  32. 100.  97. 116.  97.  32. 114. 101. 112.\n",
      " 111. 114. 116.  32.  99. 114. 101.  97. 116. 101. 100.  58.  32.  50.\n",
      "  48.  49.  53.  45.  49.  48.  45.  50.  56.  32.  49.  57.  58.  49.\n",
      "  52.  32.  43.  48.  48.  48.  48.  10.  68.  79.  73.  32. 111. 102.\n",
      "  32. 116. 104. 101.  32. 101. 110. 116. 105. 114. 101.  32.  83.  79.\n",
      "  67.  65.  84.  32.  99. 111. 108. 108. 101.  99. 116. 105. 111. 110.\n",
      "  58.  32.  49.  48.  46.  49.  53.  57.  52.  47.  80.  65.  78.  71.\n",
      "  65.  69.  65.  46.  56.  52.  57.  55.  55.  48.  10.  32.  32.  32.\n",
      "  32. 111. 114.  32. 115. 101. 101.  58.  32. 104. 116. 116. 112.  58.\n",
      "  47.  47. 100. 111. 105.  46. 112.  97. 110. 103.  97. 101.  97.  46.\n",
      " 100. 101.  47.  49.  48.  46.  49.  53.  57.  52.  47.  80.  65.  78.\n",
      "  71.  65.  69.  65.  46.  56.  52.  57.  55.  55.  48.  10.  83.  79.\n",
      "  67.  65.  84.  32.  99. 114. 117. 105. 115. 101.  32. 100.  97. 116.\n",
      "  97.  32. 105. 110.  32.  83.  79.  67.  65.  84.  32. 114. 101. 103.\n",
      " 105. 111. 110.  32.  34.  84. 114. 111. 112. 105.  99.  97. 108.  32.\n",
      "  80.  97.  99. 105. 102. 105.  99.  34.  32. 102. 111. 114.  32. 116.\n",
      " 104. 101.  32. 102. 111. 108. 108. 111. 119. 105. 110. 103.  32.  99.\n",
      " 114. 117. 105. 115. 101. 115.  58.  10.  69. 120. 112. 111.  99. 111.\n",
      " 100. 101.   9. 118. 101. 114. 115. 105. 111. 110.   9.  67. 114. 117.\n",
      " 105. 115. 101.  47.  68.  97. 116.  97. 115. 101. 116.  32.  78.  97.\n",
      " 109. 101.   9.  83. 104. 105. 112.  47.  86. 101. 115. 115. 101. 108.\n",
      "  32.  78.  97. 109. 101.   9.  80.  73.  40. 115.  41.   9.  79. 114.\n",
      " 105. 103. 105. 110.  97. 108.  32.  68.  97. 116.  97.  32.  68.  79.\n",
      "  73.   9.  79. 114. 105. 103. 105. 110.  97. 108.  32.  68.  97. 116.\n",
      "  97.  32.  82. 101. 102. 101. 114. 101. 110.  99. 101.   9.  83.  79.\n",
      "  67.  65.  84.  32.  68.  79.  73.   9.  83.  79.  67.  65.  84.  32.\n",
      "  82. 101. 102. 101. 114. 101. 110.  99. 101.   9.  87. 101. 115. 116.\n",
      " 109. 111. 115. 116.  32.  76. 111. 110. 103. 105. 116. 117. 100. 101.\n",
      "   9.  69.  97. 115. 116. 109. 111. 115. 116.  32.  76. 111. 110. 103.\n",
      " 105. 116. 117. 100. 101.   9.  83. 111. 117. 116. 104. 109. 111. 115.\n",
      " 116.  32.  76.  97. 116. 105. 116. 117. 100. 101.   9.  78. 111. 114.\n",
      " 116. 104. 109. 111. 115. 116.  32.  76.  97. 116. 105. 116. 117. 100.\n",
      " 101.   9.  83. 116.  97. 114. 116.  32.  84. 105. 109. 101.   9.  69.\n",
      " 110. 100.  32.  84. 105. 109. 101.   9.  81.  67.  32.  70. 108.  97.\n",
      " 103.   9.  65. 100. 100. 105. 116. 105. 111. 110.  97. 108.  32.  77.\n",
      " 101. 116.  97. 100.  97. 116.  97.  32.  68. 111.  99. 117. 109. 101.\n",
      " 110. 116.  40. 115.  41.  10.  48.  54.]\n",
      "[0.3254902  0.30980392 0.2627451  0.25490196 0.32941176 0.1254902\n",
      " 0.39215686 0.38039216 0.45490196 0.38039216 0.1254902  0.44705882\n",
      " 0.39607843 0.43921569 0.43529412 0.44705882 0.45490196 0.1254902\n",
      " 0.38823529 0.44705882 0.39607843 0.38039216 0.45490196 0.39607843\n",
      " 0.39215686 0.22745098 0.1254902  0.19607843 0.18823529 0.19215686\n",
      " 0.20784314 0.17647059 0.19215686 0.18823529 0.17647059 0.19607843\n",
      " 0.21960784 0.1254902  0.19215686 0.22352941 0.22745098 0.19215686\n",
      " 0.20392157 0.1254902  0.16862745 0.18823529 0.18823529 0.18823529\n",
      " 0.18823529 0.03921569 0.26666667 0.30980392 0.28627451 0.1254902\n",
      " 0.43529412 0.4        0.1254902  0.45490196 0.40784314 0.39607843\n",
      " 0.1254902  0.39607843 0.43137255 0.45490196 0.41176471 0.44705882\n",
      " 0.39607843 0.1254902  0.3254902  0.30980392 0.2627451  0.25490196\n",
      " 0.32941176 0.1254902  0.38823529 0.43529412 0.42352941 0.42352941\n",
      " 0.39607843 0.38823529 0.45490196 0.41176471 0.43529412 0.43137255\n",
      " 0.22745098 0.1254902  0.19215686 0.18823529 0.18039216 0.19215686\n",
      " 0.20784314 0.22352941 0.20392157 0.18431373 0.31372549 0.25490196\n",
      " 0.30588235 0.27843137 0.25490196 0.27058824 0.25490196 0.18039216\n",
      " 0.21960784 0.20392157 0.22352941 0.21568627 0.21568627 0.18823529\n",
      " 0.03921569 0.1254902  0.1254902  0.1254902  0.1254902  0.43529412\n",
      " 0.44705882 0.1254902  0.45098039 0.39607843 0.39607843 0.22745098\n",
      " 0.1254902  0.40784314 0.45490196 0.45490196 0.43921569 0.22745098\n",
      " 0.18431373 0.18431373 0.39215686 0.43529412 0.41176471 0.18039216\n",
      " 0.43921569 0.38039216 0.43137255 0.40392157 0.38039216 0.39607843\n",
      " 0.38039216 0.18039216 0.39215686 0.39607843 0.18431373 0.19215686\n",
      " 0.18823529 0.18039216 0.19215686 0.20784314 0.22352941 0.20392157\n",
      " 0.18431373 0.31372549 0.25490196 0.30588235 0.27843137 0.25490196\n",
      " 0.27058824 0.25490196 0.18039216 0.21960784 0.20392157 0.22352941\n",
      " 0.21568627 0.21568627 0.18823529 0.03921569 0.3254902  0.30980392\n",
      " 0.2627451  0.25490196 0.32941176 0.1254902  0.38823529 0.44705882\n",
      " 0.45882353 0.41176471 0.45098039 0.39607843 0.1254902  0.39215686\n",
      " 0.38039216 0.45490196 0.38039216 0.1254902  0.41176471 0.43137255\n",
      " 0.1254902  0.3254902  0.30980392 0.2627451  0.25490196 0.32941176\n",
      " 0.1254902  0.44705882 0.39607843 0.40392157 0.41176471 0.43529412\n",
      " 0.43137255 0.1254902  0.13333333 0.32941176 0.44705882 0.43529412\n",
      " 0.43921569 0.41176471 0.38823529 0.38039216 0.42352941 0.1254902\n",
      " 0.31372549 0.38039216 0.38823529 0.41176471 0.4        0.41176471\n",
      " 0.38823529 0.13333333 0.1254902  0.4        0.43529412 0.44705882\n",
      " 0.1254902  0.45490196 0.40784314 0.39607843 0.1254902  0.4\n",
      " 0.43529412 0.42352941 0.42352941 0.43529412 0.46666667 0.41176471\n",
      " 0.43137255 0.40392157 0.1254902  0.38823529 0.44705882 0.45882353\n",
      " 0.41176471 0.45098039 0.39607843 0.45098039 0.22745098 0.03921569\n",
      " 0.27058824 0.47058824 0.43921569 0.43529412 0.38823529 0.43529412\n",
      " 0.39215686 0.39607843 0.03529412 0.4627451  0.39607843 0.44705882\n",
      " 0.45098039 0.41176471 0.43529412 0.43137255 0.03529412 0.2627451\n",
      " 0.44705882 0.45882353 0.41176471 0.45098039 0.39607843 0.18431373\n",
      " 0.26666667 0.38039216 0.45490196 0.38039216 0.45098039 0.39607843\n",
      " 0.45490196 0.1254902  0.30588235 0.38039216 0.42745098 0.39607843\n",
      " 0.03529412 0.3254902  0.40784314 0.41176471 0.43921569 0.18431373\n",
      " 0.3372549  0.39607843 0.45098039 0.45098039 0.39607843 0.42352941\n",
      " 0.1254902  0.30588235 0.38039216 0.42745098 0.39607843 0.03529412\n",
      " 0.31372549 0.28627451 0.15686275 0.45098039 0.16078431 0.03529412\n",
      " 0.30980392 0.44705882 0.41176471 0.40392157 0.41176471 0.43137255\n",
      " 0.38039216 0.42352941 0.1254902  0.26666667 0.38039216 0.45490196\n",
      " 0.38039216 0.1254902  0.26666667 0.30980392 0.28627451 0.03529412\n",
      " 0.30980392 0.44705882 0.41176471 0.40392157 0.41176471 0.43137255\n",
      " 0.38039216 0.42352941 0.1254902  0.26666667 0.38039216 0.45490196\n",
      " 0.38039216 0.1254902  0.32156863 0.39607843 0.4        0.39607843\n",
      " 0.44705882 0.39607843 0.43137255 0.38823529 0.39607843 0.03529412\n",
      " 0.3254902  0.30980392 0.2627451  0.25490196 0.32941176 0.1254902\n",
      " 0.26666667 0.30980392 0.28627451 0.03529412 0.3254902  0.30980392\n",
      " 0.2627451  0.25490196 0.32941176 0.1254902  0.32156863 0.39607843\n",
      " 0.4        0.39607843 0.44705882 0.39607843 0.43137255 0.38823529\n",
      " 0.39607843 0.03529412 0.34117647 0.39607843 0.45098039 0.45490196\n",
      " 0.42745098 0.43529412 0.45098039 0.45490196 0.1254902  0.29803922\n",
      " 0.43529412 0.43137255 0.40392157 0.41176471 0.45490196 0.45882353\n",
      " 0.39215686 0.39607843 0.03529412 0.27058824 0.38039216 0.45098039\n",
      " 0.45490196 0.42745098 0.43529412 0.45098039 0.45490196 0.1254902\n",
      " 0.29803922 0.43529412 0.43137255 0.40392157 0.41176471 0.45490196\n",
      " 0.45882353 0.39215686 0.39607843 0.03529412 0.3254902  0.43529412\n",
      " 0.45882353 0.45490196 0.40784314 0.42745098 0.43529412 0.45098039\n",
      " 0.45490196 0.1254902  0.29803922 0.38039216 0.45490196 0.41176471\n",
      " 0.45490196 0.45882353 0.39215686 0.39607843 0.03529412 0.30588235\n",
      " 0.43529412 0.44705882 0.45490196 0.40784314 0.42745098 0.43529412\n",
      " 0.45098039 0.45490196 0.1254902  0.29803922 0.38039216 0.45490196\n",
      " 0.41176471 0.45490196 0.45882353 0.39215686 0.39607843 0.03529412\n",
      " 0.3254902  0.45490196 0.38039216 0.44705882 0.45490196 0.1254902\n",
      " 0.32941176 0.41176471 0.42745098 0.39607843 0.03529412 0.27058824\n",
      " 0.43137255 0.39215686 0.1254902  0.32941176 0.41176471 0.42745098\n",
      " 0.39607843 0.03529412 0.31764706 0.2627451  0.1254902  0.2745098\n",
      " 0.42352941 0.38039216 0.40392157 0.03529412 0.25490196 0.39215686\n",
      " 0.39215686 0.41176471 0.45490196 0.41176471 0.43529412 0.43137255\n",
      " 0.38039216 0.42352941 0.1254902  0.30196078 0.39607843 0.45490196\n",
      " 0.38039216 0.39215686 0.38039216 0.45490196 0.38039216 0.1254902\n",
      " 0.26666667 0.43529412 0.38823529 0.45882353 0.42745098 0.39607843\n",
      " 0.43137255 0.45490196 0.15686275 0.45098039 0.16078431 0.03921569\n",
      " 0.18823529 0.21176471]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_set_dir = '/Users/ryan/Documents/CS/CDAC/official_xtract/sampler_dataset/pub8'\n",
    "\n",
    "raw_features = feature_from_dir(test_set_dir, byte_num=512)\n",
    "untranslated_features = translate_bytes(raw_features)\n",
    "x = untranslated_features / 255\n",
    "\n",
    "x_train, x_test, _, _ = train_test_split(x, x)\n",
    "\n",
    "print(raw_features[0])\n",
    "print(untranslated_features[0])\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adadelta, Adam\n",
    "sgd_optimizer = SGD(lr=1, decay=0.001)\n",
    "adadelta_optimizer = Adadelta(lr=0.1, decay=0)\n",
    "adam_optimizer = Adam(lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 512)               33280     \n",
      "=================================================================\n",
      "Total params: 70,304\n",
      "Trainable params: 70,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4791 samples, validate on 1597 samples\n",
      "Epoch 1/100\n",
      "4791/4791 [==============================] - 1s 185us/step - loss: -716.1343 - acc: 0.0166 - val_loss: -1024.1845 - val_acc: 0.0095\n",
      "Epoch 2/100\n",
      "4791/4791 [==============================] - 0s 30us/step - loss: -1102.5723 - acc: 0.0075 - val_loss: -1133.6720 - val_acc: 0.0068\n",
      "Epoch 3/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1168.0996 - acc: 0.0062 - val_loss: -1169.3206 - val_acc: 0.0063\n",
      "Epoch 4/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1189.5649 - acc: 0.0059 - val_loss: -1175.2795 - val_acc: 0.0061\n",
      "Epoch 5/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1197.0349 - acc: 0.0057 - val_loss: -1179.8692 - val_acc: 0.0060\n",
      "Epoch 6/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1197.3997 - acc: 0.0057 - val_loss: -1179.8728 - val_acc: 0.0060\n",
      "Epoch 7/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1197.7496 - acc: 0.0057 - val_loss: -1185.1049 - val_acc: 0.0059\n",
      "Epoch 8/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1203.4134 - acc: 0.0057 - val_loss: -1190.4022 - val_acc: 0.0059\n",
      "Epoch 9/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1207.8643 - acc: 0.0056 - val_loss: -1190.4050 - val_acc: 0.0059\n",
      "Epoch 10/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1207.8692 - acc: 0.0057 - val_loss: -1190.4377 - val_acc: 0.0060\n",
      "Epoch 11/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.0057 - acc: 0.0056 - val_loss: -1194.8134 - val_acc: 0.0057\n",
      "Epoch 12/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.3893 - acc: 0.0056 - val_loss: -1194.8243 - val_acc: 0.0059\n",
      "Epoch 13/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.3989 - acc: 0.0058 - val_loss: -1194.8289 - val_acc: 0.0061\n",
      "Epoch 14/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.4030 - acc: 0.0063 - val_loss: -1194.8392 - val_acc: 0.0068\n",
      "Epoch 15/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.4146 - acc: 0.0073 - val_loss: -1194.8474 - val_acc: 0.0083\n",
      "Epoch 16/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.4317 - acc: 0.0085 - val_loss: -1194.8608 - val_acc: 0.0092\n",
      "Epoch 17/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.4469 - acc: 0.0098 - val_loss: -1194.8740 - val_acc: 0.0102\n",
      "Epoch 18/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.4605 - acc: 0.0109 - val_loss: -1194.8857 - val_acc: 0.0111\n",
      "Epoch 19/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.4774 - acc: 0.0121 - val_loss: -1194.8951 - val_acc: 0.0122\n",
      "Epoch 20/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.4910 - acc: 0.0132 - val_loss: -1194.9093 - val_acc: 0.0130\n",
      "Epoch 21/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.4999 - acc: 0.0139 - val_loss: -1194.9208 - val_acc: 0.0138\n",
      "Epoch 22/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.5097 - acc: 0.0146 - val_loss: -1194.9204 - val_acc: 0.0145\n",
      "Epoch 23/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.5172 - acc: 0.0152 - val_loss: -1194.9290 - val_acc: 0.0145\n",
      "Epoch 24/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1212.5173 - acc: 0.0155 - val_loss: -1194.9343 - val_acc: 0.0145\n",
      "Epoch 25/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1212.5222 - acc: 0.0156 - val_loss: -1194.9310 - val_acc: 0.0155\n",
      "Epoch 26/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1212.5268 - acc: 0.0160 - val_loss: -1194.9418 - val_acc: 0.0154\n",
      "Epoch 27/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1212.5339 - acc: 0.0165 - val_loss: -1194.9409 - val_acc: 0.0158\n",
      "Epoch 28/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.5425 - acc: 0.0169 - val_loss: -1194.9565 - val_acc: 0.0166\n",
      "Epoch 29/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.5553 - acc: 0.0179 - val_loss: -1194.9720 - val_acc: 0.0171\n",
      "Epoch 30/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.5564 - acc: 0.0179 - val_loss: -1194.9984 - val_acc: 0.0184\n",
      "Epoch 31/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1212.5918 - acc: 0.0201 - val_loss: -1195.0201 - val_acc: 0.0202\n",
      "Epoch 32/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.6124 - acc: 0.0214 - val_loss: -1195.0336 - val_acc: 0.0219\n",
      "Epoch 33/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.6271 - acc: 0.0225 - val_loss: -1195.0393 - val_acc: 0.0226\n",
      "Epoch 34/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1212.6345 - acc: 0.0230 - val_loss: -1195.0410 - val_acc: 0.0228\n",
      "Epoch 35/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1212.6406 - acc: 0.0234 - val_loss: -1195.0411 - val_acc: 0.0233\n",
      "Epoch 36/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.6460 - acc: 0.0238 - val_loss: -1195.0497 - val_acc: 0.0236\n",
      "Epoch 37/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.6502 - acc: 0.0241 - val_loss: -1195.0639 - val_acc: 0.0233\n",
      "Epoch 38/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.6493 - acc: 0.0241 - val_loss: -1195.0581 - val_acc: 0.0239\n",
      "Epoch 39/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.6538 - acc: 0.0243 - val_loss: -1195.0510 - val_acc: 0.0240\n",
      "Epoch 40/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.6559 - acc: 0.0246 - val_loss: -1195.0515 - val_acc: 0.0243\n",
      "Epoch 41/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.6602 - acc: 0.0248 - val_loss: -1195.0532 - val_acc: 0.0246\n",
      "Epoch 42/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1212.6632 - acc: 0.0250 - val_loss: -1195.0786 - val_acc: 0.0247\n",
      "Epoch 43/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.6671 - acc: 0.0253 - val_loss: -1195.0614 - val_acc: 0.0251\n",
      "Epoch 44/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.6702 - acc: 0.0255 - val_loss: -1195.0779 - val_acc: 0.0249\n",
      "Epoch 45/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1212.6712 - acc: 0.0257 - val_loss: -1195.0770 - val_acc: 0.0251\n",
      "Epoch 46/100\n",
      "4791/4791 [==============================] - 0s 29us/step - loss: -1212.6738 - acc: 0.0258 - val_loss: -1195.0691 - val_acc: 0.0255\n",
      "Epoch 47/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.6810 - acc: 0.0262 - val_loss: -1195.0796 - val_acc: 0.0258\n",
      "Epoch 48/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.6870 - acc: 0.0265 - val_loss: -1195.0454 - val_acc: 0.0265\n",
      "Epoch 49/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.6919 - acc: 0.0269 - val_loss: -1195.0236 - val_acc: 0.0268\n",
      "Epoch 50/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.6837 - acc: 0.0264 - val_loss: -1195.0899 - val_acc: 0.0256\n",
      "Epoch 51/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.6885 - acc: 0.0267 - val_loss: -1195.0408 - val_acc: 0.0269\n",
      "Epoch 52/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.6952 - acc: 0.0271 - val_loss: -1195.0733 - val_acc: 0.0272\n",
      "Epoch 53/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.6987 - acc: 0.0274 - val_loss: -1195.1036 - val_acc: 0.0271\n",
      "Epoch 54/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.6991 - acc: 0.0274 - val_loss: -1195.1046 - val_acc: 0.0272\n",
      "Epoch 55/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7007 - acc: 0.0275 - val_loss: -1195.0993 - val_acc: 0.0272\n",
      "Epoch 56/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7037 - acc: 0.0277 - val_loss: -1195.0635 - val_acc: 0.0278\n",
      "Epoch 57/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7041 - acc: 0.0277 - val_loss: -1195.0319 - val_acc: 0.0279\n",
      "Epoch 58/100\n",
      "4791/4791 [==============================] - 0s 28us/step - loss: -1212.7030 - acc: 0.0277 - val_loss: -1195.0874 - val_acc: 0.0270\n",
      "Epoch 59/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.6984 - acc: 0.0273 - val_loss: -1195.0869 - val_acc: 0.0276\n",
      "Epoch 60/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7035 - acc: 0.0278 - val_loss: -1195.0876 - val_acc: 0.0276\n",
      "Epoch 61/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7051 - acc: 0.0279 - val_loss: -1195.0558 - val_acc: 0.0281\n",
      "Epoch 62/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7052 - acc: 0.0279 - val_loss: -1195.0812 - val_acc: 0.0280\n",
      "Epoch 63/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7063 - acc: 0.0279 - val_loss: -1195.0517 - val_acc: 0.0281\n",
      "Epoch 64/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7073 - acc: 0.0280 - val_loss: -1195.0817 - val_acc: 0.0281\n",
      "Epoch 65/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.7074 - acc: 0.0280 - val_loss: -1195.1143 - val_acc: 0.0272\n",
      "Epoch 66/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.7064 - acc: 0.0279 - val_loss: -1195.0964 - val_acc: 0.0279\n",
      "Epoch 67/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7088 - acc: 0.0279 - val_loss: -1195.0871 - val_acc: 0.0282\n",
      "Epoch 68/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7123 - acc: 0.0284 - val_loss: -1195.0886 - val_acc: 0.0285\n",
      "Epoch 69/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7145 - acc: 0.0285 - val_loss: -1195.0964 - val_acc: 0.0283\n",
      "Epoch 70/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.7099 - acc: 0.0281 - val_loss: -1195.1038 - val_acc: 0.0278\n",
      "Epoch 71/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.7094 - acc: 0.0281 - val_loss: -1195.1169 - val_acc: 0.0274\n",
      "Epoch 72/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7107 - acc: 0.0282 - val_loss: -1195.0947 - val_acc: 0.0287\n",
      "Epoch 73/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7171 - acc: 0.0287 - val_loss: -1195.0981 - val_acc: 0.0290\n",
      "Epoch 74/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7223 - acc: 0.0289 - val_loss: -1195.0923 - val_acc: 0.0292\n",
      "Epoch 75/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7194 - acc: 0.0289 - val_loss: -1195.1035 - val_acc: 0.0263\n",
      "Epoch 76/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7024 - acc: 0.0276 - val_loss: -1195.1092 - val_acc: 0.0284\n",
      "Epoch 77/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7187 - acc: 0.0289 - val_loss: -1195.1270 - val_acc: 0.0288\n",
      "Epoch 78/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.7243 - acc: 0.0292 - val_loss: -1195.1345 - val_acc: 0.0285\n",
      "Epoch 79/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.7281 - acc: 0.0293 - val_loss: -1195.1144 - val_acc: 0.0298\n",
      "Epoch 80/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7335 - acc: 0.0296 - val_loss: -1195.0524 - val_acc: 0.0302\n",
      "Epoch 81/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7352 - acc: 0.0298 - val_loss: -1195.1449 - val_acc: 0.0295\n",
      "Epoch 82/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7337 - acc: 0.0297 - val_loss: -1195.1446 - val_acc: 0.0300\n",
      "Epoch 83/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7396 - acc: 0.0300 - val_loss: -1195.1321 - val_acc: 0.0300\n",
      "Epoch 84/100\n",
      "4791/4791 [==============================] - 0s 27us/step - loss: -1212.7417 - acc: 0.0303 - val_loss: -1195.0698 - val_acc: 0.0304\n",
      "Epoch 85/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7374 - acc: 0.0300 - val_loss: -1195.1254 - val_acc: 0.0299\n",
      "Epoch 86/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7386 - acc: 0.0299 - val_loss: -1195.1172 - val_acc: 0.0303\n",
      "Epoch 87/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7396 - acc: 0.0300 - val_loss: -1195.1160 - val_acc: 0.0303\n",
      "Epoch 88/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7304 - acc: 0.0295 - val_loss: -1195.1086 - val_acc: 0.0299\n",
      "Epoch 89/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7352 - acc: 0.0299 - val_loss: -1195.1326 - val_acc: 0.0299\n",
      "Epoch 90/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7338 - acc: 0.0297 - val_loss: -1195.1296 - val_acc: 0.0291\n",
      "Epoch 91/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7401 - acc: 0.0302 - val_loss: -1195.1073 - val_acc: 0.0304\n",
      "Epoch 92/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7439 - acc: 0.0304 - val_loss: -1195.1017 - val_acc: 0.0306\n",
      "Epoch 93/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7487 - acc: 0.0307 - val_loss: -1195.1239 - val_acc: 0.0307\n",
      "Epoch 94/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7509 - acc: 0.0308 - val_loss: -1195.0914 - val_acc: 0.0312\n",
      "Epoch 95/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7534 - acc: 0.0310 - val_loss: -1195.1019 - val_acc: 0.0311\n",
      "Epoch 96/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7331 - acc: 0.0299 - val_loss: -1195.1351 - val_acc: 0.0291\n",
      "Epoch 97/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7300 - acc: 0.0301 - val_loss: -1195.1537 - val_acc: 0.0291\n",
      "Epoch 98/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7184 - acc: 0.0285 - val_loss: -1195.1182 - val_acc: 0.0311\n",
      "Epoch 99/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7305 - acc: 0.0296 - val_loss: -1195.1536 - val_acc: 0.0292\n",
      "Epoch 100/100\n",
      "4791/4791 [==============================] - 0s 26us/step - loss: -1212.7454 - acc: 0.0306 - val_loss: -1195.1407 - val_acc: 0.0306\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "input_size = len(x_train[0])\n",
    "\n",
    "input_layer = Input((input_size,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense (64, activation='relu')(encoded)\n",
    "decoded = Dense(input_size, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "autoencoder.summary()\n",
    "\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                          epochs=100,\n",
    "                          batch_size = 256,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(x_test, x_test))\n",
    "\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "print(K.eval(autoencoder.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "[[-0.05084571  0.06973114 -0.01266051 ... -0.08306095 -0.01116333\n",
      "   0.00319867]\n",
      " [-0.05438119  0.03074812 -0.01937459 ... -0.0525082   0.11125287\n",
      "   0.07257906]\n",
      " [-0.06934612 -0.00385563 -0.03257179 ...  0.10350678 -0.11210641\n",
      "  -0.02891172]\n",
      " ...\n",
      " [ 0.07144225 -0.08722432 -0.02462004 ...  0.04178799  0.01336034\n",
      "   0.03918462]\n",
      " [ 0.01879815 -0.08734938 -0.06883378 ...  0.03033322  0.12023733\n",
      "   0.0033702 ]\n",
      " [-0.01016326 -0.03272462 -0.0011365  ... -0.01624553 -0.0642002\n",
      "   0.07813853]]\n",
      "LAYER 1\n",
      "[ 0.00387681 -0.03795798 -0.05553212 -0.07462566 -0.02857636 -0.00893215\n",
      "  0.15776084 -0.0478286   0.1832785   0.23961893  0.00086467  0.2561634\n",
      "  0.5707312   0.24810919  0.12568723  0.01300425  0.5469935   0.0069351\n",
      "  0.16038176  0.04704882  0.01310134  0.24956703  0.26172742  0.07942128\n",
      "  0.02002666  0.16884068 -0.00906265 -0.07353037 -0.01124712  0.0209683\n",
      "  0.49028102  0.21769533  0.11121484 -0.00531383  0.26239717 -0.07142499\n",
      "  0.40984985  0.25870073  0.0601779  -0.14756127  0.5197484  -0.06170607\n",
      "  0.27282462 -0.01625904  0.1409375   0.05125351  0.31381974 -0.01698165\n",
      " -0.02511681  0.09333891 -0.01558733 -0.01829918 -0.07940643  0.00676376\n",
      "  0.1632294   0.00492341  0.09375557  0.3835588   0.3410755   0.43822157\n",
      " -0.08539071  0.36801782 -0.10566034  0.06241499]\n",
      "LAYER 2\n",
      "[[ 0.06188938  0.00870247 -0.16597159 ...  0.14729586  0.2154997\n",
      "   0.03120168]\n",
      " [-0.09667945 -0.2213213   0.11602205 ... -0.01231503 -0.02799111\n",
      "   0.17843902]\n",
      " [-0.01003294  0.04868323  0.18718682 ... -0.20747401 -0.0025854\n",
      "   0.04299967]\n",
      " ...\n",
      " [-0.17676392  0.1942943   0.10524245 ...  0.23873582 -0.13567084\n",
      "  -0.15372626]\n",
      " [ 0.20585915  0.03502514  0.11960523 ... -0.11988246 -0.2148478\n",
      "   0.182133  ]\n",
      " [-0.19926538  0.22056091  0.05530072 ... -0.1764061   0.06630382\n",
      "  -0.24036452]]\n",
      "LAYER 3\n",
      "[-6.33289739e-02  5.16932085e-02 -5.60253800e-04  5.47767937e-01\n",
      "  1.00646809e-01 -2.59999614e-02  2.79736042e-01 -2.44930573e-02\n",
      "  6.54573321e-01 -6.16788529e-02  2.67196982e-03  2.79740036e-01\n",
      " -2.09873319e-02  3.39425951e-01  3.01501364e-01  2.27603989e-04\n",
      "  1.34195521e-01 -9.35046934e-03  5.37074953e-02  2.48706993e-02\n",
      " -1.53451608e-02  1.37721136e-01 -2.37036007e-03 -9.67517048e-02\n",
      "  1.00276634e-01  3.21320817e-02 -8.61839205e-03 -1.90418493e-02\n",
      "  5.00073582e-02  1.75421745e-01 -9.42338631e-03 -2.67748628e-03]\n",
      "LAYER 4\n",
      "[[-0.18174097 -0.14407568  0.11628331 ...  0.02829647 -0.08553318\n",
      "  -0.03432466]\n",
      " [-0.14809948  0.21491103  0.1491653  ... -0.2021808   0.12740573\n",
      "   0.14039709]\n",
      " [ 0.10917543  0.24361804 -0.2290451  ... -0.20449951  0.09238905\n",
      "  -0.19823354]\n",
      " ...\n",
      " [-0.1021383   0.22649941 -0.02320319 ...  0.2253534   0.00209851\n",
      "   0.29042718]\n",
      " [-0.11585225 -0.0624074   0.24091345 ...  0.19535704 -0.07750373\n",
      "  -0.03691552]\n",
      " [ 0.01695742 -0.02193121  0.19204004 ...  0.15549852  0.21723534\n",
      "  -0.03251778]]\n",
      "LAYER 5\n",
      "[ 2.90821102e-02 -6.63364977e-02  1.58884541e-06  1.23962022e-01\n",
      "  9.70220491e-02  5.05522185e-04  2.20046204e-05  1.17730908e-01\n",
      "  1.09526813e-01  9.55947023e-03 -3.18873078e-02  9.04822722e-02\n",
      "  1.60490438e-01 -1.61347128e-02  7.16758380e-03 -1.14808281e-04\n",
      "  7.94292311e-04 -4.02776897e-02 -1.88037544e-03  0.00000000e+00\n",
      "  1.61624685e-05  2.19139829e-01  2.04859693e-02 -1.45674087e-02\n",
      "  1.45269513e-01 -5.10712266e-02 -4.43259217e-02 -2.30962634e-02\n",
      "  2.76860744e-02 -2.27991797e-04 -1.18808020e-02 -3.79437394e-02\n",
      "  3.33029311e-04  1.96099669e-01 -3.01794428e-03  1.05030224e-01\n",
      " -1.11885704e-02 -7.48848077e-03  2.35430062e-01 -4.48329462e-04\n",
      " -1.62354726e-02  5.40611185e-02  9.15227458e-02 -3.48446406e-02\n",
      "  7.04860166e-02  5.64459013e-03  2.60681361e-01  1.15482137e-02\n",
      " -4.23270722e-05  7.80133605e-02 -3.41880089e-03  1.55892611e-01\n",
      "  1.11686468e-01 -3.03109759e-04 -3.99828330e-03  3.72628793e-02\n",
      " -1.12614629e-03  1.09664291e-01  1.28910989e-01  6.94633322e-03\n",
      "  4.54733633e-02 -1.99004146e-03 -1.30334753e-03  1.46631077e-01]\n",
      "LAYER 6\n",
      "[[-0.05908047  0.0101768   0.07728743 ...  0.07955012 -0.09551702\n",
      "  -0.02761137]\n",
      " [-0.05375562 -0.06089672  0.03208582 ... -0.07634655 -0.09618073\n",
      "   0.05454744]\n",
      " [ 0.06817168 -0.08438812  0.09931149 ... -0.07249464 -0.00628778\n",
      "  -0.09900911]\n",
      " ...\n",
      " [ 0.0650722  -0.08244785 -0.01507235 ...  0.0451807   0.01854333\n",
      "  -0.06033641]\n",
      " [-0.07460447 -0.01417787  0.08539942 ...  0.02451913 -0.05925152\n",
      "  -0.00569646]\n",
      " [ 0.07058471 -0.09108784  0.01686436 ... -0.05998705 -0.07817475\n",
      "  -0.05068637]]\n",
      "LAYER 7\n",
      "[-0.0490687  -0.0293169  -0.05136001 -0.00839638 -0.0509252  -0.02698602\n",
      " -0.04493431 -0.03902571 -0.09730743 -0.0720438  -0.01979391 -0.03734969\n",
      " -0.06824198 -0.05966812 -0.07372968 -0.06937974 -0.07770451 -0.0791976\n",
      " -0.08664892 -0.05750586 -0.02893043 -0.07104632 -0.06954198 -0.05287752\n",
      " -0.03372678 -0.0565781  -0.05863232 -0.06267344 -0.03116033 -0.06748673\n",
      " -0.04251258 -0.03828213 -0.07059063 -0.04650414 -0.07151151 -0.06176316\n",
      " -0.06442776 -0.07610922 -0.05116709 -0.07013575 -0.05209883 -0.07185198\n",
      " -0.05577986 -0.04876195 -0.06498805 -0.08511256 -0.05388466 -0.06533199\n",
      " -0.03632032 -0.08368672 -0.05994067 -0.07952221 -0.07723423 -0.07102055\n",
      " -0.0580013  -0.06037221 -0.06937643 -0.07654662 -0.05115442 -0.0544837\n",
      " -0.05039866 -0.0458051  -0.0480056  -0.03443039 -0.04352309 -0.07233665\n",
      " -0.03505198 -0.05293738 -0.04013568 -0.03566321 -0.02638779 -0.01389682\n",
      " -0.05601576 -0.05346548 -0.04202459 -0.05851085 -0.02525607 -0.05459578\n",
      " -0.05157305 -0.05963726 -0.03995221 -0.03936744 -0.03986059 -0.0430491\n",
      " -0.05513716 -0.06647842 -0.06150151 -0.05386138 -0.06685697 -0.06742972\n",
      " -0.05836677 -0.0664352  -0.03349093 -0.03540684 -0.0698102  -0.08743025\n",
      " -0.03982531 -0.02599258 -0.01769354 -0.05663709 -0.03707195 -0.02234719\n",
      " -0.03422112 -0.04456682 -0.02631163 -0.04803015 -0.02048354 -0.03648505\n",
      " -0.02575154 -0.0253029  -0.01672056 -0.04112621 -0.04778306 -0.04246847\n",
      " -0.02414428 -0.04188872 -0.02583249 -0.03177705 -0.02046922 -0.04266066\n",
      " -0.02171098 -0.02794506 -0.02595894 -0.03914992 -0.02830275 -0.03599379\n",
      " -0.02429471 -0.02581101 -0.02455543 -0.02607734 -0.0433913  -0.06348427\n",
      " -0.03504549 -0.04883095 -0.05204979 -0.05503013 -0.07495344 -0.04742247\n",
      " -0.06541558 -0.07276189 -0.0630834  -0.076134   -0.05902744 -0.06266545\n",
      " -0.08845185 -0.06294815 -0.07223272 -0.04804372 -0.05770301 -0.07075051\n",
      " -0.05797305 -0.04053637 -0.05420694 -0.0546596  -0.05475403 -0.03931168\n",
      " -0.04912298 -0.05451692 -0.06445518 -0.03793621 -0.05568109 -0.07424673\n",
      " -0.05871981 -0.06413276 -0.08223661 -0.06803903 -0.04798701 -0.08062135\n",
      " -0.07781147 -0.0647184  -0.08034182 -0.05571624 -0.06378521 -0.05854486\n",
      " -0.04177829 -0.05053681 -0.05619718 -0.03853253 -0.03548449 -0.0587232\n",
      " -0.0713679  -0.06349759 -0.02246023 -0.05590423 -0.04452708 -0.0647501\n",
      " -0.04777389 -0.03603748 -0.03634499 -0.03080102 -0.02381455 -0.02792323\n",
      " -0.0433834  -0.04983807 -0.03005979 -0.04607298 -0.04293871 -0.0350039\n",
      " -0.01561133 -0.01608306 -0.04226076 -0.04668932 -0.03668011 -0.01681984\n",
      " -0.01772701 -0.02970373 -0.01332368 -0.03035925 -0.03135087 -0.02856759\n",
      " -0.03625606 -0.03918646 -0.03558658 -0.02905889 -0.02919823 -0.0350953\n",
      " -0.0385277  -0.03485305 -0.02872147 -0.03578814 -0.02184301 -0.01609608\n",
      " -0.03482846 -0.04318222 -0.01851694 -0.03530515 -0.01892295 -0.03012696\n",
      " -0.03130163 -0.03141853 -0.03781095 -0.02541598 -0.03194214 -0.02256243\n",
      " -0.01556776 -0.02311264 -0.02535183 -0.03232209 -0.05103584 -0.03027502\n",
      " -0.02564513 -0.0322636  -0.03563927 -0.02035725 -0.02241097 -0.03252678\n",
      " -0.01763685 -0.0214776  -0.04175359 -0.02392974 -0.02056178  0.00209075\n",
      " -0.03340724 -0.0161847  -0.02780346 -0.01503807 -0.03462772 -0.03868503\n",
      " -0.03961788 -0.03708571 -0.0271834  -0.01944746 -0.00734003 -0.03846807\n",
      " -0.01634139 -0.01946695 -0.02584783 -0.02465503 -0.02896457 -0.04915572\n",
      " -0.04235198 -0.02294209 -0.03537497 -0.01664712 -0.02661246 -0.02380749\n",
      " -0.02225528 -0.02435432 -0.03749593 -0.03384545 -0.04381876 -0.0429172\n",
      " -0.03541889 -0.03796297 -0.03397045 -0.02484911 -0.02669145 -0.03186646\n",
      " -0.02526632 -0.0383934  -0.03854345 -0.04064676 -0.04050158 -0.03943556\n",
      " -0.034516   -0.03425405 -0.02865527 -0.03257201 -0.01728735 -0.01079153\n",
      " -0.01565017 -0.02984663 -0.02458492 -0.03474896 -0.03804973 -0.01582831\n",
      " -0.03315426 -0.0241705  -0.01801505 -0.02876668 -0.01936604 -0.01695742\n",
      " -0.02563277 -0.02485901 -0.0204398  -0.01848183 -0.01829802 -0.02072844\n",
      " -0.01378448 -0.03176497 -0.03090297 -0.01368561 -0.02742104 -0.03400815\n",
      " -0.03857883 -0.01641373 -0.02059383 -0.03318388 -0.01197016 -0.02330001\n",
      " -0.03170744 -0.02939917 -0.03045135 -0.03233895 -0.0202136  -0.02007931\n",
      " -0.02700637 -0.02436648 -0.0298788  -0.0222708  -0.01281924 -0.00698877\n",
      " -0.02981961 -0.0450633  -0.01660979 -0.01285808 -0.03310783 -0.02655668\n",
      " -0.02104866 -0.03828464 -0.0313274  -0.02183327 -0.02375383 -0.03518392\n",
      " -0.02810733 -0.0160567  -0.01591871 -0.03090755 -0.01751486 -0.015832\n",
      " -0.03431357 -0.02790637 -0.01549792 -0.03450655 -0.01900781 -0.03290327\n",
      " -0.01625911 -0.02102604 -0.03570441 -0.03607463 -0.04034949 -0.02932168\n",
      " -0.03795109 -0.03026177 -0.02721303 -0.00059056 -0.03514096 -0.02746746\n",
      " -0.01183268 -0.02341487 -0.02365066 -0.02509743 -0.01323937 -0.02886656\n",
      " -0.01475775 -0.0381648  -0.02867122 -0.04755064 -0.02065277 -0.01719304\n",
      " -0.0083799  -0.02395214 -0.03653773 -0.01928427 -0.04008748 -0.03361408\n",
      " -0.03632946 -0.02681044 -0.01392536 -0.01325827 -0.00971349 -0.02967691\n",
      " -0.02703572 -0.02856238 -0.02752241 -0.02995967 -0.04428865 -0.02242026\n",
      " -0.02599009 -0.03845062 -0.02139938 -0.01534227 -0.0201723  -0.03925305\n",
      " -0.02233373 -0.01627491 -0.04066986 -0.04109424 -0.04556648 -0.0348733\n",
      " -0.0546582  -0.02107366 -0.04706393 -0.02757012 -0.02932714 -0.03547139\n",
      " -0.03127714 -0.03017404 -0.0471475  -0.0302073  -0.02591061 -0.04118588\n",
      " -0.02887853 -0.04030918 -0.03979237 -0.04005555 -0.03685641 -0.02062754\n",
      " -0.03472259 -0.03551394 -0.03578352 -0.02090901 -0.02472807 -0.03095887\n",
      " -0.02375217 -0.02899462 -0.03980202 -0.04425287 -0.03363194 -0.05033958\n",
      " -0.04945986 -0.0267281  -0.03699027 -0.03023229 -0.02870233 -0.02064697\n",
      " -0.03670144 -0.0248197  -0.03580081 -0.02945032 -0.04535386 -0.03339342\n",
      " -0.0377056  -0.02547605 -0.03528241 -0.02576923 -0.02971736 -0.02314667\n",
      " -0.04184937 -0.02813963 -0.04189499 -0.04566525 -0.03486693 -0.02942956\n",
      " -0.05593355 -0.0306761  -0.03051797 -0.04144376 -0.04805722 -0.02732562\n",
      " -0.04684716 -0.01887807 -0.02696728 -0.04188424 -0.03015391 -0.03166996\n",
      " -0.0342059  -0.02475257 -0.03784424 -0.02518971 -0.03920076 -0.03460575\n",
      " -0.03990005 -0.03831732 -0.04603454 -0.02887517 -0.03135696 -0.0169028\n",
      " -0.03481094 -0.01927485 -0.04208414 -0.03675289 -0.01801927 -0.03265769\n",
      " -0.05296651 -0.02812984 -0.0311783  -0.04128417 -0.01729014 -0.03015999\n",
      " -0.03574496 -0.04033876]\n"
     ]
    }
   ],
   "source": [
    "weights = autoencoder.get_weights()\n",
    "for idx, weight in enumerate(weights):\n",
    "    print(\"LAYER {}\".format(idx))\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture --no-display \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up figure\n",
    "f = plt.figure(figsize=(12,5))\n",
    "f.add_subplot(1,2, 1)\n",
    "\n",
    "# plot accuracy as a function of epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "\n",
    "# plot loss as a function of epoch\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature and Label Grabbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "\n",
    "le = LabelEncoder()\n",
    "naivetruth_path = \"/Users/ryan/Documents/CS/CDAC/official_xtract/xtract-sampler/automated_training_results/naivetruth.csv\"\n",
    "\n",
    "def grab_labels(csv_path):\n",
    "    \"\"\"Returns the file paths and file labels from a naivetruth csv.\n",
    "    \n",
    "    Parameter:\n",
    "    csv_path (str): Path of csv file to take labels and paths from.\n",
    "    \n",
    "    Returns:\n",
    "    labels (list): List of label strings from csv_path.\n",
    "    file_paths (list): List of file_paths from csv_path.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    file_paths = []\n",
    "    \n",
    "    with open(csv_path) as label_file:\n",
    "        csv_reader = csv.reader(label_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            file_paths.append(row[0])\n",
    "            labels.append(row[2])\n",
    "    \n",
    "    return labels, file_paths\n",
    "\n",
    "y, file_paths = grab_labels(naivetruth_path)\n",
    "y.pop(0) #Gets rid of headers\n",
    "file_paths.pop(0)\n",
    "\n",
    "x = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    x.append(feature_from_file(file_path))\n",
    "\n",
    "x = encoder.predict(translate_bytes(x))\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 709\n",
      "Trainable params: 709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_59 to have shape (5,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-396eb81b5ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                validation_data=(x_test, y_test)) \n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_59 to have shape (5,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "classifier_dim = len(x[0])\n",
    "\n",
    "classifier_model = Sequential()\n",
    "classifier_model.add(Dense(16, activation='relu', input_shape=(classifier_dim,)))\n",
    "classifier_model.add(Dense(8, activation='relu'))\n",
    "classifier_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "classifier_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_model.summary()\n",
    "\n",
    "history = classifier_model.fit(x_train, y_train,\n",
    "                               epochs=100,\n",
    "                               batch_size = 256,\n",
    "                               shuffle=True,\n",
    "                               validation_data=(x_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "file_autoencoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
