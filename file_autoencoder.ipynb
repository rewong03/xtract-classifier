{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to grab features from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ktb3NKMBoSTv",
    "outputId": "253ee7d9-48e3-4dd7-e871-7734e573deaa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def feature_from_file(file_path, feature_type=\"head\", byte_num=512): # will add more feature_type later\n",
    "    \"\"\"Retreives features from a file.\n",
    "  \n",
    "    Parameters:\n",
    "    feature_type (str): \"head\" to get bytes from head of the file.\n",
    "    byte_num (int): Number of bytes to grab.\n",
    "    file_path (str): File path of file to get features from.\n",
    "    \n",
    "    Returns:\n",
    "    List of bytes from file_path. \n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        byte = f.read(1)\n",
    "        index = 1\n",
    "        features = []\n",
    "    \n",
    "        while byte and index <= byte_num:\n",
    "            features.append(byte)\n",
    "            index += 1\n",
    "            byte = f.read(1)\n",
    "        \n",
    "        if len(features) < byte_num:\n",
    "            features.extend([b'' for i in range(byte_num - len(features))])\n",
    "\n",
    "        assert len(features) == byte_num\n",
    "        return features\n",
    "\n",
    "def feature_from_dir(dir_path, feature_type=\"head\", byte_num=512):\n",
    "    \"\"\"Takes a directory and grabs features from each file.\n",
    "    \n",
    "    Parameters:\n",
    "    dir_path (str): Path of directory to take features from.\n",
    "    feature_type (str): Type of features to get.\n",
    "    byte_num (str): Number of features to take\n",
    "    \n",
    "    Return:\n",
    "    features (list): List containing a list of byte_num bytes from each fie in dir_path.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(dir_path):\n",
    "        for filename in filenames:\n",
    "            features.append(feature_from_file(os.path.join(dirpath, filename), feature_type, byte_num))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def translate_bytes(dir_features):\n",
    "    \"\"\"Translates bytes into integers.\n",
    "    \n",
    "    Parameter:\n",
    "    dir_features (list): List containing lists of bytes.\n",
    "    \n",
    "    Return:\n",
    "    translated_features (numpy array): dir_features with bytes translated to integers.\n",
    "    \"\"\"\n",
    "    translated_features = np.zeros((len(dir_features), len(dir_features[0])))\n",
    "    \n",
    "    for idx, file_features in enumerate(dir_features):\n",
    "        translated_features[idx] = np.array([int.from_bytes(c, byteorder=\"big\") for c in file_features])\n",
    "    \n",
    "    return translated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'S', b'O', b'C', b'A', b'T', b' ', b'd', b'a', b't', b'a', b' ', b'r', b'e', b'p', b'o', b'r', b't', b' ', b'c', b'r', b'e', b'a', b't', b'e', b'd', b':', b' ', b'2', b'0', b'1', b'5', b'-', b'1', b'0', b'-', b'2', b'8', b' ', b'1', b'9', b':', b'1', b'4', b' ', b'+', b'0', b'0', b'0', b'0', b'\\n', b'D', b'O', b'I', b' ', b'o', b'f', b' ', b't', b'h', b'e', b' ', b'e', b'n', b't', b'i', b'r', b'e', b' ', b'S', b'O', b'C', b'A', b'T', b' ', b'c', b'o', b'l', b'l', b'e', b'c', b't', b'i', b'o', b'n', b':', b' ', b'1', b'0', b'.', b'1', b'5', b'9', b'4', b'/', b'P', b'A', b'N', b'G', b'A', b'E', b'A', b'.', b'8', b'4', b'9', b'7', b'7', b'0', b'\\n', b' ', b' ', b' ', b' ', b'o', b'r', b' ', b's', b'e', b'e', b':', b' ', b'h', b't', b't', b'p', b':', b'/', b'/', b'd', b'o', b'i', b'.', b'p', b'a', b'n', b'g', b'a', b'e', b'a', b'.', b'd', b'e', b'/', b'1', b'0', b'.', b'1', b'5', b'9', b'4', b'/', b'P', b'A', b'N', b'G', b'A', b'E', b'A', b'.', b'8', b'4', b'9', b'7', b'7', b'0', b'\\n', b'S', b'O', b'C', b'A', b'T', b' ', b'c', b'r', b'u', b'i', b's', b'e', b' ', b'd', b'a', b't', b'a', b' ', b'i', b'n', b' ', b'S', b'O', b'C', b'A', b'T', b' ', b'r', b'e', b'g', b'i', b'o', b'n', b' ', b'\"', b'T', b'r', b'o', b'p', b'i', b'c', b'a', b'l', b' ', b'P', b'a', b'c', b'i', b'f', b'i', b'c', b'\"', b' ', b'f', b'o', b'r', b' ', b't', b'h', b'e', b' ', b'f', b'o', b'l', b'l', b'o', b'w', b'i', b'n', b'g', b' ', b'c', b'r', b'u', b'i', b's', b'e', b's', b':', b'\\n', b'E', b'x', b'p', b'o', b'c', b'o', b'd', b'e', b'\\t', b'v', b'e', b'r', b's', b'i', b'o', b'n', b'\\t', b'C', b'r', b'u', b'i', b's', b'e', b'/', b'D', b'a', b't', b'a', b's', b'e', b't', b' ', b'N', b'a', b'm', b'e', b'\\t', b'S', b'h', b'i', b'p', b'/', b'V', b'e', b's', b's', b'e', b'l', b' ', b'N', b'a', b'm', b'e', b'\\t', b'P', b'I', b'(', b's', b')', b'\\t', b'O', b'r', b'i', b'g', b'i', b'n', b'a', b'l', b' ', b'D', b'a', b't', b'a', b' ', b'D', b'O', b'I', b'\\t', b'O', b'r', b'i', b'g', b'i', b'n', b'a', b'l', b' ', b'D', b'a', b't', b'a', b' ', b'R', b'e', b'f', b'e', b'r', b'e', b'n', b'c', b'e', b'\\t', b'S', b'O', b'C', b'A', b'T', b' ', b'D', b'O', b'I', b'\\t', b'S', b'O', b'C', b'A', b'T', b' ', b'R', b'e', b'f', b'e', b'r', b'e', b'n', b'c', b'e', b'\\t', b'W', b'e', b's', b't', b'm', b'o', b's', b't', b' ', b'L', b'o', b'n', b'g', b'i', b't', b'u', b'd', b'e', b'\\t', b'E', b'a', b's', b't', b'm', b'o', b's', b't', b' ', b'L', b'o', b'n', b'g', b'i', b't', b'u', b'd', b'e', b'\\t', b'S', b'o', b'u', b't', b'h', b'm', b'o', b's', b't', b' ', b'L', b'a', b't', b'i', b't', b'u', b'd', b'e', b'\\t', b'N', b'o', b'r', b't', b'h', b'm', b'o', b's', b't', b' ', b'L', b'a', b't', b'i', b't', b'u', b'd', b'e', b'\\t', b'S', b't', b'a', b'r', b't', b' ', b'T', b'i', b'm', b'e', b'\\t', b'E', b'n', b'd', b' ', b'T', b'i', b'm', b'e', b'\\t', b'Q', b'C', b' ', b'F', b'l', b'a', b'g', b'\\t', b'A', b'd', b'd', b'i', b't', b'i', b'o', b'n', b'a', b'l', b' ', b'M', b'e', b't', b'a', b'd', b'a', b't', b'a', b' ', b'D', b'o', b'c', b'u', b'm', b'e', b'n', b't', b'(', b's', b')', b'\\n', b'0', b'6']\n",
      "[ 83.  79.  67.  65.  84.  32. 100.  97. 116.  97.  32. 114. 101. 112.\n",
      " 111. 114. 116.  32.  99. 114. 101.  97. 116. 101. 100.  58.  32.  50.\n",
      "  48.  49.  53.  45.  49.  48.  45.  50.  56.  32.  49.  57.  58.  49.\n",
      "  52.  32.  43.  48.  48.  48.  48.  10.  68.  79.  73.  32. 111. 102.\n",
      "  32. 116. 104. 101.  32. 101. 110. 116. 105. 114. 101.  32.  83.  79.\n",
      "  67.  65.  84.  32.  99. 111. 108. 108. 101.  99. 116. 105. 111. 110.\n",
      "  58.  32.  49.  48.  46.  49.  53.  57.  52.  47.  80.  65.  78.  71.\n",
      "  65.  69.  65.  46.  56.  52.  57.  55.  55.  48.  10.  32.  32.  32.\n",
      "  32. 111. 114.  32. 115. 101. 101.  58.  32. 104. 116. 116. 112.  58.\n",
      "  47.  47. 100. 111. 105.  46. 112.  97. 110. 103.  97. 101.  97.  46.\n",
      " 100. 101.  47.  49.  48.  46.  49.  53.  57.  52.  47.  80.  65.  78.\n",
      "  71.  65.  69.  65.  46.  56.  52.  57.  55.  55.  48.  10.  83.  79.\n",
      "  67.  65.  84.  32.  99. 114. 117. 105. 115. 101.  32. 100.  97. 116.\n",
      "  97.  32. 105. 110.  32.  83.  79.  67.  65.  84.  32. 114. 101. 103.\n",
      " 105. 111. 110.  32.  34.  84. 114. 111. 112. 105.  99.  97. 108.  32.\n",
      "  80.  97.  99. 105. 102. 105.  99.  34.  32. 102. 111. 114.  32. 116.\n",
      " 104. 101.  32. 102. 111. 108. 108. 111. 119. 105. 110. 103.  32.  99.\n",
      " 114. 117. 105. 115. 101. 115.  58.  10.  69. 120. 112. 111.  99. 111.\n",
      " 100. 101.   9. 118. 101. 114. 115. 105. 111. 110.   9.  67. 114. 117.\n",
      " 105. 115. 101.  47.  68.  97. 116.  97. 115. 101. 116.  32.  78.  97.\n",
      " 109. 101.   9.  83. 104. 105. 112.  47.  86. 101. 115. 115. 101. 108.\n",
      "  32.  78.  97. 109. 101.   9.  80.  73.  40. 115.  41.   9.  79. 114.\n",
      " 105. 103. 105. 110.  97. 108.  32.  68.  97. 116.  97.  32.  68.  79.\n",
      "  73.   9.  79. 114. 105. 103. 105. 110.  97. 108.  32.  68.  97. 116.\n",
      "  97.  32.  82. 101. 102. 101. 114. 101. 110.  99. 101.   9.  83.  79.\n",
      "  67.  65.  84.  32.  68.  79.  73.   9.  83.  79.  67.  65.  84.  32.\n",
      "  82. 101. 102. 101. 114. 101. 110.  99. 101.   9.  87. 101. 115. 116.\n",
      " 109. 111. 115. 116.  32.  76. 111. 110. 103. 105. 116. 117. 100. 101.\n",
      "   9.  69.  97. 115. 116. 109. 111. 115. 116.  32.  76. 111. 110. 103.\n",
      " 105. 116. 117. 100. 101.   9.  83. 111. 117. 116. 104. 109. 111. 115.\n",
      " 116.  32.  76.  97. 116. 105. 116. 117. 100. 101.   9.  78. 111. 114.\n",
      " 116. 104. 109. 111. 115. 116.  32.  76.  97. 116. 105. 116. 117. 100.\n",
      " 101.   9.  83. 116.  97. 114. 116.  32.  84. 105. 109. 101.   9.  69.\n",
      " 110. 100.  32.  84. 105. 109. 101.   9.  81.  67.  32.  70. 108.  97.\n",
      " 103.   9.  65. 100. 100. 105. 116. 105. 111. 110.  97. 108.  32.  77.\n",
      " 101. 116.  97. 100.  97. 116.  97.  32.  68. 111.  99. 117. 109. 101.\n",
      " 110. 116.  40. 115.  41.  10.  48.  54.]\n",
      "[0.3254902  0.30980392 0.2627451  0.25490196 0.32941176 0.1254902\n",
      " 0.39215686 0.38039216 0.45490196 0.38039216 0.1254902  0.44705882\n",
      " 0.39607843 0.43921569 0.43529412 0.44705882 0.45490196 0.1254902\n",
      " 0.38823529 0.44705882 0.39607843 0.38039216 0.45490196 0.39607843\n",
      " 0.39215686 0.22745098 0.1254902  0.19607843 0.18823529 0.19215686\n",
      " 0.20784314 0.17647059 0.19215686 0.18823529 0.17647059 0.19607843\n",
      " 0.21960784 0.1254902  0.19215686 0.22352941 0.22745098 0.19215686\n",
      " 0.20392157 0.1254902  0.16862745 0.18823529 0.18823529 0.18823529\n",
      " 0.18823529 0.03921569 0.26666667 0.30980392 0.28627451 0.1254902\n",
      " 0.43529412 0.4        0.1254902  0.45490196 0.40784314 0.39607843\n",
      " 0.1254902  0.39607843 0.43137255 0.45490196 0.41176471 0.44705882\n",
      " 0.39607843 0.1254902  0.3254902  0.30980392 0.2627451  0.25490196\n",
      " 0.32941176 0.1254902  0.38823529 0.43529412 0.42352941 0.42352941\n",
      " 0.39607843 0.38823529 0.45490196 0.41176471 0.43529412 0.43137255\n",
      " 0.22745098 0.1254902  0.19215686 0.18823529 0.18039216 0.19215686\n",
      " 0.20784314 0.22352941 0.20392157 0.18431373 0.31372549 0.25490196\n",
      " 0.30588235 0.27843137 0.25490196 0.27058824 0.25490196 0.18039216\n",
      " 0.21960784 0.20392157 0.22352941 0.21568627 0.21568627 0.18823529\n",
      " 0.03921569 0.1254902  0.1254902  0.1254902  0.1254902  0.43529412\n",
      " 0.44705882 0.1254902  0.45098039 0.39607843 0.39607843 0.22745098\n",
      " 0.1254902  0.40784314 0.45490196 0.45490196 0.43921569 0.22745098\n",
      " 0.18431373 0.18431373 0.39215686 0.43529412 0.41176471 0.18039216\n",
      " 0.43921569 0.38039216 0.43137255 0.40392157 0.38039216 0.39607843\n",
      " 0.38039216 0.18039216 0.39215686 0.39607843 0.18431373 0.19215686\n",
      " 0.18823529 0.18039216 0.19215686 0.20784314 0.22352941 0.20392157\n",
      " 0.18431373 0.31372549 0.25490196 0.30588235 0.27843137 0.25490196\n",
      " 0.27058824 0.25490196 0.18039216 0.21960784 0.20392157 0.22352941\n",
      " 0.21568627 0.21568627 0.18823529 0.03921569 0.3254902  0.30980392\n",
      " 0.2627451  0.25490196 0.32941176 0.1254902  0.38823529 0.44705882\n",
      " 0.45882353 0.41176471 0.45098039 0.39607843 0.1254902  0.39215686\n",
      " 0.38039216 0.45490196 0.38039216 0.1254902  0.41176471 0.43137255\n",
      " 0.1254902  0.3254902  0.30980392 0.2627451  0.25490196 0.32941176\n",
      " 0.1254902  0.44705882 0.39607843 0.40392157 0.41176471 0.43529412\n",
      " 0.43137255 0.1254902  0.13333333 0.32941176 0.44705882 0.43529412\n",
      " 0.43921569 0.41176471 0.38823529 0.38039216 0.42352941 0.1254902\n",
      " 0.31372549 0.38039216 0.38823529 0.41176471 0.4        0.41176471\n",
      " 0.38823529 0.13333333 0.1254902  0.4        0.43529412 0.44705882\n",
      " 0.1254902  0.45490196 0.40784314 0.39607843 0.1254902  0.4\n",
      " 0.43529412 0.42352941 0.42352941 0.43529412 0.46666667 0.41176471\n",
      " 0.43137255 0.40392157 0.1254902  0.38823529 0.44705882 0.45882353\n",
      " 0.41176471 0.45098039 0.39607843 0.45098039 0.22745098 0.03921569\n",
      " 0.27058824 0.47058824 0.43921569 0.43529412 0.38823529 0.43529412\n",
      " 0.39215686 0.39607843 0.03529412 0.4627451  0.39607843 0.44705882\n",
      " 0.45098039 0.41176471 0.43529412 0.43137255 0.03529412 0.2627451\n",
      " 0.44705882 0.45882353 0.41176471 0.45098039 0.39607843 0.18431373\n",
      " 0.26666667 0.38039216 0.45490196 0.38039216 0.45098039 0.39607843\n",
      " 0.45490196 0.1254902  0.30588235 0.38039216 0.42745098 0.39607843\n",
      " 0.03529412 0.3254902  0.40784314 0.41176471 0.43921569 0.18431373\n",
      " 0.3372549  0.39607843 0.45098039 0.45098039 0.39607843 0.42352941\n",
      " 0.1254902  0.30588235 0.38039216 0.42745098 0.39607843 0.03529412\n",
      " 0.31372549 0.28627451 0.15686275 0.45098039 0.16078431 0.03529412\n",
      " 0.30980392 0.44705882 0.41176471 0.40392157 0.41176471 0.43137255\n",
      " 0.38039216 0.42352941 0.1254902  0.26666667 0.38039216 0.45490196\n",
      " 0.38039216 0.1254902  0.26666667 0.30980392 0.28627451 0.03529412\n",
      " 0.30980392 0.44705882 0.41176471 0.40392157 0.41176471 0.43137255\n",
      " 0.38039216 0.42352941 0.1254902  0.26666667 0.38039216 0.45490196\n",
      " 0.38039216 0.1254902  0.32156863 0.39607843 0.4        0.39607843\n",
      " 0.44705882 0.39607843 0.43137255 0.38823529 0.39607843 0.03529412\n",
      " 0.3254902  0.30980392 0.2627451  0.25490196 0.32941176 0.1254902\n",
      " 0.26666667 0.30980392 0.28627451 0.03529412 0.3254902  0.30980392\n",
      " 0.2627451  0.25490196 0.32941176 0.1254902  0.32156863 0.39607843\n",
      " 0.4        0.39607843 0.44705882 0.39607843 0.43137255 0.38823529\n",
      " 0.39607843 0.03529412 0.34117647 0.39607843 0.45098039 0.45490196\n",
      " 0.42745098 0.43529412 0.45098039 0.45490196 0.1254902  0.29803922\n",
      " 0.43529412 0.43137255 0.40392157 0.41176471 0.45490196 0.45882353\n",
      " 0.39215686 0.39607843 0.03529412 0.27058824 0.38039216 0.45098039\n",
      " 0.45490196 0.42745098 0.43529412 0.45098039 0.45490196 0.1254902\n",
      " 0.29803922 0.43529412 0.43137255 0.40392157 0.41176471 0.45490196\n",
      " 0.45882353 0.39215686 0.39607843 0.03529412 0.3254902  0.43529412\n",
      " 0.45882353 0.45490196 0.40784314 0.42745098 0.43529412 0.45098039\n",
      " 0.45490196 0.1254902  0.29803922 0.38039216 0.45490196 0.41176471\n",
      " 0.45490196 0.45882353 0.39215686 0.39607843 0.03529412 0.30588235\n",
      " 0.43529412 0.44705882 0.45490196 0.40784314 0.42745098 0.43529412\n",
      " 0.45098039 0.45490196 0.1254902  0.29803922 0.38039216 0.45490196\n",
      " 0.41176471 0.45490196 0.45882353 0.39215686 0.39607843 0.03529412\n",
      " 0.3254902  0.45490196 0.38039216 0.44705882 0.45490196 0.1254902\n",
      " 0.32941176 0.41176471 0.42745098 0.39607843 0.03529412 0.27058824\n",
      " 0.43137255 0.39215686 0.1254902  0.32941176 0.41176471 0.42745098\n",
      " 0.39607843 0.03529412 0.31764706 0.2627451  0.1254902  0.2745098\n",
      " 0.42352941 0.38039216 0.40392157 0.03529412 0.25490196 0.39215686\n",
      " 0.39215686 0.41176471 0.45490196 0.41176471 0.43529412 0.43137255\n",
      " 0.38039216 0.42352941 0.1254902  0.30196078 0.39607843 0.45490196\n",
      " 0.38039216 0.39215686 0.38039216 0.45490196 0.38039216 0.1254902\n",
      " 0.26666667 0.43529412 0.38823529 0.45882353 0.42745098 0.39607843\n",
      " 0.43137255 0.45490196 0.15686275 0.45098039 0.16078431 0.03921569\n",
      " 0.18823529 0.21176471]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_set_dir = '/Users/ryan/Documents/CS/CDAC/official_xtract/sampler_dataset/pub8'\n",
    "\n",
    "raw_features = feature_from_dir(test_set_dir, byte_num=512)\n",
    "untranslated_features = translate_bytes(raw_features)\n",
    "x = untranslated_features / 255\n",
    "\n",
    "x_train, x_test, _, _ = train_test_split(x, x)\n",
    "\n",
    "print(raw_features[0])\n",
    "print(untranslated_features[0])\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24705882 0.66666667 0.56470588 0.82352941 0.78039216 0.72941176\n",
      " 0.42745098 0.89411765 0.89019608 0.26666667 0.14509804 0.12941176\n",
      " 0.2        0.21568627 0.10196078 0.60392157 0.7372549  0.77254902\n",
      " 0.16078431 0.78039216 0.82745098 0.25882353 0.80784314 0.72156863\n",
      " 0.30980392 0.6627451  0.55294118 0.52941176 0.15294118 0.4\n",
      " 0.50980392 0.88235294 0.83137255 0.13333333 0.85882353 0.86666667\n",
      " 0.16862745 0.10196078 0.89411765 0.55686275 0.0745098  0.96470588\n",
      " 0.0627451  0.94509804 0.01960784 0.37254902 0.52941176 0.85490196\n",
      " 0.08235294 0.29411765 0.78823529 0.85098039 0.96862745 1.\n",
      " 0.76078431 0.43529412 0.39607843 0.02745098 0.94509804 0.68235294\n",
      " 0.23137255 0.30588235 0.30980392 0.78039216 0.80392157 0.24705882\n",
      " 0.25490196 0.59215686 0.89803922 0.96078431 0.94117647 0.36862745\n",
      " 0.32156863 0.13333333 0.07843137 0.0627451  0.71764706 0.69019608\n",
      " 0.27058824 0.48235294 0.16470588 0.10980392 0.4627451  0.77254902\n",
      " 0.63529412 0.21960784 0.50588235 0.27058824 0.33333333 0.13333333\n",
      " 0.47843137 0.27843137 0.11372549 0.55294118 0.76470588 0.4\n",
      " 0.74509804 0.62745098 0.18823529 0.11764706 0.72941176 0.44313725\n",
      " 0.4745098  0.48627451 0.03529412 0.41960784 0.65098039 0.78823529\n",
      " 0.4745098  0.12941176 0.32941176 0.87843137 0.8745098  0.8745098\n",
      " 0.46666667 0.74117647 0.29019608 0.18039216 0.73333333 0.09019608\n",
      " 0.96470588 0.8627451  0.90196078 1.         0.52941176 0.88627451\n",
      " 0.12156863 0.51764706 0.20784314 0.10980392 0.54509804 0.76862745\n",
      " 0.05490196 0.08235294 0.07843137 0.88235294 0.0745098  0.90588235\n",
      " 0.90588235 0.47058824 0.50588235 0.07843137 0.30980392 0.4745098\n",
      " 0.29803922 0.74901961 0.19607843 0.17254902 0.39607843 0.80784314\n",
      " 0.36470588 0.04705882 0.92156863 0.15294118 0.15686275 0.05882353\n",
      " 0.00392157 0.06666667 0.48627451 0.52941176 0.81960784 0.68235294\n",
      " 0.42745098 0.23529412 0.19607843 0.35294118 0.66666667 0.83137255\n",
      " 0.97254902 0.37254902 0.7254902  0.94901961 0.97254902 0.11372549\n",
      " 0.3254902  0.31372549 0.30980392 0.08235294 0.85490196 0.71764706\n",
      " 0.76470588 0.25098039 0.55294118 0.76470588 0.30980392 0.53333333\n",
      " 0.72941176 0.32156863 0.56862745 0.16470588 0.48235294 0.50980392\n",
      " 0.76078431 0.94509804 0.20392157 0.9372549  0.75686275 0.60784314\n",
      " 0.09803922 0.17254902 0.10196078 0.22352941 0.6        0.05882353\n",
      " 0.88235294 0.85882353 0.59215686 0.12156863 0.02352941 0.50980392\n",
      " 0.76470588 0.58431373 0.56862745 0.20392157 0.5254902  0.36862745\n",
      " 0.83529412 0.81960784 0.09803922 0.45098039 0.45882353 0.3254902\n",
      " 0.02745098 0.03529412 0.94509804 0.21960784 0.98823529 0.19607843\n",
      " 0.68627451 0.42745098 0.04313725 0.35686275 0.40784314 0.47058824\n",
      " 0.17254902 0.63921569 0.12941176 0.77647059 0.21960784 0.58039216\n",
      " 0.57647059 0.61568627 0.03921569 0.08627451 0.16078431 0.52941176\n",
      " 0.08235294 0.2745098  0.20784314 0.55294118 0.52156863 0.68235294\n",
      " 0.57647059 0.65882353 0.4627451  0.10980392 0.24313725 0.44313725\n",
      " 0.39215686 0.55686275 0.11764706 0.72941176 0.79607843 0.80784314\n",
      " 0.60784314 0.85098039 0.84705882 0.84705882 0.18823529 0.83137255\n",
      " 0.39607843 0.78823529 0.5254902  0.14901961 0.03529412 0.32941176\n",
      " 0.84705882 0.04705882 0.03529412 0.82745098 0.25098039 0.21176471\n",
      " 0.96470588 0.19607843 0.93333333 0.61176471 0.27843137 0.01960784\n",
      " 0.71372549 0.23529412 0.21568627 0.77254902 0.3372549  0.76862745\n",
      " 0.75294118 0.11372549 0.96862745 0.94901961 0.43921569 0.87058824\n",
      " 0.52941176 0.76470588 0.31372549 0.59607843 0.08627451 0.91764706\n",
      " 0.15686275 0.36862745 0.76078431 0.04313725 0.64705882 0.32156863\n",
      " 0.4745098  0.28235294 0.07058824 0.78823529 0.11372549 0.96078431\n",
      " 0.36078431 0.16078431 0.75294118 0.29019608 0.51764706 0.61568627\n",
      " 0.59607843 0.25882353 0.50588235 0.21176471 0.42745098 0.93333333\n",
      " 0.99607843 0.09019608 0.69411765 0.36078431 0.68627451 0.79607843\n",
      " 0.6        0.80784314 0.80392157 0.67843137 0.8        0.7254902\n",
      " 0.05882353 0.65490196 0.16470588 0.29019608 0.19607843 0.4\n",
      " 0.03137255 0.42745098 0.4627451  0.85882353 0.13333333 0.13333333\n",
      " 0.94509804 0.78431373 0.         0.96862745 0.53333333 0.3254902\n",
      " 0.01176471 0.3254902  0.87843137 0.76470588 0.52156863 0.6627451\n",
      " 0.36862745 0.58039216 0.02745098 0.94117647 0.29019608 0.00392157\n",
      " 0.13333333 0.7372549  0.71764706 0.34509804 0.10588235 0.22745098\n",
      " 0.45098039 0.23529412 0.48627451 0.0745098  0.49803922 0.90588235\n",
      " 0.2745098  0.80392157 0.55294118 0.51372549 0.36470588 0.58823529\n",
      " 0.19607843 0.53333333 0.02352941 0.42352941 0.51764706 0.29803922\n",
      " 0.43137255 0.9254902  0.99607843 0.96078431 0.76078431 0.47058824\n",
      " 0.41176471 0.26666667 0.01960784 0.33333333 0.29411765 0.1254902\n",
      " 0.21960784 0.7372549  0.47843137 0.25098039 0.06666667 0.70980392\n",
      " 0.25098039 0.23921569 0.57647059 0.02352941 0.47843137 0.9372549\n",
      " 0.39215686 0.82352941 0.67058824 0.61568627 0.56862745 0.0745098\n",
      " 0.48627451 0.27058824 0.97647059 0.57254902 0.92941176 0.63921569\n",
      " 0.18823529 0.48235294 0.60784314 0.67058824 0.35686275 0.87843137\n",
      " 0.97254902 0.77254902 0.63529412 0.90980392 0.30196078 0.\n",
      " 0.29019608 0.24313725 0.91764706 0.43921569 0.57647059 0.01568627\n",
      " 0.54509804 0.4745098  0.74901961 0.98823529 0.36078431 0.74901961\n",
      " 0.64313725 0.92156863 0.62352941 0.65490196 0.70980392 0.51764706\n",
      " 0.71764706 0.16078431 0.0627451  0.43529412 0.5372549  0.48235294\n",
      " 0.98823529 0.49019608 0.9254902  0.98431373 0.21568627 0.97647059\n",
      " 0.45490196 0.50588235 0.12941176 0.17647059 0.69803922 0.10196078\n",
      " 0.08235294 0.96862745 0.02745098 0.74509804 0.15686275 0.55294118\n",
      " 0.3372549  0.85490196 0.99607843 0.3254902  0.50980392 0.22352941\n",
      " 0.2        0.31372549 0.63137255 0.05882353 0.07843137 0.26666667\n",
      " 0.77254902 0.47058824 0.74901961 0.65490196 0.47843137 0.59607843\n",
      " 0.10196078 0.05490196 0.20784314 0.28235294 0.25490196 0.01960784\n",
      " 0.30588235 0.32941176]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = np.zeros((30000, 512))\n",
    "for idx, thing in enumerate(x):\n",
    "    x[idx] = [random.randint(0, 255) for i in range(512)]\n",
    "\n",
    "x = x / 255\n",
    "print(x[0])\n",
    "\n",
    "x_train, x_test, _, _ = train_test_split(x,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "elu_9 (ELU)                  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "elu_10 (ELU)                 (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "elu_11 (ELU)                 (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               33280     \n",
      "=================================================================\n",
      "Total params: 70,304\n",
      "Trainable params: 70,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 22500 samples, validate on 7500 samples\n",
      "Epoch 1/200\n",
      "22500/22500 [==============================] - 1s 50us/step - loss: 0.6935 - acc: 0.0039 - val_loss: 0.6933 - val_acc: 0.0039\n",
      "Epoch 2/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6933 - acc: 0.0039 - val_loss: 0.6932 - val_acc: 0.0039\n",
      "Epoch 3/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6932 - acc: 0.0040 - val_loss: 0.6932 - val_acc: 0.0039\n",
      "Epoch 4/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6931 - acc: 0.0040 - val_loss: 0.6931 - val_acc: 0.0039\n",
      "Epoch 5/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6931 - acc: 0.0040 - val_loss: 0.6931 - val_acc: 0.0039\n",
      "Epoch 6/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6930 - acc: 0.0040 - val_loss: 0.6930 - val_acc: 0.0040\n",
      "Epoch 7/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6930 - acc: 0.0041 - val_loss: 0.6930 - val_acc: 0.0040\n",
      "Epoch 8/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6929 - acc: 0.0041 - val_loss: 0.6929 - val_acc: 0.0040\n",
      "Epoch 9/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6929 - acc: 0.0041 - val_loss: 0.6929 - val_acc: 0.0040\n",
      "Epoch 10/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6928 - acc: 0.0041 - val_loss: 0.6928 - val_acc: 0.0041\n",
      "Epoch 11/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6928 - acc: 0.0042 - val_loss: 0.6928 - val_acc: 0.0041\n",
      "Epoch 12/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6927 - acc: 0.0042 - val_loss: 0.6927 - val_acc: 0.0041\n",
      "Epoch 13/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6927 - acc: 0.0042 - val_loss: 0.6927 - val_acc: 0.0041\n",
      "Epoch 14/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6926 - acc: 0.0042 - val_loss: 0.6926 - val_acc: 0.0042\n",
      "Epoch 15/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6925 - acc: 0.0042 - val_loss: 0.6925 - val_acc: 0.0042\n",
      "Epoch 16/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6924 - acc: 0.0043 - val_loss: 0.6924 - val_acc: 0.0042\n",
      "Epoch 17/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6923 - acc: 0.0043 - val_loss: 0.6923 - val_acc: 0.0042\n",
      "Epoch 18/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6922 - acc: 0.0043 - val_loss: 0.6922 - val_acc: 0.0043\n",
      "Epoch 19/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6921 - acc: 0.0043 - val_loss: 0.6921 - val_acc: 0.0043\n",
      "Epoch 20/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6920 - acc: 0.0044 - val_loss: 0.6920 - val_acc: 0.0043\n",
      "Epoch 21/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6919 - acc: 0.0044 - val_loss: 0.6919 - val_acc: 0.0043\n",
      "Epoch 22/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6917 - acc: 0.0044 - val_loss: 0.6917 - val_acc: 0.0043\n",
      "Epoch 23/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6916 - acc: 0.0044 - val_loss: 0.6916 - val_acc: 0.0044\n",
      "Epoch 24/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6914 - acc: 0.0045 - val_loss: 0.6914 - val_acc: 0.0044\n",
      "Epoch 25/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6912 - acc: 0.0045 - val_loss: 0.6913 - val_acc: 0.0044\n",
      "Epoch 26/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6910 - acc: 0.0045 - val_loss: 0.6911 - val_acc: 0.0044\n",
      "Epoch 27/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6909 - acc: 0.0045 - val_loss: 0.6909 - val_acc: 0.0045\n",
      "Epoch 28/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6907 - acc: 0.0046 - val_loss: 0.6907 - val_acc: 0.0045\n",
      "Epoch 29/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6904 - acc: 0.0046 - val_loss: 0.6905 - val_acc: 0.0045\n",
      "Epoch 30/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6902 - acc: 0.0046 - val_loss: 0.6903 - val_acc: 0.0045\n",
      "Epoch 31/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6900 - acc: 0.0046 - val_loss: 0.6901 - val_acc: 0.0046\n",
      "Epoch 32/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6898 - acc: 0.0047 - val_loss: 0.6898 - val_acc: 0.0046\n",
      "Epoch 33/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6895 - acc: 0.0047 - val_loss: 0.6896 - val_acc: 0.0046\n",
      "Epoch 34/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6893 - acc: 0.0047 - val_loss: 0.6894 - val_acc: 0.0046\n",
      "Epoch 35/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6890 - acc: 0.0048 - val_loss: 0.6891 - val_acc: 0.0047\n",
      "Epoch 36/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6888 - acc: 0.0048 - val_loss: 0.6888 - val_acc: 0.0047\n",
      "Epoch 37/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6885 - acc: 0.0048 - val_loss: 0.6886 - val_acc: 0.0047\n",
      "Epoch 38/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6882 - acc: 0.0048 - val_loss: 0.6883 - val_acc: 0.0047\n",
      "Epoch 39/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6880 - acc: 0.0048 - val_loss: 0.6881 - val_acc: 0.0047\n",
      "Epoch 40/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6877 - acc: 0.0049 - val_loss: 0.6879 - val_acc: 0.0047\n",
      "Epoch 41/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6875 - acc: 0.0049 - val_loss: 0.6877 - val_acc: 0.0048\n",
      "Epoch 42/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6873 - acc: 0.0049 - val_loss: 0.6875 - val_acc: 0.0048\n",
      "Epoch 43/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6871 - acc: 0.0049 - val_loss: 0.6873 - val_acc: 0.0048\n",
      "Epoch 44/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6869 - acc: 0.0049 - val_loss: 0.6871 - val_acc: 0.0048\n",
      "Epoch 45/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6867 - acc: 0.0050 - val_loss: 0.6869 - val_acc: 0.0048\n",
      "Epoch 46/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6865 - acc: 0.0050 - val_loss: 0.6867 - val_acc: 0.0048\n",
      "Epoch 47/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6863 - acc: 0.0050 - val_loss: 0.6866 - val_acc: 0.0049\n",
      "Epoch 48/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6862 - acc: 0.0050 - val_loss: 0.6864 - val_acc: 0.0049\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6860 - acc: 0.0050 - val_loss: 0.6863 - val_acc: 0.0049\n",
      "Epoch 50/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6858 - acc: 0.0050 - val_loss: 0.6861 - val_acc: 0.0049\n",
      "Epoch 51/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6857 - acc: 0.0050 - val_loss: 0.6860 - val_acc: 0.0049\n",
      "Epoch 52/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6855 - acc: 0.0050 - val_loss: 0.6858 - val_acc: 0.0049\n",
      "Epoch 53/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6854 - acc: 0.0051 - val_loss: 0.6857 - val_acc: 0.0049\n",
      "Epoch 54/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6852 - acc: 0.0051 - val_loss: 0.6856 - val_acc: 0.0049\n",
      "Epoch 55/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6851 - acc: 0.0051 - val_loss: 0.6855 - val_acc: 0.0049\n",
      "Epoch 56/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6850 - acc: 0.0051 - val_loss: 0.6853 - val_acc: 0.0049\n",
      "Epoch 57/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6848 - acc: 0.0051 - val_loss: 0.6852 - val_acc: 0.0050\n",
      "Epoch 58/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6847 - acc: 0.0051 - val_loss: 0.6851 - val_acc: 0.0050\n",
      "Epoch 59/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6846 - acc: 0.0051 - val_loss: 0.6850 - val_acc: 0.0050\n",
      "Epoch 60/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6845 - acc: 0.0051 - val_loss: 0.6849 - val_acc: 0.0050\n",
      "Epoch 61/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6844 - acc: 0.0051 - val_loss: 0.6848 - val_acc: 0.0050\n",
      "Epoch 62/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6842 - acc: 0.0051 - val_loss: 0.6847 - val_acc: 0.0050\n",
      "Epoch 63/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6841 - acc: 0.0051 - val_loss: 0.6846 - val_acc: 0.0050\n",
      "Epoch 64/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6840 - acc: 0.0052 - val_loss: 0.6845 - val_acc: 0.0050\n",
      "Epoch 65/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6839 - acc: 0.0052 - val_loss: 0.6844 - val_acc: 0.0050\n",
      "Epoch 66/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6838 - acc: 0.0052 - val_loss: 0.6843 - val_acc: 0.0050\n",
      "Epoch 67/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6837 - acc: 0.0052 - val_loss: 0.6842 - val_acc: 0.0050\n",
      "Epoch 68/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6836 - acc: 0.0052 - val_loss: 0.6841 - val_acc: 0.0050\n",
      "Epoch 69/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6835 - acc: 0.0052 - val_loss: 0.6841 - val_acc: 0.0050\n",
      "Epoch 70/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6835 - acc: 0.0052 - val_loss: 0.6840 - val_acc: 0.0051\n",
      "Epoch 71/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6834 - acc: 0.0052 - val_loss: 0.6839 - val_acc: 0.0051\n",
      "Epoch 72/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6833 - acc: 0.0052 - val_loss: 0.6839 - val_acc: 0.0051\n",
      "Epoch 73/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6832 - acc: 0.0052 - val_loss: 0.6838 - val_acc: 0.0051\n",
      "Epoch 74/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6831 - acc: 0.0052 - val_loss: 0.6837 - val_acc: 0.0051\n",
      "Epoch 75/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6830 - acc: 0.0052 - val_loss: 0.6837 - val_acc: 0.0051\n",
      "Epoch 76/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6830 - acc: 0.0052 - val_loss: 0.6836 - val_acc: 0.0051\n",
      "Epoch 77/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6829 - acc: 0.0052 - val_loss: 0.6835 - val_acc: 0.0051\n",
      "Epoch 78/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6828 - acc: 0.0052 - val_loss: 0.6835 - val_acc: 0.0051\n",
      "Epoch 79/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6828 - acc: 0.0052 - val_loss: 0.6834 - val_acc: 0.0051\n",
      "Epoch 80/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6827 - acc: 0.0052 - val_loss: 0.6834 - val_acc: 0.0051\n",
      "Epoch 81/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6826 - acc: 0.0052 - val_loss: 0.6833 - val_acc: 0.0051\n",
      "Epoch 82/200\n",
      "22500/22500 [==============================] - 1s 33us/step - loss: 0.6826 - acc: 0.0053 - val_loss: 0.6833 - val_acc: 0.0051\n",
      "Epoch 83/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6825 - acc: 0.0053 - val_loss: 0.6833 - val_acc: 0.0051\n",
      "Epoch 84/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6825 - acc: 0.0053 - val_loss: 0.6832 - val_acc: 0.0051\n",
      "Epoch 85/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6825 - acc: 0.0053 - val_loss: 0.6832 - val_acc: 0.0051\n",
      "Epoch 86/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6824 - acc: 0.0053 - val_loss: 0.6832 - val_acc: 0.0051\n",
      "Epoch 87/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6824 - acc: 0.0053 - val_loss: 0.6831 - val_acc: 0.0051\n",
      "Epoch 88/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6823 - acc: 0.0053 - val_loss: 0.6831 - val_acc: 0.0051\n",
      "Epoch 89/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6823 - acc: 0.0053 - val_loss: 0.6831 - val_acc: 0.0051\n",
      "Epoch 90/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6823 - acc: 0.0053 - val_loss: 0.6831 - val_acc: 0.0051\n",
      "Epoch 91/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6823 - acc: 0.0053 - val_loss: 0.6831 - val_acc: 0.0051\n",
      "Epoch 92/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6823 - acc: 0.0053 - val_loss: 0.6831 - val_acc: 0.0051\n",
      "Epoch 93/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6822 - acc: 0.0053 - val_loss: 0.6831 - val_acc: 0.0051\n",
      "Epoch 94/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6822 - acc: 0.0053 - val_loss: 0.6831 - val_acc: 0.0051\n",
      "Epoch 95/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6822 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 96/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6822 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 97/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6822 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 98/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6821 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 99/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6821 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 100/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6821 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 101/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6821 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 102/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6821 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 103/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6821 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 104/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6821 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 105/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6821 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 106/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6820 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 107/200\n",
      "22500/22500 [==============================] - 1s 37us/step - loss: 0.6820 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6820 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 109/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6820 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 110/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6820 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 111/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6820 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 112/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6820 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 113/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6820 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 114/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6820 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 115/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6820 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 116/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6819 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 117/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6819 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 118/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6819 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 119/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6819 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 120/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6819 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 121/200\n",
      "22500/22500 [==============================] - 1s 37us/step - loss: 0.6819 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 122/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6819 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 123/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6819 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 124/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6819 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 125/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6819 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 126/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 127/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 128/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 129/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 130/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 131/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 132/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 133/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 134/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 135/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 136/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6818 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 137/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 138/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 139/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 140/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 141/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 142/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 143/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 144/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 145/200\n",
      "22500/22500 [==============================] - 1s 39us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 146/200\n",
      "22500/22500 [==============================] - 1s 43us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 147/200\n",
      "22500/22500 [==============================] - 1s 44us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 148/200\n",
      "22500/22500 [==============================] - 1s 45us/step - loss: 0.6817 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 149/200\n",
      "22500/22500 [==============================] - 1s 43us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 150/200\n",
      "22500/22500 [==============================] - 1s 44us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 151/200\n",
      "22500/22500 [==============================] - 1s 40us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 152/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 153/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 154/200\n",
      "22500/22500 [==============================] - 1s 37us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 155/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 156/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 157/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 158/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 159/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 160/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6816 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 161/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 162/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 163/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 164/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6830 - val_acc: 0.0051\n",
      "Epoch 165/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 167/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 168/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 169/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 170/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 171/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 172/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6815 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 173/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 174/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 175/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 176/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 177/200\n",
      "22500/22500 [==============================] - 1s 38us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 178/200\n",
      "22500/22500 [==============================] - 1s 40us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 179/200\n",
      "22500/22500 [==============================] - 1s 40us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 180/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 181/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 182/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 183/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 184/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 185/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6814 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 186/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 187/200\n",
      "22500/22500 [==============================] - 1s 37us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 188/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 189/200\n",
      "22500/22500 [==============================] - 1s 37us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 190/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 191/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 192/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 193/200\n",
      "22500/22500 [==============================] - 1s 34us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 194/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 195/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 196/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 197/200\n",
      "22500/22500 [==============================] - 1s 36us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 198/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6813 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 199/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6812 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n",
      "Epoch 200/200\n",
      "22500/22500 [==============================] - 1s 35us/step - loss: 0.6812 - acc: 0.0053 - val_loss: 0.6829 - val_acc: 0.0051\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, ELU\n",
    "\n",
    "input_size = len(x_train[0])\n",
    "\n",
    "input_layer = Input((input_size,))\n",
    "encoded = Dense(64)(input_layer)\n",
    "encoded = ELU(alpha=0.5)(encoded)\n",
    "encoded = Dense(32)(encoded)\n",
    "encoded = ELU(alpha=0.5)(encoded)\n",
    "decoded = Dense (64)(encoded)\n",
    "decoded = ELU(alpha=0.5)(decoded)\n",
    "decoded = Dense(input_size, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "autoencoder.summary()\n",
    "\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                          epochs=200,\n",
    "                          batch_size = 128,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "[[ 0.04516916 -0.11460132  0.00158543 ...  0.02157585  0.031204\n",
      "  -0.07058668]\n",
      " [-0.04159594 -0.19428807  0.07823841 ...  0.07592207 -0.05109053\n",
      "  -0.04396152]\n",
      " [-0.156869    0.04172282 -0.08458458 ...  0.06972968  0.07720609\n",
      "  -0.04586922]\n",
      " ...\n",
      " [ 0.02052808  0.02928666 -0.06108273 ...  0.04808288  0.01113593\n",
      "   0.06107713]\n",
      " [ 0.10532656  0.05753546  0.038265   ...  0.04207017 -0.04915527\n",
      "   0.02855334]\n",
      " [ 0.10043205 -0.06568845 -0.05911394 ...  0.08024327 -0.03173422\n",
      "   0.12120359]]\n",
      "LAYER 1\n",
      "[-5.30932844e-01  5.83524466e-01 -1.21704906e-01  6.15366042e-01\n",
      "  8.74747336e-01 -1.45123988e-01  5.81771135e-01  4.74385142e-01\n",
      " -3.70074093e-01 -3.00137043e-01  2.44794503e-01 -9.55730826e-02\n",
      "  1.04448423e-01  1.14490964e-01  3.88405323e-02  3.41689996e-02\n",
      "  3.66036922e-01 -3.35229516e-01  3.84234756e-01 -4.16607074e-02\n",
      "  8.13068986e-01 -6.88292980e-02  1.31359720e+00 -2.94336975e-01\n",
      "  2.30335414e-01 -1.53482407e-01 -2.36116841e-01  7.00363442e-02\n",
      "  1.24582894e-01  1.02526076e-01  9.51662660e-01  7.12794960e-01\n",
      "  4.60501462e-01  3.60617548e-01  7.40377724e-01  1.87917668e-02\n",
      " -4.28095996e-01 -2.17203304e-01 -2.43996844e-01  1.21521331e-01\n",
      "  3.67407411e-01 -4.61595133e-02  3.85511041e-01  1.11748695e+00\n",
      " -3.07142019e-01  1.83979243e-01 -5.21473289e-01  9.64591354e-02\n",
      " -1.36377607e-02 -3.03999245e-01  5.73756874e-01  4.35821444e-01\n",
      "  4.64460609e-04  4.34474684e-02  9.87273216e-01 -3.97347629e-01\n",
      " -3.76939863e-01  1.00275445e+00  6.21800184e-01  6.33427262e-01\n",
      "  7.10902214e-01  1.95326939e-01  5.62762618e-01 -1.16575956e-01]\n",
      "LAYER 2\n",
      "[[-0.2834422   0.27927136 -0.18679214 ... -0.15131888 -0.06059626\n",
      "  -0.16402169]\n",
      " [ 0.19259983  0.00229261 -0.00910762 ... -0.03672282  0.13815314\n",
      "   0.20617636]\n",
      " [-0.06792652 -0.1950419   0.09803274 ...  0.17909002  0.2581764\n",
      "   0.00326518]\n",
      " ...\n",
      " [-0.23365207  0.19421975  0.0429408  ... -0.07603253 -0.13391627\n",
      "  -0.05277714]\n",
      " [-0.01166774 -0.0155743  -0.06583874 ...  0.36321515 -0.07925409\n",
      "   0.11923454]\n",
      " [-0.13309897 -0.14006051  0.13685544 ...  0.20240305 -0.07937784\n",
      "  -0.21273771]]\n",
      "LAYER 3\n",
      "[ 0.3257036  -0.15058872  0.00468953 -0.14140514  0.04419537 -0.04375058\n",
      "  0.34357536  1.2743871   0.3250827   0.13913529  0.21624403  1.0715414\n",
      "  0.06670579  0.27005255  0.07835738 -0.03083513 -0.09908835  0.14518379\n",
      " -0.29942474 -0.03412853 -0.04959312  0.07427684  0.19591343  0.10286706\n",
      "  0.20996132  0.47393566  0.14616857 -0.14245206  0.04714691 -0.03274069\n",
      "  0.6924346   0.5986867 ]\n",
      "LAYER 4\n",
      "[[ 0.02240179  0.22695327  0.23166522 ... -0.23760812 -0.18354614\n",
      "   0.2665051 ]\n",
      " [ 0.47609022  0.03792948  0.17063707 ...  0.2904042   0.73156774\n",
      "  -0.01054072]\n",
      " [-0.10765406  0.15121946 -0.17537598 ... -0.20636876  0.05964546\n",
      "  -0.25421774]\n",
      " ...\n",
      " [-0.16585214  0.01032471  0.25584155 ...  0.14237918  0.20890956\n",
      "   0.3786848 ]\n",
      " [-0.46652415  0.01818178  0.41611087 ... -0.10068037 -0.6360312\n",
      "   0.35125223]\n",
      " [ 0.05314282  0.09962562 -0.01803419 ...  0.24445507 -0.25743926\n",
      "  -0.00700835]]\n",
      "LAYER 5\n",
      "[-0.09312225  0.07332232  0.39202186 -0.07707525  0.21010694 -0.04014212\n",
      "  0.2747724  -0.04242344 -0.03255794 -0.12548931 -0.07774689 -0.00607527\n",
      " -0.15194602 -0.08796323 -0.14522694 -0.04443917 -0.1595961   0.09522726\n",
      " -0.04297982  0.01163335  0.3955109  -0.06991378 -0.06940145  0.02566785\n",
      " -0.0330302  -0.06748866  0.32288533 -0.09755974 -0.04907012 -0.04903535\n",
      "  0.48249257 -0.09163635 -0.005226   -0.11885972  0.22257656 -0.10739598\n",
      " -0.01407679 -0.06569301 -0.09295096 -0.015986    0.14840288 -0.06494444\n",
      " -0.0563035   0.464553   -0.04521982  0.09156244 -0.08177529 -0.1308958\n",
      "  0.1795557  -0.04901891  0.09022047  0.05616054  0.03147516  0.30286267\n",
      "  0.19322452  0.12429512  0.35576788 -0.18125191 -0.04657039 -0.03868093\n",
      " -0.09068535 -0.03062585 -0.1705807   0.11965763]\n",
      "LAYER 6\n",
      "[[ 0.13681105  0.101236    0.08543155 ...  0.0150675   0.01781615\n",
      "   0.01086016]\n",
      " [ 0.04117994  0.04454222  0.04684929 ... -0.04421219 -0.11373363\n",
      "  -0.11689104]\n",
      " [-0.08405402 -0.12889758 -0.01480631 ... -0.07111786 -0.03013824\n",
      "  -0.07926152]\n",
      " ...\n",
      " [ 0.05742564 -0.0017135   0.05150305 ...  0.0828847  -0.04509132\n",
      "  -0.0736239 ]\n",
      " [-0.05345546 -0.04397105 -0.1520684  ...  0.06375059  0.10438203\n",
      "  -0.02100546]\n",
      " [-0.04385584  0.10514439  0.05983152 ... -0.02668437 -0.09374004\n",
      "  -0.08798961]]\n",
      "LAYER 7\n",
      "[-8.76693279e-02 -5.11590503e-02 -6.63414225e-02 -4.83631939e-02\n",
      " -4.34813723e-02 -7.41225332e-02 -1.04486257e-01 -1.07809283e-01\n",
      " -1.16497777e-01 -1.44797355e-01 -1.06788002e-01 -6.17048219e-02\n",
      " -1.36962041e-01 -1.16813049e-01 -9.83649939e-02 -1.31298020e-01\n",
      " -1.23969220e-01 -7.57298246e-02 -1.05687670e-01 -1.43096149e-01\n",
      " -1.02949992e-01 -1.08680911e-01 -1.52093142e-01 -1.21531792e-01\n",
      " -7.16548264e-02 -8.34248662e-02 -7.71890655e-02 -6.71235695e-02\n",
      " -5.41108847e-02 -6.79362491e-02 -7.34260529e-02 -8.76468495e-02\n",
      " -5.15602939e-02 -8.53765309e-02 -7.88947344e-02 -7.06497952e-02\n",
      " -5.46794906e-02 -1.09787486e-01 -8.19505453e-02 -6.83908984e-02\n",
      " -4.37082499e-02 -4.35940698e-02 -7.63206556e-02 -8.75748694e-02\n",
      " -6.24416992e-02 -8.86122808e-02 -6.07244074e-02 -6.36396334e-02\n",
      " -3.82912755e-02 -8.54750797e-02 -1.60975844e-01 -1.50988176e-01\n",
      " -1.59552991e-01 -1.20053507e-01 -1.12815447e-01 -6.19680211e-02\n",
      " -7.91523531e-02 -8.97787884e-02 -8.42644051e-02 -5.21334745e-02\n",
      " -4.52978574e-02 -4.43046838e-02 -7.45584583e-03 -5.99639453e-02\n",
      " -7.03962818e-02 -7.30338246e-02 -9.62133333e-02 -6.59855530e-02\n",
      " -4.01663482e-02 -3.65119278e-02 -2.74678767e-02 -3.58250402e-02\n",
      " -7.47244805e-02 -6.26844838e-02 -6.02633059e-02 -4.07270268e-02\n",
      " -3.30731682e-02 -4.42382097e-02 -6.69629276e-02 -7.11327493e-02\n",
      " -5.62409088e-02 -4.88258228e-02 -6.88082948e-02 -5.99348731e-02\n",
      " -9.87556353e-02 -1.30957231e-01 -9.21431929e-02 -1.08362481e-01\n",
      " -1.10631920e-01 -1.42983332e-01 -1.01397373e-01 -6.62347227e-02\n",
      " -5.34101427e-02 -3.84204015e-02 -5.28436750e-02 -9.91054177e-02\n",
      " -7.14958981e-02 -6.46795854e-02 -6.55480400e-02 -1.18583441e-01\n",
      " -7.57253170e-02 -2.93044969e-02 -5.86303473e-02 -6.38109818e-03\n",
      " -4.48710024e-02 -2.72996835e-02 -3.49895768e-02 -1.64224133e-02\n",
      " -3.67754325e-02 -5.08750640e-02 -3.19846161e-02 -4.90139201e-02\n",
      " -2.30520759e-02 -6.52152598e-02 -2.14110594e-02 -2.51700971e-02\n",
      " -2.27268320e-02 -7.07349703e-02 -3.28264162e-02 -5.35295606e-02\n",
      "  1.27108506e-04 -2.39480268e-02 -4.08409201e-02 -6.74648508e-02\n",
      " -4.55675460e-02 -8.94052386e-02 -5.95888868e-02 -1.50840310e-02\n",
      " -5.35172597e-02  1.07001681e-02 -2.50945650e-02 -6.95741847e-02\n",
      " -1.09169781e-01 -4.84438241e-02 -7.95535222e-02 -6.92723021e-02\n",
      " -7.73331672e-02 -8.43975544e-02 -5.67631945e-02 -4.69526052e-02\n",
      " -1.03050992e-01 -7.68150911e-02 -8.78757834e-02 -6.68724105e-02\n",
      " -6.05097003e-02 -3.32946181e-02 -4.14838716e-02 -7.10510015e-02\n",
      " -4.17378917e-02 -5.97468168e-02 -5.69473319e-02 -3.86334211e-02\n",
      " -7.12349489e-02 -3.44890393e-02 -1.17976844e-01 -1.19052030e-01\n",
      " -7.20835850e-02 -5.27586490e-02 -7.16463253e-02 -6.38932213e-02\n",
      " -1.02227889e-01 -9.31131095e-02 -3.93860377e-02 -6.97083399e-02\n",
      " -1.06422618e-01 -5.23132123e-02 -4.90159020e-02 -9.77552831e-02\n",
      " -4.06638458e-02 -8.43526721e-02 -3.50072719e-02 -6.40379786e-02\n",
      " -7.49194622e-02 -4.97330464e-02 -9.06866714e-02 -6.35122582e-02\n",
      " -4.96332459e-02 -5.61252944e-02 -5.52579872e-02 -3.19928192e-02\n",
      " -5.59006706e-02 -1.63977332e-02 -4.98732850e-02 -2.84836721e-02\n",
      " -6.18208945e-02 -8.04529265e-02 -5.23155667e-02 -3.35289091e-02\n",
      " -2.70544607e-02 -4.51670028e-02 -5.12387380e-02  2.79792603e-02\n",
      " -5.97261302e-02 -2.89423261e-02 -4.64185029e-02  1.95736010e-02\n",
      " -2.81498823e-02 -1.97889674e-02 -3.07699796e-02 -4.33580913e-02\n",
      " -5.57807460e-02 -3.65510285e-02 -3.60927060e-02 -6.98822588e-02\n",
      " -3.91478129e-02 -1.17023200e-01 -5.59523739e-02 -2.90599875e-02\n",
      " -8.40507597e-02 -5.99620789e-02 -7.21064061e-02 -1.04481041e-01\n",
      " -4.24785167e-02 -4.87509854e-02 -3.85531820e-02 -4.22694460e-02\n",
      " -4.89426181e-02 -5.23237064e-02 -3.10125854e-02 -2.65648831e-02\n",
      " -6.63399771e-02 -4.65718620e-02 -6.93424568e-02 -6.16856478e-02\n",
      " -4.17369679e-02 -6.95829093e-02 -6.01024516e-02 -1.42438896e-02\n",
      " -6.58059642e-02 -6.40307367e-02 -7.30631202e-02 -4.61724289e-02\n",
      " -6.35733679e-02 -3.30366977e-02 -6.92018270e-02 -4.13656421e-02\n",
      " -4.61008400e-02 -4.99231927e-02 -4.82166223e-02 -1.09927235e-02\n",
      " -6.36664107e-02 -9.10513401e-02 -5.46174385e-02 -3.92225310e-02\n",
      " -4.38016020e-02 -3.52651626e-02 -6.87785521e-02 -5.79755008e-02\n",
      " -5.19858263e-02 -2.67101545e-02 -5.20215370e-02 -4.08573225e-02\n",
      " -7.26085156e-02 -6.51556849e-02 -2.66207438e-02 -4.56200726e-02\n",
      " -6.32453710e-02 -3.84856947e-02 -9.75932553e-02 -6.09322600e-02\n",
      " -8.73081610e-02 -5.28548099e-02 -7.84088522e-02 -2.12624874e-02\n",
      " -6.17769733e-02 -6.14422262e-02 -1.34596843e-02 -9.32206586e-03\n",
      " -1.17852790e-02  2.10053902e-02 -5.89872040e-02 -3.20369303e-02\n",
      " -5.12696318e-02 -4.34475020e-02 -7.01381713e-02 -6.20596334e-02\n",
      " -5.17497249e-02 -3.54852453e-02 -4.26089242e-02 -4.06168960e-02\n",
      " -5.41269705e-02 -6.12053983e-02 -3.77326570e-02 -6.91825822e-02\n",
      " -1.36284474e-02 -2.13488378e-02 -8.06972664e-03 -5.13543263e-02\n",
      " -4.65990901e-02 -5.71547560e-02 -2.40966328e-03 -2.45697871e-02\n",
      " -8.13035015e-03 -2.28910241e-02 -4.36761826e-02 -2.18017995e-02\n",
      " -3.89253721e-02 -2.46973298e-02 -7.20176175e-02 -5.53847775e-02\n",
      " -4.21882086e-02 -5.86756365e-03 -3.65843587e-02 -2.36949231e-02\n",
      " -3.64009924e-02 -2.64318977e-02 -1.18744932e-02 -3.41388397e-02\n",
      " -3.28601710e-02 -6.49027526e-02 -2.62688026e-02 -1.01429084e-02\n",
      " -5.96576072e-02 -7.60115236e-02 -3.46312746e-02 -4.31035943e-02\n",
      " -4.81123179e-02 -2.22467892e-02 -2.42029727e-02 -6.41788077e-03\n",
      " -3.19715776e-02 -6.49000034e-02 -5.49350791e-02 -3.16240899e-02\n",
      " -3.62114869e-02 -3.40709202e-02 -4.23812382e-02 -2.46577673e-02\n",
      " -4.40458097e-02 -6.47179112e-02 -5.99994548e-02 -4.24808860e-02\n",
      " -3.80804054e-02 -5.20992801e-02 -3.29617187e-02 -7.75002614e-02\n",
      " -8.06128010e-02 -3.57787535e-02 -3.08965091e-02 -7.29992166e-02\n",
      " -5.28166406e-02 -5.23339510e-02 -8.97429287e-02 -4.71609719e-02\n",
      " -5.22313938e-02 -3.87204103e-02 -5.44805340e-02 -6.71560466e-02\n",
      " -9.55282077e-02 -3.24567854e-02 -5.40344305e-02 -4.54976596e-02\n",
      " -1.72537975e-02 -5.36048338e-02 -5.80707677e-02 -3.42963450e-02\n",
      " -7.40959272e-02 -7.30546936e-02 -5.71320280e-02 -6.48868382e-02\n",
      " -4.63319682e-02 -4.56394628e-02 -8.54632035e-02 -8.41202512e-02\n",
      " -4.73976657e-02 -6.83371425e-02 -4.95204292e-02 -6.09313622e-02\n",
      " -7.26043060e-02 -4.93140928e-02 -4.37358432e-02 -1.73857547e-02\n",
      " -4.64567468e-02 -1.69023648e-02 -6.27295226e-02 -7.76210725e-02\n",
      " -6.46267235e-02 -8.26310664e-02 -6.37432560e-02 -4.11967710e-02\n",
      " -4.06154431e-02 -1.96721796e-02 -2.82684322e-02  4.08892613e-03\n",
      " -6.98209256e-02 -2.44579371e-02 -4.80414666e-02 -7.63867572e-02\n",
      " -4.80735162e-03 -5.56331910e-02 -8.65469053e-02 -7.31204748e-02\n",
      " -5.73347360e-02 -5.36088608e-02 -5.07257320e-02 -4.75919135e-02\n",
      " -6.06675372e-02 -5.98517992e-03 -3.39152925e-02 -4.89872657e-02\n",
      " -3.99894677e-02 -5.64714558e-02 -6.19363002e-02 -3.79554555e-02\n",
      " -5.03668562e-02 -6.22156188e-02 -6.36278093e-02 -2.25795023e-02\n",
      " -7.23961443e-02 -4.78000902e-02 -4.36920188e-02 -4.31043543e-02\n",
      " -5.83028384e-02 -3.99213694e-02 -5.77963293e-02 -2.30194237e-02\n",
      " -5.61541542e-02 -3.49596404e-02 -4.38433588e-02 -2.46194061e-02\n",
      " -5.13504259e-02 -7.17971921e-02 -6.69069588e-02 -2.52231061e-02\n",
      " -4.30867635e-02 -6.23641610e-02 -6.46178424e-02 -4.40888293e-02\n",
      " -6.95741475e-02 -5.00340648e-02  4.92609758e-03 -8.64953548e-02\n",
      " -5.08410148e-02 -5.07167615e-02 -3.98376174e-02 -5.15505672e-02\n",
      " -3.52333859e-02 -4.94676307e-02 -7.96094611e-02 -6.17137514e-02\n",
      " -4.05447036e-02 -6.17993511e-02 -2.32812110e-02 -3.39507163e-02\n",
      " -3.37171182e-02 -3.99977639e-02 -3.92990112e-02 -6.63300976e-02\n",
      " -4.26132344e-02 -4.44603153e-02 -6.12196885e-02 -4.24805842e-02\n",
      " -5.32699451e-02 -5.22219390e-02 -3.80151272e-02 -4.88499776e-02\n",
      " -1.72656644e-02 -5.44971190e-02 -5.41755930e-02 -4.20359448e-02\n",
      " -8.73793662e-02 -3.43784951e-02 -7.65454471e-02 -5.50503768e-02\n",
      " -4.93404716e-02 -7.79996663e-02 -4.40133959e-02 -4.65685353e-02\n",
      " -6.21603690e-02 -6.56915903e-02 -7.35813305e-02 -4.64108549e-02\n",
      " -3.27491686e-02 -7.43032843e-02 -7.10642636e-02 -6.84515536e-02\n",
      " -6.60438910e-02 -5.42070307e-02 -6.65758476e-02 -5.33613786e-02\n",
      " -2.85746753e-02 -6.87752217e-02 -6.44031465e-02 -5.48674054e-02\n",
      " -7.37803355e-02 -2.41315924e-02 -5.39465025e-02 -2.27086414e-02\n",
      " -5.24523742e-02 -6.79411665e-02 -9.17655379e-02 -5.17258570e-02\n",
      " -4.29792851e-02 -4.11232077e-02 -4.30835746e-02 -5.48932925e-02\n",
      " -6.71320260e-02 -6.74474537e-02 -5.63047864e-02 -4.53638025e-02\n",
      " -4.01804075e-02 -9.42136422e-02 -7.83393234e-02 -5.85575625e-02\n",
      " -6.91466779e-02 -5.46651557e-02 -5.27604036e-02 -8.67113844e-02\n",
      " -6.82625845e-02 -2.13740245e-02 -5.46069033e-02 -3.99563462e-02]\n"
     ]
    }
   ],
   "source": [
    "weights = autoencoder.get_weights()\n",
    "for idx, weight in enumerate(weights):\n",
    "    print(\"LAYER {}\".format(idx))\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture --no-display \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up figure\n",
    "f = plt.figure(figsize=(12,5))\n",
    "f.add_subplot(1,2, 1)\n",
    "\n",
    "# plot accuracy as a function of epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "\n",
    "# plot loss as a function of epoch\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "file_autoencoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
