{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to grab features from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ktb3NKMBoSTv",
    "outputId": "253ee7d9-48e3-4dd7-e871-7734e573deaa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def feature_from_file(file_path, feature_type=\"head\", byte_num=512): # will add more feature_type later\n",
    "    \"\"\"Retreives features from a file.\n",
    "  \n",
    "    Parameters:\n",
    "    feature_type (str): \"head\" to get bytes from head of the file.\n",
    "    byte_num (int): Number of bytes to grab.\n",
    "    file_path (str): File path of file to get features from.\n",
    "    \n",
    "    Returns:\n",
    "    List of bytes from file_path. \n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        byte = f.read(1)\n",
    "        index = 1\n",
    "        features = []\n",
    "    \n",
    "        while byte and index <= byte_num:\n",
    "            features.append(byte)\n",
    "            index += 1\n",
    "            byte = f.read(1)\n",
    "        \n",
    "        if len(features) < byte_num:\n",
    "            features.extend([b'' for i in range(byte_num - len(features))])\n",
    "\n",
    "        assert len(features) == byte_num\n",
    "        return features\n",
    "\n",
    "def feature_from_dir(dir_path, feature_type=\"head\", byte_num=512):\n",
    "    \"\"\"Takes a directory and grabs features from each file.\n",
    "    \n",
    "    Parameters:\n",
    "    dir_path (str): Path of directory to take features from.\n",
    "    feature_type (str): Type of features to get.\n",
    "    byte_num (str): Number of features to take\n",
    "    \n",
    "    Return:\n",
    "    features (list): List containing a list of byte_num bytes from each fie in dir_path.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(dir_path):\n",
    "        for filename in filenames:\n",
    "            features.append(feature_from_file(os.path.join(dirpath, filename), feature_type, byte_num))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def translate_bytes(dir_features):\n",
    "    \"\"\"Translates bytes into integers.\n",
    "    \n",
    "    Parameter:\n",
    "    dir_features (list): List containing lists of bytes.\n",
    "    \n",
    "    Return:\n",
    "    translated_features (numpy array): dir_features with bytes translated to integers.\n",
    "    \"\"\"\n",
    "    translated_features = np.zeros((len(dir_features), len(dir_features[0])))\n",
    "    \n",
    "    for idx, file_features in enumerate(dir_features):\n",
    "        translated_features[idx] = np.array([int.from_bytes(c, byteorder=\"big\") for c in file_features])\n",
    "    \n",
    "    return translated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'S', b'O', b'C', b'A', b'T', b' ', b'd', b'a', b't', b'a', b' ', b'r', b'e', b'p', b'o', b'r', b't', b' ', b'c', b'r', b'e', b'a', b't', b'e', b'd', b':', b' ', b'2', b'0', b'1', b'5', b'-', b'1', b'0', b'-', b'2', b'8', b' ', b'1', b'9', b':', b'1', b'4', b' ', b'+', b'0', b'0', b'0', b'0', b'\\n', b'D', b'O', b'I', b' ', b'o', b'f', b' ', b't', b'h', b'e', b' ', b'e', b'n', b't', b'i', b'r', b'e', b' ', b'S', b'O', b'C', b'A', b'T', b' ', b'c', b'o', b'l', b'l', b'e', b'c', b't', b'i', b'o', b'n', b':', b' ', b'1', b'0', b'.', b'1', b'5', b'9', b'4', b'/', b'P', b'A', b'N', b'G', b'A', b'E', b'A', b'.', b'8', b'4', b'9', b'7', b'7', b'0', b'\\n', b' ', b' ', b' ', b' ', b'o', b'r', b' ', b's', b'e', b'e', b':', b' ', b'h', b't', b't', b'p', b':', b'/', b'/', b'd', b'o', b'i', b'.', b'p', b'a', b'n', b'g', b'a', b'e', b'a', b'.', b'd', b'e', b'/', b'1', b'0', b'.', b'1', b'5', b'9', b'4', b'/', b'P', b'A', b'N', b'G', b'A', b'E', b'A', b'.', b'8', b'4', b'9', b'7', b'7', b'0', b'\\n', b'S', b'O', b'C', b'A', b'T', b' ', b'c', b'r', b'u', b'i', b's', b'e', b' ', b'd', b'a', b't', b'a', b' ', b'i', b'n', b' ', b'S', b'O', b'C', b'A', b'T', b' ', b'r', b'e', b'g', b'i', b'o', b'n', b' ', b'\"', b'T', b'r', b'o', b'p', b'i', b'c', b'a', b'l', b' ', b'P', b'a', b'c', b'i', b'f', b'i', b'c', b'\"', b' ', b'f', b'o', b'r', b' ', b't', b'h', b'e', b' ', b'f', b'o', b'l', b'l', b'o', b'w', b'i', b'n', b'g', b' ', b'c', b'r', b'u', b'i', b's', b'e', b's', b':', b'\\n', b'E', b'x', b'p', b'o', b'c', b'o', b'd', b'e', b'\\t', b'v', b'e', b'r', b's', b'i', b'o', b'n', b'\\t', b'C', b'r', b'u', b'i', b's', b'e', b'/', b'D', b'a', b't', b'a', b's', b'e', b't', b' ', b'N', b'a', b'm', b'e', b'\\t', b'S', b'h', b'i', b'p', b'/', b'V', b'e', b's', b's', b'e', b'l', b' ', b'N', b'a', b'm', b'e', b'\\t', b'P', b'I', b'(', b's', b')', b'\\t', b'O', b'r', b'i', b'g', b'i', b'n', b'a', b'l', b' ', b'D', b'a', b't', b'a', b' ', b'D', b'O', b'I', b'\\t', b'O', b'r', b'i', b'g', b'i', b'n', b'a', b'l', b' ', b'D', b'a', b't', b'a', b' ', b'R', b'e', b'f', b'e', b'r', b'e', b'n', b'c', b'e', b'\\t', b'S', b'O', b'C', b'A', b'T', b' ', b'D', b'O', b'I', b'\\t', b'S', b'O', b'C', b'A', b'T', b' ', b'R', b'e', b'f', b'e', b'r', b'e', b'n', b'c', b'e', b'\\t', b'W', b'e', b's', b't', b'm', b'o', b's', b't', b' ', b'L', b'o', b'n', b'g', b'i', b't', b'u', b'd', b'e', b'\\t', b'E', b'a', b's', b't', b'm', b'o', b's', b't', b' ', b'L', b'o', b'n', b'g', b'i', b't', b'u', b'd', b'e', b'\\t', b'S', b'o', b'u', b't', b'h', b'm', b'o', b's', b't', b' ', b'L', b'a', b't', b'i', b't', b'u', b'd', b'e', b'\\t', b'N', b'o', b'r', b't', b'h', b'm', b'o', b's', b't', b' ', b'L', b'a', b't', b'i', b't', b'u', b'd', b'e', b'\\t', b'S', b't', b'a', b'r', b't', b' ', b'T', b'i', b'm', b'e', b'\\t', b'E', b'n', b'd', b' ', b'T', b'i', b'm', b'e', b'\\t', b'Q', b'C', b' ', b'F', b'l', b'a', b'g', b'\\t', b'A', b'd', b'd', b'i', b't', b'i', b'o', b'n', b'a', b'l', b' ', b'M', b'e', b't', b'a', b'd', b'a', b't', b'a', b' ', b'D', b'o', b'c', b'u', b'm', b'e', b'n', b't', b'(', b's', b')', b'\\n', b'0', b'6']\n",
      "[ 83.  79.  67.  65.  84.  32. 100.  97. 116.  97.  32. 114. 101. 112.\n",
      " 111. 114. 116.  32.  99. 114. 101.  97. 116. 101. 100.  58.  32.  50.\n",
      "  48.  49.  53.  45.  49.  48.  45.  50.  56.  32.  49.  57.  58.  49.\n",
      "  52.  32.  43.  48.  48.  48.  48.  10.  68.  79.  73.  32. 111. 102.\n",
      "  32. 116. 104. 101.  32. 101. 110. 116. 105. 114. 101.  32.  83.  79.\n",
      "  67.  65.  84.  32.  99. 111. 108. 108. 101.  99. 116. 105. 111. 110.\n",
      "  58.  32.  49.  48.  46.  49.  53.  57.  52.  47.  80.  65.  78.  71.\n",
      "  65.  69.  65.  46.  56.  52.  57.  55.  55.  48.  10.  32.  32.  32.\n",
      "  32. 111. 114.  32. 115. 101. 101.  58.  32. 104. 116. 116. 112.  58.\n",
      "  47.  47. 100. 111. 105.  46. 112.  97. 110. 103.  97. 101.  97.  46.\n",
      " 100. 101.  47.  49.  48.  46.  49.  53.  57.  52.  47.  80.  65.  78.\n",
      "  71.  65.  69.  65.  46.  56.  52.  57.  55.  55.  48.  10.  83.  79.\n",
      "  67.  65.  84.  32.  99. 114. 117. 105. 115. 101.  32. 100.  97. 116.\n",
      "  97.  32. 105. 110.  32.  83.  79.  67.  65.  84.  32. 114. 101. 103.\n",
      " 105. 111. 110.  32.  34.  84. 114. 111. 112. 105.  99.  97. 108.  32.\n",
      "  80.  97.  99. 105. 102. 105.  99.  34.  32. 102. 111. 114.  32. 116.\n",
      " 104. 101.  32. 102. 111. 108. 108. 111. 119. 105. 110. 103.  32.  99.\n",
      " 114. 117. 105. 115. 101. 115.  58.  10.  69. 120. 112. 111.  99. 111.\n",
      " 100. 101.   9. 118. 101. 114. 115. 105. 111. 110.   9.  67. 114. 117.\n",
      " 105. 115. 101.  47.  68.  97. 116.  97. 115. 101. 116.  32.  78.  97.\n",
      " 109. 101.   9.  83. 104. 105. 112.  47.  86. 101. 115. 115. 101. 108.\n",
      "  32.  78.  97. 109. 101.   9.  80.  73.  40. 115.  41.   9.  79. 114.\n",
      " 105. 103. 105. 110.  97. 108.  32.  68.  97. 116.  97.  32.  68.  79.\n",
      "  73.   9.  79. 114. 105. 103. 105. 110.  97. 108.  32.  68.  97. 116.\n",
      "  97.  32.  82. 101. 102. 101. 114. 101. 110.  99. 101.   9.  83.  79.\n",
      "  67.  65.  84.  32.  68.  79.  73.   9.  83.  79.  67.  65.  84.  32.\n",
      "  82. 101. 102. 101. 114. 101. 110.  99. 101.   9.  87. 101. 115. 116.\n",
      " 109. 111. 115. 116.  32.  76. 111. 110. 103. 105. 116. 117. 100. 101.\n",
      "   9.  69.  97. 115. 116. 109. 111. 115. 116.  32.  76. 111. 110. 103.\n",
      " 105. 116. 117. 100. 101.   9.  83. 111. 117. 116. 104. 109. 111. 115.\n",
      " 116.  32.  76.  97. 116. 105. 116. 117. 100. 101.   9.  78. 111. 114.\n",
      " 116. 104. 109. 111. 115. 116.  32.  76.  97. 116. 105. 116. 117. 100.\n",
      " 101.   9.  83. 116.  97. 114. 116.  32.  84. 105. 109. 101.   9.  69.\n",
      " 110. 100.  32.  84. 105. 109. 101.   9.  81.  67.  32.  70. 108.  97.\n",
      " 103.   9.  65. 100. 100. 105. 116. 105. 111. 110.  97. 108.  32.  77.\n",
      " 101. 116.  97. 100.  97. 116.  97.  32.  68. 111.  99. 117. 109. 101.\n",
      " 110. 116.  40. 115.  41.  10.  48.  54.]\n",
      "[0.3254902  0.30980392 0.2627451  0.25490196 0.32941176 0.1254902\n",
      " 0.39215686 0.38039216 0.45490196 0.38039216 0.1254902  0.44705882\n",
      " 0.39607843 0.43921569 0.43529412 0.44705882 0.45490196 0.1254902\n",
      " 0.38823529 0.44705882 0.39607843 0.38039216 0.45490196 0.39607843\n",
      " 0.39215686 0.22745098 0.1254902  0.19607843 0.18823529 0.19215686\n",
      " 0.20784314 0.17647059 0.19215686 0.18823529 0.17647059 0.19607843\n",
      " 0.21960784 0.1254902  0.19215686 0.22352941 0.22745098 0.19215686\n",
      " 0.20392157 0.1254902  0.16862745 0.18823529 0.18823529 0.18823529\n",
      " 0.18823529 0.03921569 0.26666667 0.30980392 0.28627451 0.1254902\n",
      " 0.43529412 0.4        0.1254902  0.45490196 0.40784314 0.39607843\n",
      " 0.1254902  0.39607843 0.43137255 0.45490196 0.41176471 0.44705882\n",
      " 0.39607843 0.1254902  0.3254902  0.30980392 0.2627451  0.25490196\n",
      " 0.32941176 0.1254902  0.38823529 0.43529412 0.42352941 0.42352941\n",
      " 0.39607843 0.38823529 0.45490196 0.41176471 0.43529412 0.43137255\n",
      " 0.22745098 0.1254902  0.19215686 0.18823529 0.18039216 0.19215686\n",
      " 0.20784314 0.22352941 0.20392157 0.18431373 0.31372549 0.25490196\n",
      " 0.30588235 0.27843137 0.25490196 0.27058824 0.25490196 0.18039216\n",
      " 0.21960784 0.20392157 0.22352941 0.21568627 0.21568627 0.18823529\n",
      " 0.03921569 0.1254902  0.1254902  0.1254902  0.1254902  0.43529412\n",
      " 0.44705882 0.1254902  0.45098039 0.39607843 0.39607843 0.22745098\n",
      " 0.1254902  0.40784314 0.45490196 0.45490196 0.43921569 0.22745098\n",
      " 0.18431373 0.18431373 0.39215686 0.43529412 0.41176471 0.18039216\n",
      " 0.43921569 0.38039216 0.43137255 0.40392157 0.38039216 0.39607843\n",
      " 0.38039216 0.18039216 0.39215686 0.39607843 0.18431373 0.19215686\n",
      " 0.18823529 0.18039216 0.19215686 0.20784314 0.22352941 0.20392157\n",
      " 0.18431373 0.31372549 0.25490196 0.30588235 0.27843137 0.25490196\n",
      " 0.27058824 0.25490196 0.18039216 0.21960784 0.20392157 0.22352941\n",
      " 0.21568627 0.21568627 0.18823529 0.03921569 0.3254902  0.30980392\n",
      " 0.2627451  0.25490196 0.32941176 0.1254902  0.38823529 0.44705882\n",
      " 0.45882353 0.41176471 0.45098039 0.39607843 0.1254902  0.39215686\n",
      " 0.38039216 0.45490196 0.38039216 0.1254902  0.41176471 0.43137255\n",
      " 0.1254902  0.3254902  0.30980392 0.2627451  0.25490196 0.32941176\n",
      " 0.1254902  0.44705882 0.39607843 0.40392157 0.41176471 0.43529412\n",
      " 0.43137255 0.1254902  0.13333333 0.32941176 0.44705882 0.43529412\n",
      " 0.43921569 0.41176471 0.38823529 0.38039216 0.42352941 0.1254902\n",
      " 0.31372549 0.38039216 0.38823529 0.41176471 0.4        0.41176471\n",
      " 0.38823529 0.13333333 0.1254902  0.4        0.43529412 0.44705882\n",
      " 0.1254902  0.45490196 0.40784314 0.39607843 0.1254902  0.4\n",
      " 0.43529412 0.42352941 0.42352941 0.43529412 0.46666667 0.41176471\n",
      " 0.43137255 0.40392157 0.1254902  0.38823529 0.44705882 0.45882353\n",
      " 0.41176471 0.45098039 0.39607843 0.45098039 0.22745098 0.03921569\n",
      " 0.27058824 0.47058824 0.43921569 0.43529412 0.38823529 0.43529412\n",
      " 0.39215686 0.39607843 0.03529412 0.4627451  0.39607843 0.44705882\n",
      " 0.45098039 0.41176471 0.43529412 0.43137255 0.03529412 0.2627451\n",
      " 0.44705882 0.45882353 0.41176471 0.45098039 0.39607843 0.18431373\n",
      " 0.26666667 0.38039216 0.45490196 0.38039216 0.45098039 0.39607843\n",
      " 0.45490196 0.1254902  0.30588235 0.38039216 0.42745098 0.39607843\n",
      " 0.03529412 0.3254902  0.40784314 0.41176471 0.43921569 0.18431373\n",
      " 0.3372549  0.39607843 0.45098039 0.45098039 0.39607843 0.42352941\n",
      " 0.1254902  0.30588235 0.38039216 0.42745098 0.39607843 0.03529412\n",
      " 0.31372549 0.28627451 0.15686275 0.45098039 0.16078431 0.03529412\n",
      " 0.30980392 0.44705882 0.41176471 0.40392157 0.41176471 0.43137255\n",
      " 0.38039216 0.42352941 0.1254902  0.26666667 0.38039216 0.45490196\n",
      " 0.38039216 0.1254902  0.26666667 0.30980392 0.28627451 0.03529412\n",
      " 0.30980392 0.44705882 0.41176471 0.40392157 0.41176471 0.43137255\n",
      " 0.38039216 0.42352941 0.1254902  0.26666667 0.38039216 0.45490196\n",
      " 0.38039216 0.1254902  0.32156863 0.39607843 0.4        0.39607843\n",
      " 0.44705882 0.39607843 0.43137255 0.38823529 0.39607843 0.03529412\n",
      " 0.3254902  0.30980392 0.2627451  0.25490196 0.32941176 0.1254902\n",
      " 0.26666667 0.30980392 0.28627451 0.03529412 0.3254902  0.30980392\n",
      " 0.2627451  0.25490196 0.32941176 0.1254902  0.32156863 0.39607843\n",
      " 0.4        0.39607843 0.44705882 0.39607843 0.43137255 0.38823529\n",
      " 0.39607843 0.03529412 0.34117647 0.39607843 0.45098039 0.45490196\n",
      " 0.42745098 0.43529412 0.45098039 0.45490196 0.1254902  0.29803922\n",
      " 0.43529412 0.43137255 0.40392157 0.41176471 0.45490196 0.45882353\n",
      " 0.39215686 0.39607843 0.03529412 0.27058824 0.38039216 0.45098039\n",
      " 0.45490196 0.42745098 0.43529412 0.45098039 0.45490196 0.1254902\n",
      " 0.29803922 0.43529412 0.43137255 0.40392157 0.41176471 0.45490196\n",
      " 0.45882353 0.39215686 0.39607843 0.03529412 0.3254902  0.43529412\n",
      " 0.45882353 0.45490196 0.40784314 0.42745098 0.43529412 0.45098039\n",
      " 0.45490196 0.1254902  0.29803922 0.38039216 0.45490196 0.41176471\n",
      " 0.45490196 0.45882353 0.39215686 0.39607843 0.03529412 0.30588235\n",
      " 0.43529412 0.44705882 0.45490196 0.40784314 0.42745098 0.43529412\n",
      " 0.45098039 0.45490196 0.1254902  0.29803922 0.38039216 0.45490196\n",
      " 0.41176471 0.45490196 0.45882353 0.39215686 0.39607843 0.03529412\n",
      " 0.3254902  0.45490196 0.38039216 0.44705882 0.45490196 0.1254902\n",
      " 0.32941176 0.41176471 0.42745098 0.39607843 0.03529412 0.27058824\n",
      " 0.43137255 0.39215686 0.1254902  0.32941176 0.41176471 0.42745098\n",
      " 0.39607843 0.03529412 0.31764706 0.2627451  0.1254902  0.2745098\n",
      " 0.42352941 0.38039216 0.40392157 0.03529412 0.25490196 0.39215686\n",
      " 0.39215686 0.41176471 0.45490196 0.41176471 0.43529412 0.43137255\n",
      " 0.38039216 0.42352941 0.1254902  0.30196078 0.39607843 0.45490196\n",
      " 0.38039216 0.39215686 0.38039216 0.45490196 0.38039216 0.1254902\n",
      " 0.26666667 0.43529412 0.38823529 0.45882353 0.42745098 0.39607843\n",
      " 0.43137255 0.45490196 0.15686275 0.45098039 0.16078431 0.03921569\n",
      " 0.18823529 0.21176471]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_set_dir = '/Users/ryan/Documents/CS/CDAC/official_xtract/sampler_dataset/pub8'\n",
    "\n",
    "raw_features = feature_from_dir(test_set_dir, byte_num=512)\n",
    "untranslated_features = translate_bytes(raw_features)\n",
    "x = untranslated_features / 255\n",
    "\n",
    "x_train, x_test, _, _ = train_test_split(x, x)\n",
    "\n",
    "print(raw_features[0])\n",
    "print(untranslated_features[0])\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adadelta\n",
    "sgd_optimizer = SGD(lr=1, decay=0.001)\n",
    "adadelta_optimizer = Adadelta(lr=0.1, decay=-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1007 21:44:26.125338 4751209920 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1007 21:44:26.135404 4751209920 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1007 21:44:26.138383 4751209920 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1007 21:44:26.173716 4751209920 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1007 21:44:26.186160 4751209920 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1007 21:44:26.189635 4751209920 deprecation.py:323] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1007 21:44:26.302307 4751209920 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               33280     \n",
      "=================================================================\n",
      "Total params: 70,304\n",
      "Trainable params: 70,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15321 samples, validate on 5107 samples\n",
      "Epoch 1/100\n",
      "15321/15321 [==============================] - 1s 43us/step - loss: 0.5516 - acc: 0.2935 - val_loss: 0.4254 - val_acc: 0.3369\n",
      "Epoch 2/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.4150 - acc: 0.3369 - val_loss: 0.4057 - val_acc: 0.3374\n",
      "Epoch 3/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.4029 - acc: 0.3376 - val_loss: 0.3969 - val_acc: 0.3380\n",
      "Epoch 4/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3977 - acc: 0.3379 - val_loss: 0.3939 - val_acc: 0.3381\n",
      "Epoch 5/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3950 - acc: 0.3380 - val_loss: 0.3923 - val_acc: 0.3382\n",
      "Epoch 6/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3931 - acc: 0.3381 - val_loss: 0.3905 - val_acc: 0.3384\n",
      "Epoch 7/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3915 - acc: 0.3382 - val_loss: 0.3890 - val_acc: 0.3384\n",
      "Epoch 8/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3901 - acc: 0.3383 - val_loss: 0.3879 - val_acc: 0.3384\n",
      "Epoch 9/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3890 - acc: 0.3383 - val_loss: 0.3871 - val_acc: 0.3385\n",
      "Epoch 10/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3882 - acc: 0.3384 - val_loss: 0.3864 - val_acc: 0.3385\n",
      "Epoch 11/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3874 - acc: 0.3384 - val_loss: 0.3858 - val_acc: 0.3385\n",
      "Epoch 12/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3868 - acc: 0.3385 - val_loss: 0.3851 - val_acc: 0.3386\n",
      "Epoch 13/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3862 - acc: 0.3385 - val_loss: 0.3846 - val_acc: 0.3386\n",
      "Epoch 14/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3856 - acc: 0.3385 - val_loss: 0.3840 - val_acc: 0.3386\n",
      "Epoch 15/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3851 - acc: 0.3386 - val_loss: 0.3835 - val_acc: 0.3387\n",
      "Epoch 16/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3845 - acc: 0.3386 - val_loss: 0.3831 - val_acc: 0.3387\n",
      "Epoch 17/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3841 - acc: 0.3387 - val_loss: 0.3827 - val_acc: 0.3388\n",
      "Epoch 18/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3837 - acc: 0.3387 - val_loss: 0.3824 - val_acc: 0.3387\n",
      "Epoch 19/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3832 - acc: 0.3387 - val_loss: 0.3819 - val_acc: 0.3388\n",
      "Epoch 20/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3829 - acc: 0.3388 - val_loss: 0.3816 - val_acc: 0.3389\n",
      "Epoch 21/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3825 - acc: 0.3388 - val_loss: 0.3813 - val_acc: 0.3389\n",
      "Epoch 22/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3822 - acc: 0.3388 - val_loss: 0.3810 - val_acc: 0.3389\n",
      "Epoch 23/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3819 - acc: 0.3388 - val_loss: 0.3808 - val_acc: 0.3389\n",
      "Epoch 24/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3815 - acc: 0.3388 - val_loss: 0.3803 - val_acc: 0.3389\n",
      "Epoch 25/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3812 - acc: 0.3388 - val_loss: 0.3801 - val_acc: 0.3389\n",
      "Epoch 26/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3810 - acc: 0.3388 - val_loss: 0.3798 - val_acc: 0.3389\n",
      "Epoch 27/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3807 - acc: 0.3388 - val_loss: 0.3796 - val_acc: 0.3389\n",
      "Epoch 28/100\n",
      "15321/15321 [==============================] - 0s 28us/step - loss: 0.3804 - acc: 0.3388 - val_loss: 0.3794 - val_acc: 0.3389\n",
      "Epoch 29/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3803 - acc: 0.3388 - val_loss: 0.3792 - val_acc: 0.3389\n",
      "Epoch 30/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3800 - acc: 0.3388 - val_loss: 0.3789 - val_acc: 0.3389\n",
      "Epoch 31/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3798 - acc: 0.3388 - val_loss: 0.3787 - val_acc: 0.3389\n",
      "Epoch 32/100\n",
      "15321/15321 [==============================] - 0s 27us/step - loss: 0.3796 - acc: 0.3388 - val_loss: 0.3785 - val_acc: 0.3389\n",
      "Epoch 33/100\n",
      "15321/15321 [==============================] - 0s 25us/step - loss: 0.3795 - acc: 0.3388 - val_loss: 0.3784 - val_acc: 0.3389\n",
      "Epoch 34/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3793 - acc: 0.3389 - val_loss: 0.3783 - val_acc: 0.3389\n",
      "Epoch 35/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3792 - acc: 0.3389 - val_loss: 0.3782 - val_acc: 0.3389\n",
      "Epoch 36/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3790 - acc: 0.3389 - val_loss: 0.3781 - val_acc: 0.3389\n",
      "Epoch 37/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3789 - acc: 0.3389 - val_loss: 0.3779 - val_acc: 0.3389\n",
      "Epoch 38/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3788 - acc: 0.3389 - val_loss: 0.3779 - val_acc: 0.3389\n",
      "Epoch 39/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3787 - acc: 0.3389 - val_loss: 0.3780 - val_acc: 0.3389\n",
      "Epoch 40/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3786 - acc: 0.3389 - val_loss: 0.3776 - val_acc: 0.3389\n",
      "Epoch 41/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3785 - acc: 0.3389 - val_loss: 0.3776 - val_acc: 0.3389\n",
      "Epoch 42/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3783 - acc: 0.3389 - val_loss: 0.3775 - val_acc: 0.3389\n",
      "Epoch 43/100\n",
      "15321/15321 [==============================] - 0s 27us/step - loss: 0.3782 - acc: 0.3389 - val_loss: 0.3774 - val_acc: 0.3389\n",
      "Epoch 44/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3781 - acc: 0.3389 - val_loss: 0.3773 - val_acc: 0.3389\n",
      "Epoch 45/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3780 - acc: 0.3389 - val_loss: 0.3772 - val_acc: 0.3389\n",
      "Epoch 46/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3780 - acc: 0.3389 - val_loss: 0.3771 - val_acc: 0.3389\n",
      "Epoch 47/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3779 - acc: 0.3389 - val_loss: 0.3772 - val_acc: 0.3389\n",
      "Epoch 48/100\n",
      "15321/15321 [==============================] - 0s 27us/step - loss: 0.3778 - acc: 0.3389 - val_loss: 0.3770 - val_acc: 0.3389\n",
      "Epoch 49/100\n",
      "15321/15321 [==============================] - 0s 27us/step - loss: 0.3777 - acc: 0.3389 - val_loss: 0.3769 - val_acc: 0.3389\n",
      "Epoch 50/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3776 - acc: 0.3389 - val_loss: 0.3768 - val_acc: 0.3389\n",
      "Epoch 51/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3775 - acc: 0.3389 - val_loss: 0.3768 - val_acc: 0.3389\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3774 - acc: 0.3389 - val_loss: 0.3769 - val_acc: 0.3389\n",
      "Epoch 53/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3774 - acc: 0.3389 - val_loss: 0.3768 - val_acc: 0.3389\n",
      "Epoch 54/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3773 - acc: 0.3389 - val_loss: 0.3767 - val_acc: 0.3389\n",
      "Epoch 55/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3773 - acc: 0.3389 - val_loss: 0.3766 - val_acc: 0.3389\n",
      "Epoch 56/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3772 - acc: 0.3389 - val_loss: 0.3765 - val_acc: 0.3389\n",
      "Epoch 57/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3771 - acc: 0.3389 - val_loss: 0.3765 - val_acc: 0.3389\n",
      "Epoch 58/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3771 - acc: 0.3389 - val_loss: 0.3764 - val_acc: 0.3389\n",
      "Epoch 59/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3770 - acc: 0.3389 - val_loss: 0.3763 - val_acc: 0.3389\n",
      "Epoch 60/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3769 - acc: 0.3389 - val_loss: 0.3763 - val_acc: 0.3389\n",
      "Epoch 61/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3769 - acc: 0.3389 - val_loss: 0.3762 - val_acc: 0.3389\n",
      "Epoch 62/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3768 - acc: 0.3389 - val_loss: 0.3762 - val_acc: 0.3390\n",
      "Epoch 63/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3768 - acc: 0.3389 - val_loss: 0.3761 - val_acc: 0.3389\n",
      "Epoch 64/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3767 - acc: 0.3389 - val_loss: 0.3761 - val_acc: 0.3389\n",
      "Epoch 65/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3766 - acc: 0.3389 - val_loss: 0.3760 - val_acc: 0.3389\n",
      "Epoch 66/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3765 - acc: 0.3389 - val_loss: 0.3760 - val_acc: 0.3389\n",
      "Epoch 67/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3765 - acc: 0.3389 - val_loss: 0.3759 - val_acc: 0.3390\n",
      "Epoch 68/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3765 - acc: 0.3389 - val_loss: 0.3759 - val_acc: 0.3389\n",
      "Epoch 69/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3764 - acc: 0.3389 - val_loss: 0.3759 - val_acc: 0.3389\n",
      "Epoch 70/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3763 - acc: 0.3389 - val_loss: 0.3758 - val_acc: 0.3390\n",
      "Epoch 71/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3764 - acc: 0.3389 - val_loss: 0.3758 - val_acc: 0.3389\n",
      "Epoch 72/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3762 - acc: 0.3389 - val_loss: 0.3758 - val_acc: 0.3390\n",
      "Epoch 73/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3762 - acc: 0.3389 - val_loss: 0.3758 - val_acc: 0.3390\n",
      "Epoch 74/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3763 - acc: 0.3389 - val_loss: 0.3757 - val_acc: 0.3389\n",
      "Epoch 75/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3762 - acc: 0.3389 - val_loss: 0.3757 - val_acc: 0.3389\n",
      "Epoch 76/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3761 - acc: 0.3389 - val_loss: 0.3757 - val_acc: 0.3390\n",
      "Epoch 77/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3761 - acc: 0.3389 - val_loss: 0.3756 - val_acc: 0.3389\n",
      "Epoch 78/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3760 - acc: 0.3389 - val_loss: 0.3756 - val_acc: 0.3390\n",
      "Epoch 79/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3759 - acc: 0.3389 - val_loss: 0.3755 - val_acc: 0.3389\n",
      "Epoch 80/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3759 - acc: 0.3389 - val_loss: 0.3755 - val_acc: 0.3390\n",
      "Epoch 81/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3759 - acc: 0.3389 - val_loss: 0.3754 - val_acc: 0.3389\n",
      "Epoch 82/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3758 - acc: 0.3389 - val_loss: 0.3754 - val_acc: 0.3390\n",
      "Epoch 83/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3758 - acc: 0.3389 - val_loss: 0.3754 - val_acc: 0.3390\n",
      "Epoch 84/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3758 - acc: 0.3389 - val_loss: 0.3754 - val_acc: 0.3389\n",
      "Epoch 85/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3757 - acc: 0.3389 - val_loss: 0.3753 - val_acc: 0.3390\n",
      "Epoch 86/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3757 - acc: 0.3389 - val_loss: 0.3753 - val_acc: 0.3390\n",
      "Epoch 87/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3756 - acc: 0.3389 - val_loss: 0.3753 - val_acc: 0.3389\n",
      "Epoch 88/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3756 - acc: 0.3389 - val_loss: 0.3753 - val_acc: 0.3390\n",
      "Epoch 89/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3757 - acc: 0.3389 - val_loss: 0.3753 - val_acc: 0.3389\n",
      "Epoch 90/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3756 - acc: 0.3389 - val_loss: 0.3753 - val_acc: 0.3390\n",
      "Epoch 91/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3755 - acc: 0.3389 - val_loss: 0.3752 - val_acc: 0.3390\n",
      "Epoch 92/100\n",
      "15321/15321 [==============================] - 0s 27us/step - loss: 0.3755 - acc: 0.3389 - val_loss: 0.3752 - val_acc: 0.3390\n",
      "Epoch 93/100\n",
      "15321/15321 [==============================] - 0s 27us/step - loss: 0.3755 - acc: 0.3389 - val_loss: 0.3752 - val_acc: 0.3389\n",
      "Epoch 94/100\n",
      "15321/15321 [==============================] - 1s 33us/step - loss: 0.3755 - acc: 0.3389 - val_loss: 0.3752 - val_acc: 0.3390\n",
      "Epoch 95/100\n",
      "15321/15321 [==============================] - 0s 29us/step - loss: 0.3754 - acc: 0.3389 - val_loss: 0.3751 - val_acc: 0.3390\n",
      "Epoch 96/100\n",
      "15321/15321 [==============================] - 0s 26us/step - loss: 0.3754 - acc: 0.3389 - val_loss: 0.3751 - val_acc: 0.3390\n",
      "Epoch 97/100\n",
      "15321/15321 [==============================] - 0s 29us/step - loss: 0.3753 - acc: 0.3389 - val_loss: 0.3752 - val_acc: 0.3390\n",
      "Epoch 98/100\n",
      "15321/15321 [==============================] - 0s 28us/step - loss: 0.3754 - acc: 0.3389 - val_loss: 0.3751 - val_acc: 0.3390\n",
      "Epoch 99/100\n",
      "15321/15321 [==============================] - 0s 27us/step - loss: 0.3753 - acc: 0.3389 - val_loss: 0.3750 - val_acc: 0.3390\n",
      "Epoch 100/100\n",
      "15321/15321 [==============================] - 0s 27us/step - loss: 0.3753 - acc: 0.3389 - val_loss: 0.3751 - val_acc: 0.3389\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "input_size = len(x_train[0])\n",
    "\n",
    "input_layer = Input((input_size,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense (64, activation='relu')(encoded)\n",
    "decoded = Dense(input_size, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "autoencoder.summary()\n",
    "\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                          epochs=100,\n",
    "                          batch_size = 256,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(x_test, x_test))\n",
    "\n",
    "print(K.eval(autoencoder.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "[[ 0.02770511  0.20840076  0.13901624 ... -0.22429916 -0.02655648\n",
      "   0.08965909]\n",
      " [-0.01120008  0.28830615  0.11001299 ...  0.09919403 -0.00479276\n",
      "   0.18426345]\n",
      " [-0.03292449 -0.02589399 -0.01674716 ...  0.09799673  0.09437383\n",
      "   0.05815312]\n",
      " ...\n",
      " [-0.08238504 -0.04559893  0.01158955 ... -0.07375364 -0.09019234\n",
      "   0.05264566]\n",
      " [-0.13604142 -0.10572207  0.21692961 ... -0.01700942  0.00631992\n",
      "  -0.08276074]\n",
      " [-0.15033086 -0.24469809 -0.16048017 ... -0.08871324  0.00324926\n",
      "  -0.01033799]]\n",
      "LAYER 1\n",
      "[ 0.33324665  0.16894658  0.36436534 -0.2820532   0.3112829  -0.34924823\n",
      "  0.66355133 -0.31188565  0.16667551  0.54794574  0.35951594  0.26423296\n",
      " -0.02087246  0.40922067  0.30514207  0.35802513  0.02284421 -0.30599037\n",
      " -0.31970474  0.55048656  0.23926361  0.09845064 -0.02014095  0.09782772\n",
      " -0.27656454  0.10414974  0.10038996 -0.0241366  -0.02669938  0.38402838\n",
      "  0.9365611   0.2565795   0.6793131   0.33182502  0.49004602  0.5530113\n",
      " -0.25547537  0.03878358  0.12784885  0.14432849  0.05826097  0.06372909\n",
      "  0.10030882  0.31705633  0.5465923  -0.11241557 -0.01998544  0.4502643\n",
      "  0.3340255  -0.00244498  0.03675816 -0.29153168  0.40176597 -0.28110543\n",
      "  0.30879176  0.33897945 -0.2693103   0.08097196  0.32123658  0.13142227\n",
      " -0.28974226 -0.36416414  0.27739024  0.20169124]\n",
      "LAYER 2\n",
      "[[-0.02833493 -0.03103281 -0.12780322 ...  0.24788415  0.2649779\n",
      "  -0.1195607 ]\n",
      " [ 0.29088882  0.18520737 -0.3645259  ...  0.05470109 -0.14423096\n",
      "   1.1036592 ]\n",
      " [ 0.05989286 -0.29662997 -0.04121801 ...  0.0233829  -0.0331183\n",
      "   0.3297193 ]\n",
      " ...\n",
      " [ 0.2276981  -0.2671497  -0.3409194  ...  0.16780628 -0.01325025\n",
      "  -0.01827005]\n",
      " [ 0.3194207  -0.00843342 -0.04642331 ...  0.2693041   0.09157458\n",
      "  -0.29560784]\n",
      " [ 0.16670981  0.09350177  0.24577653 ...  0.3793765   0.35243288\n",
      "   0.55421245]]\n",
      "LAYER 3\n",
      "[ 0.26535097  0.11749601  0.0617087   0.08045327  0.4809458   0.3762462\n",
      "  0.24922545  0.06279412  0.3612567  -0.24427304 -0.1762328   0.24521364\n",
      "  0.09591017 -0.18367742 -0.07061347  0.21175574 -0.3191266   0.1258947\n",
      "  0.11209156  0.23327796  0.31022274  0.36867085  0.00938656  0.33718237\n",
      "  0.5059842   0.12735075 -0.00737282  0.02242884  0.4695982   0.35974485\n",
      "  0.29038137 -0.16047405]\n",
      "LAYER 4\n",
      "[[ 0.00771083  0.3376627  -0.11006328 ... -0.15472174 -0.12659149\n",
      "  -0.21366993]\n",
      " [ 0.16930696 -0.44924852  0.22690946 ...  0.51196074  0.1516487\n",
      "   0.08560044]\n",
      " [ 0.15559547 -0.01302376 -0.0665746  ...  0.30829957 -0.05874338\n",
      "   0.33221278]\n",
      " ...\n",
      " [-0.68084586  0.15854242  0.015819   ...  0.12554093 -0.36969778\n",
      "   0.37794146]\n",
      " [-0.4425465  -0.12620068  0.21254644 ...  0.16291508 -0.26895592\n",
      "   0.23164864]\n",
      " [ 0.9815395  -0.8555535  -0.20042293 ... -0.49970338  0.40927827\n",
      "  -0.09460939]]\n",
      "LAYER 5\n",
      "[ 0.05795373  0.07566509  0.18223567  0.2604532   0.22451106  0.17231445\n",
      "  0.13708097 -0.12845457  0.05510379  0.26170626  0.1115572  -0.24881357\n",
      "  0.17284162 -0.07287229  0.23025936  0.18844365  0.06168227  0.24684307\n",
      "  0.19325085  0.23993775  0.15281203 -0.07132673 -0.00842335  0.09314795\n",
      "  0.00506235 -0.12119963  0.27417836 -0.23985355  0.17918248  0.18019633\n",
      "  0.2958688   0.09366792  0.13897972 -0.04576758 -0.08220185 -0.01522301\n",
      "  0.13044916  0.11499505  0.1451226  -0.16860394  0.21170361 -0.02084469\n",
      "  0.00842828 -0.00265314 -0.00238209  0.17074805 -0.23566508  0.12846082\n",
      "  0.11603416  0.28759047  0.15255858 -0.26849708  0.13535444  0.20296289\n",
      "  0.2277869  -0.00160065  0.15865229 -0.2596547  -0.25796556  0.15601155\n",
      " -0.17278469  0.23601487 -0.0513868   0.20122224]\n",
      "LAYER 6\n",
      "[[-0.002544    0.04408734  0.02420198 ...  0.18516374 -0.12457397\n",
      "  -0.1202854 ]\n",
      " [ 0.3180695   0.09313449  0.28853914 ...  0.24204332  0.56111336\n",
      "   0.23621929]\n",
      " [ 0.48129708  0.31388965  0.27429748 ...  0.19542669 -0.0241867\n",
      "   0.05512897]\n",
      " ...\n",
      " [-0.2396071  -0.18226914  0.01483862 ... -0.1898299  -0.16950223\n",
      "  -0.06457431]\n",
      " [ 0.13714446  0.09553848  0.20362532 ...  0.06040544  0.22797778\n",
      "   0.1605592 ]\n",
      " [-0.21504544 -0.03890384 -0.12839818 ... -0.08204995 -0.21250327\n",
      "  -0.03540716]]\n",
      "LAYER 7\n",
      "[-1.01129189e-01 -1.67620555e-01 -1.04732372e-01 -1.37359202e-01\n",
      " -3.10575336e-01 -2.48655856e-01 -1.50403112e-01 -2.55843818e-01\n",
      " -1.22172318e-01 -1.44646734e-01 -2.00299636e-01 -1.85281321e-01\n",
      " -2.24522546e-01 -1.52887851e-01 -1.30892888e-01 -2.28674278e-01\n",
      " -8.69615451e-02 -1.91719890e-01 -1.93814144e-01 -1.51871741e-01\n",
      " -2.31591240e-01 -1.09763868e-01 -2.54069477e-01 -2.45463610e-01\n",
      " -2.05197185e-01 -8.68460387e-02 -1.30324975e-01 -1.31227091e-01\n",
      " -1.64148241e-01 -9.18877870e-02 -6.72733113e-02 -9.63202491e-02\n",
      " -1.20562829e-01 -1.79490820e-01 -7.85536170e-02 -1.13209471e-01\n",
      " -5.08121476e-02 -8.15513581e-02 -3.21497843e-02 -4.84729372e-02\n",
      " -3.98595352e-03 -1.26794666e-01 -5.82951121e-02 -2.25727245e-01\n",
      " -1.78736255e-01 -2.82380313e-01 -1.21716894e-01 -1.66958481e-01\n",
      " -1.19153701e-01 -1.06645033e-01 -2.58541673e-01 -1.40761971e-01\n",
      " -1.11203790e-01 -3.40010911e-01 -2.41892904e-01 -1.17389277e-01\n",
      " -1.71642065e-01 -1.41079530e-01 -2.26372201e-02 -2.01314855e-02\n",
      " -6.31312141e-03  1.65552981e-02 -4.63814996e-02 -9.96347293e-02\n",
      " -1.14457898e-01 -1.40425265e-01 -1.69980839e-01 -1.10254884e-01\n",
      " -6.52813105e-05 -5.32298647e-02 -7.98063427e-02 -4.95312512e-02\n",
      " -3.05182990e-02 -4.02564406e-02 -5.46916910e-02 -7.32783154e-02\n",
      "  6.18554130e-02  1.14927031e-02 -6.07146174e-02 -1.28210425e-01\n",
      " -4.07215320e-02 -1.00697443e-01 -5.50286472e-02 -5.66139631e-02\n",
      " -2.12305576e-01 -2.30744615e-01 -1.75027966e-01 -1.26027510e-01\n",
      "  2.92997390e-01  1.36878997e-01 -1.04073361e-01 -2.79892888e-02\n",
      " -5.14691509e-03 -5.23240231e-02 -9.78964940e-02 -1.33340910e-01\n",
      " -8.07422251e-02 -1.93193212e-01 -9.61261615e-02 -2.17262328e-01\n",
      " -1.58639159e-02 -7.34286457e-02 -2.97686048e-02 -9.60757360e-02\n",
      " -5.07022813e-02 -1.21489823e-01 -9.32387486e-02 -9.64267552e-02\n",
      " -4.05216441e-02 -9.24662277e-02 -6.50309026e-02 -7.13587627e-02\n",
      " -7.66944587e-02 -7.66122490e-02 -7.80334696e-02 -9.21432003e-02\n",
      " -1.82450086e-01 -1.39292315e-01 -1.28260463e-01 -1.21307544e-01\n",
      " -1.20693214e-01 -1.61882624e-01 -3.46437767e-02 -1.41965836e-01\n",
      " -2.93229446e-02 -1.14172585e-01 -1.13915607e-01 -1.55485496e-01\n",
      " -8.39484334e-02 -8.99111181e-02 -9.98780057e-02 -1.66247070e-01\n",
      " -1.59433946e-01 -3.19771230e-01 -9.31220502e-02 -8.68917331e-02\n",
      " -1.26634866e-01 -6.55671433e-02 -5.52147962e-02 -5.04196174e-02\n",
      " -1.15107603e-01 -9.48182568e-02 -9.85901356e-02 -1.69137359e-01\n",
      " -2.07371905e-01  4.35315678e-03 -9.45969392e-03  4.03283630e-03\n",
      " -1.72923863e-01 -1.66966721e-01 -9.85783041e-02 -1.57973450e-02\n",
      " -9.24135074e-02 -5.25675975e-02  3.33399847e-02 -8.22480321e-02\n",
      " -3.34971435e-02 -4.06595804e-02  5.89696951e-02  1.89315110e-01\n",
      " -3.87185663e-02  5.22645302e-02  4.42741401e-02 -4.45772670e-02\n",
      "  6.44682869e-02 -8.72290730e-02 -4.89658723e-03  4.91213053e-02\n",
      " -8.81606862e-02 -1.64352030e-01 -7.52254874e-02  5.52036846e-03\n",
      " -9.35561806e-02 -4.67476919e-02 -1.22472964e-01 -4.16098610e-02\n",
      " -9.93527621e-02  9.76935029e-03 -5.04980907e-02 -4.14563864e-02\n",
      " -3.87973599e-02 -5.29298559e-02 -6.24614768e-02  8.15268140e-03\n",
      " -5.58990836e-02 -1.27779737e-01 -4.41579334e-02 -9.50936973e-03\n",
      " -3.10153067e-02 -8.77083540e-02 -7.52078071e-02 -1.15575910e-01\n",
      " -1.56508505e-01 -1.30783021e-01 -1.30501941e-01 -1.26192570e-01\n",
      " -9.03149024e-02 -9.62150618e-02 -1.11322761e-01 -7.23458603e-02\n",
      " -1.05260924e-01 -8.04298669e-02 -1.17348649e-01 -1.26519457e-01\n",
      " -1.49139792e-01 -6.29186034e-02 -8.42025280e-02 -6.84602708e-02\n",
      " -4.77446392e-02 -5.07854670e-02  8.29693154e-02 -4.68186429e-03\n",
      " -1.92886423e-02 -8.14303011e-02 -1.00594863e-01 -5.94542921e-02\n",
      " -1.69606373e-01 -8.50766338e-03 -1.35153174e-01 -6.15666844e-02\n",
      " -1.06343187e-01 -7.56525174e-02 -1.13087051e-01 -6.45042732e-02\n",
      " -1.07638426e-01 -8.78196284e-02 -1.07011609e-01 -8.92631710e-02\n",
      " -1.34345368e-01 -1.17037095e-01 -8.24941397e-02 -3.19652483e-02\n",
      " -1.09788142e-01 -5.13029657e-02 -6.37408495e-02 -6.95739761e-02\n",
      " -1.40893847e-01 -9.58106592e-02 -7.85918087e-02 -3.75452712e-02\n",
      " -7.06487000e-02 -1.08593009e-01 -5.47356233e-02 -1.01055101e-01\n",
      " -6.57229647e-02 -1.63513944e-02 -6.48034364e-02 -6.57118782e-02\n",
      " -1.67968556e-01 -1.19636320e-01 -1.55793548e-01 -3.02302632e-02\n",
      " -8.05830881e-02 -3.75334360e-02  3.66308540e-02 -2.77544595e-02\n",
      " -6.86808005e-02 -5.69787174e-02  1.18238628e-02 -6.18401095e-02\n",
      " -9.46846828e-02 -1.08458571e-01 -4.05024737e-02 -6.28978610e-02\n",
      " -7.85781518e-02 -3.45948823e-02 -7.76426941e-02 -1.05746925e-01\n",
      " -1.13923572e-01 -8.66359472e-02 -1.10010661e-01 -9.70489010e-02\n",
      " -1.19479336e-01 -1.09815910e-01 -8.30006972e-02 -8.24313685e-02\n",
      " -1.68653563e-01 -1.64236277e-01 -1.21047117e-01 -1.20218717e-01\n",
      " -7.56358430e-02 -6.60961941e-02 -7.26478696e-02 -8.07191953e-02\n",
      " -7.93312564e-02 -6.92787468e-02 -9.32821408e-02 -9.39629227e-02\n",
      " -4.62455675e-03 -5.07374071e-02 -1.16968481e-02 -1.28845610e-02\n",
      " -4.81608100e-02 -4.62622419e-02 -5.36374450e-02 -1.00864746e-01\n",
      " -1.29632443e-01 -3.11303623e-02 -4.06499915e-02  6.16587326e-03\n",
      " -1.45160407e-02 -2.36653332e-02 -4.79754433e-02 -2.14276761e-02\n",
      " -2.68433746e-02 -2.04694550e-02 -1.00314699e-01 -4.62484099e-02\n",
      "  1.01732137e-02 -4.67239991e-02 -2.17211209e-02 -6.72002658e-02\n",
      " -5.08862659e-02 -5.55522852e-02 -3.95704359e-02 -7.42447078e-02\n",
      " -7.32906014e-02 -4.47265543e-02 -9.02604163e-02 -8.68471861e-02\n",
      " -8.89154449e-02 -9.60724726e-02 -1.65162221e-01 -1.13959908e-01\n",
      " -1.09296739e-01 -1.42577738e-01 -1.41488805e-01 -1.11978628e-01\n",
      " -7.09355175e-02 -3.16019095e-02 -8.88235047e-02  1.55486772e-03\n",
      " -5.91175519e-02 -2.83002630e-02  4.97904606e-02  2.29289383e-02\n",
      " -6.45290837e-02 -2.38333084e-02 -9.56840739e-02 -3.91084738e-02\n",
      " -7.81441182e-02 -9.19835549e-03 -9.63547230e-02 -6.42636195e-02\n",
      " -5.75159341e-02 -9.08796266e-02 -7.53260031e-02 -1.11400768e-01\n",
      " -3.35648507e-02 -8.95578563e-02 -3.13755274e-02 -4.35942933e-02\n",
      " -1.33756474e-01 -6.80349320e-02 -7.24351183e-02 -1.26786590e-01\n",
      " -1.72932997e-01 -5.28167710e-02 -5.41702062e-02 -9.29022953e-02\n",
      " -4.18207198e-02  7.60237407e-03 -9.57393870e-02 -9.01076272e-02\n",
      " -7.47128427e-02 -1.90957543e-02 -1.24205658e-02 -4.31738235e-02\n",
      " -4.05843891e-02 -8.30705389e-02 -5.90871833e-02 -1.42824724e-01\n",
      " -7.21016228e-02 -6.77007958e-02 -6.18265085e-02 -9.64513496e-02\n",
      " -7.48574361e-02 -1.29600286e-01 -9.90837961e-02 -3.50953043e-02\n",
      " -5.08184657e-02 -9.63025019e-02 -7.32916147e-02 -1.37287691e-01\n",
      " -6.65967092e-02 -2.96608936e-02 -7.02108592e-02 -3.06244772e-02\n",
      "  3.03775351e-02 -7.56341517e-02 -1.28961261e-02 -4.56152968e-02\n",
      " -1.62622351e-02 -8.60796198e-02 -1.26211569e-01 -1.00249581e-01\n",
      " -7.98954740e-02 -5.64275719e-02 -1.21860035e-01 -1.50990307e-01\n",
      " -1.19200781e-01 -1.52404383e-01 -1.87089026e-01 -1.27249718e-01\n",
      " -7.87681863e-02 -1.00860059e-01 -2.74227690e-02 -1.27440039e-02\n",
      " -7.42765516e-02 -5.93765862e-02 -8.39657113e-02 -1.26172394e-01\n",
      " -5.19887507e-02 -8.56516585e-02 -2.31220685e-02 -3.88550165e-04\n",
      " -6.91277832e-02 -8.90759006e-02 -1.10110618e-01 -1.07066959e-01\n",
      " -1.28642857e-01 -8.49917382e-02 -9.81661081e-02 -6.33189902e-02\n",
      " -9.86909494e-02 -8.48080218e-02 -1.49790272e-01 -1.19408935e-01\n",
      " -9.56705436e-02 -8.89605284e-02 -9.67629068e-03 -4.72653322e-02\n",
      " -7.33824968e-02 -1.05735697e-01 -9.93845016e-02 -8.89001563e-02\n",
      " -1.34648740e-01 -6.99697286e-02 -6.49186224e-02 -7.57670924e-02\n",
      " -2.41337623e-02 -4.35335673e-02 -8.84274319e-02 -8.62355009e-02\n",
      " -1.05233923e-01 -9.42669362e-02 -8.79910588e-02 -1.46050289e-01\n",
      " -1.01688780e-01 -1.20107584e-01 -1.53480828e-01 -1.44785777e-01\n",
      " -9.11304131e-02 -1.15433291e-01 -8.51777121e-02 -7.55370855e-02\n",
      " -8.54623765e-02 -7.69469962e-02 -1.26353920e-01 -1.11382626e-01\n",
      " -9.46987197e-02 -1.12858608e-01 -3.79226319e-02 -5.47812991e-02\n",
      " -5.45728356e-02 -7.72809312e-02 -6.07615449e-02 -1.26576215e-01\n",
      " -9.47470963e-02 -4.97367382e-02 -8.85204300e-02 -4.65491414e-02\n",
      " -1.66113004e-02  2.20019603e-03 -3.58222760e-02  1.54300872e-02\n",
      " -3.29437479e-02 -8.11061189e-02 -1.27592057e-01 -6.89117536e-02\n",
      " -3.96284536e-02 -2.98429420e-03  7.27549940e-03 -7.68573210e-02\n",
      "  1.06892297e-02 -1.95395947e-02 -1.11527033e-01 -5.43808304e-02\n",
      " -1.12496629e-01 -6.29609302e-02 -6.59645051e-02 -5.84641844e-02\n",
      " -4.15977724e-02 -1.41605241e-02 -3.06039825e-02 -3.69081087e-02\n",
      " -5.24580292e-02 -8.48465413e-02 -6.65637329e-02 -1.01717912e-01\n",
      " -6.76609650e-02 -4.46891785e-02 -7.59126395e-02 -3.95844318e-02\n",
      " -1.27984762e-01 -3.71586420e-02 -1.00526214e-01 -1.10240191e-01\n",
      " -1.16450824e-01 -6.94181025e-02 -8.87571424e-02 -8.35724846e-02]\n"
     ]
    }
   ],
   "source": [
    "weights = autoencoder.get_weights()\n",
    "for idx, weight in enumerate(weights):\n",
    "    print(\"LAYER {}\".format(idx))\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('crappy_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAFNCAYAAADLm0PlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeZiU5ZX38e+pqqabpdlBENAmBmU1LC1qiI7iMrjhQuISNZKJGo2OZh0xk2g0ZsYYx8k4QQ0uWUyUEI1KRhTljWhIcGmQKIsKKMoiyA4iDVTVef94nmrKtmkK7Op6quv3ua66qGet00VbHE+d+77N3RERERERkU8vVugARERERERaCiXXIiIiIiJNRMm1iIiIiEgTUXItIiIiItJElFyLiIiIiDQRJdciIiIiIk1EybW0GGb2azO7Jcdzl5nZifmOSUREml5Tfd7vy31EcqXkWkRERESkiSi5FokYM0sUOgYRERHZP0qupVmFX899z8xeM7NtZna/mR1gZk+Z2VYzm2FmnbLOH2tmC8xsk5nNNLMBWceGmdnc8Lo/ABX1Xut0M5sXXvt3Mzs8xxhPM7NXzWyLmS03sx/VO/6F8H6bwuPjw/2tzey/zOxdM9tsZrPCfceZ2YoG3ocTw+c/MrNHzOx3ZrYFGG9mI81sdvga75vZL8ysVdb1g8zsWTPbYGZrzOz7ZtbDzD4ysy5Z5w03s7VmVpbLzy4i0lSK4fO+gZgvM7Ml4WfrVDM7MNxvZvbfZvZB+G/D62Y2ODx2qpktDGNbaWbf3a83TFoMJddSCOOAk4BDgTOAp4DvA90IfievATCzQ4GHgW+Gx6YBfzazVmGi+TjwINAZ+GN4X8JrhwEPAF8HugC/BKaaWXkO8W0DvgJ0BE4DrjSzs8L7HhzG+79hTEOBeeF1twMjgM+HMf0bkM7xPTkTeCR8zd8DKeBbQFfgaOAE4BthDJXADOBp4EDgs8D/c/fVwEzg3Kz7XgxMdvddOcYhItKUov55X8fMRgP/SfAZ2hN4F5gcHj4ZODb8OTqE56wPj90PfN3dK4HBwF/25XWl5VFyLYXwv+6+xt1XAn8FXnL3V929FngMGBaedx7wpLs/GyaHtwOtCZLXo4Ay4OfuvsvdHwFeyXqNy4FfuvtL7p5y998AO8LrGuXuM939dXdPu/trBB/4/xQe/jIww90fDl93vbvPM7MY8C/Ate6+MnzNv7v7jhzfk9nu/nj4mtvdfY67v+juSXdfRvCPRSaG04HV7v5f7l7r7lvd/aXw2G+AiwDMLA5cQPAPkohIIUT6876eC4EH3H1u+Nl9PXC0mVUBu4BKoD9g7r7I3d8Pr9sFDDSz9u6+0d3n7uPrSguj5FoKYU3W8+0NbLcLnx9IUDkAwN3TwHKgV3hspbt71rXvZj0/GPhO+BXhJjPbBPQJr2uUmR1pZs+F7RSbgSsIKsiE91jawGVdCb6mbOhYLpbXi+FQM/s/M1sdtor8Rw4xADxB8CHfl6BatNndX97PmEREPq1If97XUz+GDwmq073c/S/AL4CJwAdmNsnM2oenjgNOBd41s+fN7Oh9fF1pYZRcS5StIvjQBIKeN4IPzJXA+0CvcF/GQVnPlwM/cfeOWY827v5wDq/7EDAV6OPuHYB7gMzrLAcOaeCadUDtHo5tA9pk/Rxxgq89s3m97buBN4B+7t6e4GvU7Bg+01DgYTVoCkH1+mJUtRaR4lCoz/vGYmhL0GayEsDd73T3EcBAgvaQ74X7X3H3M4HuBO0rU/bxdaWFUXItUTYFOM3MTggH5H2H4Ku+vwOzgSRwjZmVmdk5wMisa+8Frgir0GZmbS0YqFiZw+tWAhvcvdbMRhK0gmT8HjjRzM41s4SZdTGzoWGV5QHgDjM70MziZnZ02PP3FlARvn4Z8ANgb72AlcAW4EMz6w9cmXXs/4CeZvZNMys3s0ozOzLr+G+B8cBYlFyLSHEo1Od9toeBr5rZ0PCz+z8I2liWmdkR4f3LCAomtUA67Am/0Mw6hO0sW8h9rI20UEquJbLc/U2CCuz/ElSGzwDOcPed7r4TOIcgidxA0K/3p6xra4DLCL7G2wgsCc/NxTeAm81sK3ADWVUId3+P4Ou/74SvOw/4XHj4u8DrBL2AG4CfAjF33xze8z6CCsg24GOzhzTguwRJ/VaCfzj+kBXDVoKWjzOA1cBi4Pis438j+HCf6+7ZX52KiERSAT/vs2OYAfwQeJSgWn4IcH54uD3BZ/FGgtaR9cDPwmMXA8vCFr4rCHq3pYTZx1uYRKQlMLO/AA+5+32FjkVERKSUKLkWaWHM7AjgWYKe8a2FjkdERKSUqC1EpAUxs98QzIH9TSXWIiIizU+VaxERERGRJqLKtYiIiIhIE1FyLSIiIiLSRBKFDqCpdO3a1auqqgodhojIfpkzZ846d6+/uFCLps9tESlWjX1mt5jkuqqqipqamkKHISKyX8ys5OYk1+e2iBSrxj6z1RYiIiIiItJE8ppcm9kYM3vTzJaY2YQGjl9hZq+b2Twzm2VmA+sdP8jMPjSz7+YzThERERGRppC35NrM4sBE4BRgIHBB/eSZYAW5Ie4+FLgNuKPe8TuAp/IVo4iIiIhIU8pnz/VIYIm7vw1gZpOBM4GFmRPcfUvW+W2Bukm3zews4B1gWx5jFBEREWkxdu3axYoVK6itrS10KC1CRUUFvXv3pqysLOdr8plc9wKWZ22vAI6sf5KZXQV8G2gFjA73tQOuA04C1BIiIiIikoMVK1ZQWVlJVVUVZlbocIqau7N+/XpWrFhB3759c76u4AMa3X2iux9CkEz/INz9I+C/3f3Dxq41s8vNrMbMatauXZvnSEVERESirba2li5duiixbgJmRpcuXfb5W4B8Vq5XAn2ytnuH+/ZkMnB3+PxI4ItmdhvQEUibWa27/yL7AnefBEwCqK6u1jruIiIiUvKUWDed/Xkv81m5fgXoZ2Z9zawVcD4wNfsEM+uXtXkasBjA3Y9x9yp3rwJ+DvxH/cRaRERERKJl06ZN3HXXXft83amnnsqmTZsaPeeGG25gxowZ+xtas8lb5drdk2Z2NTAdiAMPuPsCM7sZqHH3qcDVZnYisAvYCFySr3hEREREJL8yyfU3vvGNj+1PJpMkEntOO6dNm7bXe998882fOr7mkNcVGt19GjCt3r4bsp5fm8M9ftT0kZUod/A0pFPBn54Knu8+ITzmwXMMzMJrkuGx1O7rITzukNoFqZ3BfjOwWNYjHuzLiJdBvFVw/2Rt8EjtCu7t6WB/LB5cRxize/ja6fAcD/Zlv1YmXrPwvNTuazP3SSeDh8WCGGJl4f5d4f44xBLB8eR22FUb7I8lgphi8eCaeFnwPqR2BH9aHOLhdamdkNwJnsIx0hYPXyOMPXw4jhPDLR5cF/5c7o6nd/+MMcLzLUbaEqSJB/dOJ4O/NYvjseA/ZU/tgtQuPJ0m7Wk87RBPYPEyLJbA0yk8HfwdmjnmHv7VGAZ4+Hvhng7eNsL3N1EevF/xBJ523FOQSmGpHcHPi+GxBB6LgQf3DX6W4HfEMczTwX2DXxw88zvhjoXnuoPjmMWC9xrDPBm8d5n74Xjd7xXgaSy9Kzgn/HsyiwU/yyd+/x0IfzaLBffBMNKYp4M/CQaxEP7p7runMcKCa2IxjPA1Mv8Nhb8f5QccSrf+o/bhP8zmZ2ZjgP8hKHzc5+631js+HvgZu1v5fuHu94XHUsDr4f733H1suL8vQXtfF2AOcLG772zKuJet28bfl67n9M/1pH1F7iP3RaT5TJgwgaVLlzJ06FDKysqoqKigU6dOvPHGG7z11lucddZZLF++nNraWq699louv/xyYPeKrR9++CGnnHIKX/jCF/j73/9Or169eOKJJ2jdujXjx4/n9NNP54tf/CJVVVVccskl/PnPf2bXrl388Y9/pH///qxdu5Yvf/nLrFq1iqOPPppnn32WOXPm0LVr12Z7D1rM8uct3rZ1sGY+fLAoSMLadsdbdyK54V2S778O6xbj2zfBzg+x5A6SlmAXZeBJypMfUp7cSsJ3FfqnKDlGkL1I6Xi58xmRTq6z1iA4iWAWp1fMbKq7L6x36h/c/eoGbrE9XJugvp8SDESfbGb3AF9j9ziaJvGPFZv4/mOvc9RnOiu5FomoW2+9lfnz5zNv3jxmzpzJaaedxvz58+tm23jggQfo3Lkz27dv54gjjmDcuHF06dLlY/dYvHgxDz/8MPfeey/nnnsujz76KBdddNEnXqtr167MnTuXu+66i9tvv5377ruPm266idGjR3P99dfz9NNPc//99zfLz51NyXVzSqehdhN8tD54bF4BG5fBpvfAjHSiNdtTcdLb1sGHH2AfrSW2fQNlOzZQltr+idsZUAZs99Ys8V5s9Eq20ZMdXkaZJSkjSZoYm70tW2nDDlqFVV4Lq3tx3OJ1zfpm1FWMYwaJmBE3woppjDTxoAprhhMjZhALK8XpWCvcEkE10gFSxNyJmRMnjVlQGY0ZJEiS8CQxc5LWil1WTjqWCKvGMQwn5mlipOoqi2CkwyqvY1gshsWM4KgT81RQtAbM06Rtd0XYMCwW1CSTnmCXG4YH75EHVeyUJUgRJ06KOCkMJxkrJxnGlrA0CdLEPEXMk1g6SZIYu0iQ9Bhl5pTFUsRxUlbGLkvgFidukCAdVkkNCN5LD4c7xGNBZTrm6WCfWfCdgcUwC2JOW4y0G0HNOk3cgkq5Wyx4T3HihFXsWAK3MiweJ2YWvO+ehOSuoFodj0GsDDfDPUZQD7awIpwG4ljM6t43zDFPEUvtwtI7sXQqOE4MjyVIx8rweDmGY54k5qm63w8neJ+DvxcPrjELtsNqdeZ3zwljtRgxPKycpyDteCye9XcZ/vqG1X8jjVsi/L2Mh9eE37JkqstZ35rU/a6H9zFP4WknHf5epc3CoxCzWPDfhAW/60Gg6d3fQpDGwv+OgmPB63ar949EBO11DYJ9ZcEbOxr4crjrNwSzPjVpch0L//5SaY1fF8nFTX9ewMJVW/Z+4j4YeGB7bjxjUM7njxw58mPT2N1555089thjACxfvpzFixd/Irnu27cvQ4cG/w8/YsQIli1b1uC9zznnnLpz/vSnPwEwa9asuvuPGTOGTp065RxrU1FyvT+SO2HDUtj4bpAY49CmC1R0DPavnIt/sJBUchdJN9LJXcRrN1C2c1OQfNSzyTqQdKPcd9CKJBtpxzrvwDrvwHr6stEPZzVdWV1xCJvb96Nj2zb0Kf+Qnq224ZW9oUNv2lWU0aF1GT1bl9E+fN6+dYKKRDzslNDIYREBclyDABhnZscCbwHfcvfMNRVmVgMkgVvd/XGCVpBN7p7Mumevpg48Ef5fTlLJtUjRaNu2bd3zmTNnMmPGDGbPnk2bNm047rjjGpzmrry8vO55PB5n+/ZPFhizz4vH4ySTyQbPKQQl17nasRXeeQEWToW3noLazXs8dR0dmZ8+mFpvR4w0Kdqy0fuwnvZs9Eo+SnRkV0Vntld0Z2tFLxIVbenarpweHSroXllOu4oErcsStCtPMKiyFV3altO5bSviMSXIItIs/gw87O47zOzrBJXo0eGxg919pZl9BviLmb0O7PkDsR4zuxy4HOCggw7ap6Ayn4GqXIvkZl8qzE2lsrKSrVu3Nnhs8+bNdOrUiTZt2vDGG2/w4osvNvnrjxo1iilTpnDdddfxzDPPsHHjxiZ/jb1Rct2Qndvggzdg/WL4YCEs+xu+6lXMU9Qm2vNq66N5OjmAf2zrzErvSpIYfco/ol/lTuh4MK069aJbZQVdK8vrEuPPtG1FpzZldGzTilaJgq/dIyKla69rELj7+qzN+4Dbso6tDP9828xmAsOAR4GOZpYIq9d7XNfg06xPkIgruRaJui5dujBq1CgGDx5M69atOeCAA+qOjRkzhnvuuYcBAwZw2GGHcdRRRzX56994441ccMEFPPjggxx99NH06NGDysrKJn+dxii5rm/NAvj1abA9+D+dlMVZnDiMv6TG8tfkQF6pPYweZZUMO6QTp/fuwOG9O3LoAe3o0LpMrRciUgzq1iAgSIDPZ3evNABm1tPd3w83xwKLwv2dgI/CinZXYBRwm7u7mT0HfJFgxpBLgCeaOvBMz7XaQkSi7aGHHmpwf3l5OU899VSDxzJ91V27dmX+/Pl1+7/73e/WPf/1r3/9ifMBqqurmTlzJgAdOnRg+vTpJBIJZs+ezSuvvPKxNpPmoOQ626b34Hfj8ERrZo+4gZ/NNRbWdqZ/ry4cUdWZS6o68z8Hd6R7ZUWhIxUR2S85rkFwjZmNJeir3gCMDy8fAPzSzNIEi5DdmjXLyHXAZDO7BXgVaPIh+olY8K1f2pVci0jD3nvvPc4991zS6TStWrXi3nvvbfYYlFxnbFsPD56D7/qI73e4jYf/VsmIgzvxxNmD6d+jfaGjExFpMjmsQXA9cH0D1/0dGLKHe75NMBNJ3mR6rpMpJdci0rB+/frx6quvFjQGJdcQDFZ86Ev4pveY2Pt2Hn6zkpvGDuLiow4mpkGEIiKRoJ5rESkGGlm38yN46HxYNY+Zh9/G7W924ZoT+nHJ56uUWIuIRMjunuv0Xs4UESmc0k6ukztgysXw7t94+9g7uOyl7hx/WDe+eUK/QkcmIiL1ZOa5Vs+1iERZaSfXT34blsyAM37OhLcO44D2Ffz8vGGqWIuIRJB6rkWkGJR2cn3kFXDaHTBiPFtrkwzoWUmHNmWFjkpERBqgnmuRlqddu3YArFq1ii9+8YsNnnPcccdRU1PT6H1+/vOf89FHH9Vtn3rqqWzatKnpAt0HpZ1c9xgCR3wNgHTa6/r5REQkeuKa51qkxTrwwAN55JFH9vv6+sn1tGnT6NixY1OEts9KO7nOknLX8uIiIhEWV8+1SORNmDCBiRMn1m3/6Ec/4pZbbuGEE05g+PDhDBkyhCee+OQaU8uWLWPw4MEAbN++nfPPP58BAwZw9tlns3379rrzrrzySqqrqxk0aBA33ngjAHfeeSerVq3i+OOP5/jjjwegqqqKdevWAXDHHXcwePBgBg8ezM9//vO61xswYACXXXYZgwYN4uSTT/7Y63waSq5D6bSr11pEJMIyi8io51okus477zymTJlStz1lyhQuueQSHnvsMebOnctzzz3Hd77zHbyR/0m+++67adOmDYsWLeKmm25izpw5dcd+8pOfUFNTw2uvvcbzzz/Pa6+9xjXXXMOBBx7Ic889x3PPPfexe82ZM4df/epXvPTSS7z44ovce++9dfNgL168mKuuuooFCxbQsWNHHn300SZ5DzTPdSjlXveVo4iIRE9cPdci++apCbD69aa9Z48hcMqtezw8bNgwPvjgA1atWsXatWvp1KkTPXr04Fvf+hYvvPACsViMlStXsmbNGnr06NHgPV544QWuueYaAA4//HAOP/zwumNTpkxh0qRJJJNJ3n//fRYuXPix4/XNmjWLs88+m7Zt2wJwzjnn8Ne//pWxY8fSt29fhg4dCsCIESM+tqT6p6HkOpRKqy1ERCTK1HMtUhy+9KUv8cgjj7B69WrOO+88fv/737N27VrmzJlDWVkZVVVV1NbW7vN933nnHW6//XZeeeUVOnXqxPjx4/frPhnl5eV1z+PxeJO1hSi5DmlAo4hItGUKICn1XIvkppEKcz6dd955XHbZZaxbt47nn3+eKVOm0L17d8rKynjuued49913G73+2GOP5aGHHmL06NHMnz+f1157DYAtW7bQtm1bOnTowJo1a3jqqac47rjjAKisrGTr1q107dr1Y/c65phjGD9+PBMmTMDdeeyxx3jwwQfz8nNnKLkOBQMaCx2FiIjsSWYRmVRKKzSKRNmgQYPYunUrvXr1omfPnlx44YWcccYZDBkyhOrqavr379/o9VdeeSVf/epXGTBgAAMGDGDEiBEAfO5zn2PYsGH079+fPn36MGrUqLprLr/8csaMGVPXe50xfPhwxo8fz8iRIwG49NJLGTZsWJO1gDTEGmsoLybV1dW+tzkQG73+lhmcNLA7/3nOnvt2RETyxczmuHt1oeNoTvv6ub2ldheH/+gZfnDaAC495jN5jEykeC1atIgBAwYUOowWpaH3tLHPbNVqQ2lNxSciEmmZnmsNaBSRKFNyHUqlNVuIiEiUqedaRIqBkuuQ5rkWEYm23T3XSq5FJLqUXIc0z7WISLRlKteaik+kcS1lPF0U7M97qeQ6pHmuRUSizcyImXquRRpTUVHB+vXrlWA3AXdn/fr1VFRU7NN1moovlHa1hYiIRF0iFlPPtUgjevfuzYoVK1i7dm2hQ2kRKioq6N279z5do+Q6pAGNIiLRF4+ZKtcijSgrK6Nv376FDqOkqS2EoOyfdlS5FhGJuHjMSGpAo4hEmJJrdvfvqXItIhJtQeVaKzSKSHQpuWb3nKla/lxEJNoSMVPPtYhEmtJJIFMEUVuIiEi0qedaRKJOyTVZlWu1hYiIRJp6rkUk6pRck9Vzrcq1iEikqXItIlGn5Jpg6XOAmCrXIlICzGyMmb1pZkvMbEIDx8eb2Vozmxc+Lg33DzWz2Wa2wMxeM7Pzsq75tZm9k3XN0HzErp5rEYk6zXNN9oBGJdci0rKZWRyYCJwErABeMbOp7r6w3ql/cPer6+37CPiKuy82swOBOWY23d03hce/5+6P5DP+eMy0/LmIRJoq12RVrpVci0jLNxJY4u5vu/tOYDJwZi4Xuvtb7r44fL4K+ADolrdIGxCPGSn1XItIhCm5RgMaRaSk9AKWZ22vCPfVNy5s/XjEzPrUP2hmI4FWwNKs3T8Jr/lvMytv0qhD8VhMlWsRiTQl12QPaCxwICIi0fBnoMrdDweeBX6TfdDMegIPAl9198yKLtcD/YEjgM7AdQ3d2MwuN7MaM6tZu3btPgeWiBlp9VyLSIQpnSRrnmtVrkWk5VsJZFeie4f76rj7enffEW7eB4zIHDOz9sCTwL+7+4tZ17zvgR3ArwjaTz7B3Se5e7W7V3frtu8dJeq5FpGoU3KNBjSKSEl5BehnZn3NrBVwPjA1+4SwMp0xFlgU7m8FPAb8tv7Axcw1ZmbAWcD8fASv5c9FJOo0Wwia51pESoe7J83samA6EAcecPcFZnYzUOPuU4FrzGwskAQ2AOPDy88FjgW6mFlm33h3nwf83sy6AQbMA67IR/xaREZEok7JNdT176ktRERKgbtPA6bV23dD1vPrCXqo61/3O+B3e7jn6CYOs0GJmLErpcq1iESX2kJQ5VpEpFio51pEok7JNbuTa1WuRUSiTcufi0jU5TW5zmGJ3SvM7PVwqdxZZjYw3D8yawndf5jZ2fmMM60BjSIiRSGh5FpEIi5vyXXWErunAAOBCzLJc5aH3H2Iuw8FbgPuCPfPB6rD/WOAX5pZ3vrDNc+1iEhxUOVaRKIun+nkXpfYdfctWZttAQ/3f+TuyXB/RWZ/vmhAo4hIcUhohUYRibh8zhbS0BK7R9Y/ycyuAr5NsIzu6Kz9RwIPAAcDF2cl200uM/BcbSEiItEWU+VaRCKu4I0Q7j7R3Q8hWCr3B1n7X3L3QQRL6V5vZhX1r/20y+hm1LWFqHItIhJp6rkWkajLZ3K91yV265lMsKrXx7j7IuBDYHADxz7VMroZdW0hqlyLiESaeq5FJOrymVznssRuv6zN04DF4f6+mQGMZnYw0B9Ylq9ANc+1iEhxSMSMpJY/F5EIy1vPdY5L7F5tZicCu4CNwCXh5V8AJpjZLiANfMPd1+Ur1pSm4hMRKQrquRaRqMvr8uc5LLF77R6uexB4MJ+xZUur51pEpCio51pEoq7gAxqjQG0hIiLFQcufi0jUKblG81yLiBQLVa5FJOqUXKN5rkVEikVMlWsRiTgl12QPaCxwICIi0qhEzOrGyYiIRJHSSXYPaFRbiIhItMXD5c/dlWCLSDQpuUYDGkVEikVmVicVr0UkqpRcs7stRJVrEZFoS8SDz2ktJCMiUaXkmqx5rlW5FhGJtMzntHJrEYkqJddohUYRkWKRiKlyLSLRpuSa3T3XagsREYm2zOe05roWkahSco0GNIqIFIvdPddKrkUkmpRck5Vcq3ItIhJpu3uulVyLSDQpuSZr+XO9GyIikba751rJtYhEk9JJtPy5iEixUM+1iESdkmuyKtdqCxERiTT1XItI1Cm5RgMaRaS0mNkYM3vTzJaY2YQGjo83s7VmNi98XJp17BIzWxw+LsnaP8LMXg/veadZfqoV8bB/T5VrEYkqJddoQKOIlA4ziwMTgVOAgcAFZjawgVP/4O5Dw8d94bWdgRuBI4GRwI1m1ik8/27gMqBf+BiTj/gzPddKrkUkqpRckz2gUcm1iLR4I4El7v62u+8EJgNn5njtPwPPuvsGd98IPAuMMbOeQHt3f9HdHfgtcFY+gs+072kRGRGJKiXXBBUQtYSISInoBSzP2l4R7qtvnJm9ZmaPmFmfvVzbK3y+t3t+aqpci0jUKbkmWP5cLSEiInX+DFS5++EE1enfNNWNzexyM6sxs5q1a9fu8/XxuJJrEYk2JdcEixFojmsRKRErgT5Z273DfXXcfb277wg37wNG7OXaleHzPd4z696T3L3a3au7deu2z8Grci0iUaeUkmCea1WuRaREvAL0M7O+ZtYKOB+Ymn1C2EOdMRZYFD6fDpxsZp3CgYwnA9Pd/X1gi5kdFc4S8hXgiXwEHzdNxSci0ZYodABRkHbXYEYRKQnunjSzqwkS5TjwgLsvMLObgRp3nwpcY2ZjgSSwARgfXrvBzH5MkKAD3OzuG8Ln3wB+DbQGngofTS6uyrWIRJySazSgUURKi7tPA6bV23dD1vPrgev3cO0DwAMN7K8BBjdtpJ+UUM+1iESc2kLQgEYRkWKhRWREJOqUXJMZ0KjkWkQk6tRzLSJRp+SasC1ElWsRkcjb3XOtRWREJJqUXBO2hahyLSISebt7rgsciIjIHii5RvNci4gUi0whRMufi0hUKaUEUg4JZdciIpGXaeHTgEYRiSpllISVa3WFiIhE3u7KtZJrEYkmJddonmsRkWKR6blOK7kWkYhSck0woDGm2UJERCJPlWsRiTol1wQVEA9ieK0AACAASURBVFWuRUSiTz3XIhJ1Sq7RVHwiIsUioRUaRSTilFwTfEirLUREJPricVWuRSTalFwDaVWuRUSKQkI91yIScUqu0fLnIiLFImZa/lxEok3JNZBOoxUaRUSKQKZyreXPRSSqlFKiAY0iIsUiFjPMVLkWkehSco0GNIqIFJO4mXquRSSylFyjAY0iIsUkHjPNFiIikaXkGg1oFBEpJgkl1yISYUquCdtCVLkWESkK8ZjaQkQkupRco8q1iEgxUVuIiESZkms0W4iISDGJx2KqXItIZOU1uTazMWb2ppktMbMJDRy/wsxeN7N5ZjbLzAaG+08ysznhsTlmNjqfcabVFiIiUjQSMSOt5FpEIipvybWZxYGJwCnAQOCCTPKc5SF3H+LuQ4HbgDvC/euAM9x9CHAJ8GC+4oSwcq3cWkSkKKjnWkSiLJ+V65HAEnd/2913ApOBM7NPcPctWZttAQ/3v+ruq8L9C4DWZlaer0CDFRqVXYuIFIOg51qLyIhINCXyeO9ewPKs7RXAkfVPMrOrgG8DrYCG2j/GAXPdfUcD114OXA5w0EEH7XegGtAoIlI8Eqpci0iE5VS5NrM/mdlpZtbklW53n+juhwDXAT+o97qDgJ8CX9/DtZPcvdrdq7t167bfMWhAo4iUkr2Nh8k6b5yZuZlVh9sXhmNkMo+0mQ0Nj80M75k51j1f8cdjRtqVXItINOWaLN8FfBlYbGa3mtlhOVyzEuiTtd073Lcnk4GzMhtm1ht4DPiKuy/NMc79ogGNIlIqchwPg5lVAtcCL2X2ufvv3X1oOE7mYuAdd5+XddmFmePu/kG+foZ4zEimlFyLSDTllFy7+wx3vxAYDiwDZpjZ383sq2ZWtofLXgH6mVlfM2sFnA9MzT7BzPplbZ4GLA73dwSeBCa4+9/25QfaH8GARiXXIlIS9joeJvRjgm8Oa/dwnwvCa5ud5rkWkSjLuc3DzLoA44FLgVeB/yFItp9t6Hx3TwJXA9OBRcAUd19gZjeb2djwtKvNbIGZzSPou74ksx/4LHBDc3zFmEqrLURESkZD42F6ZZ9gZsOBPu7+ZCP3OQ94uN6+X4Wf1z80y1/FQj3XIhJlOQ1oNLPHgMMIpsQ7w93fDw/9wcxq9nSdu08DptXbd0PW82v3cN0twC25xNYU0mknpsq1iAjh2Jo7CIopezrnSOAjd5+ftftCd18ZtpM8StA28tsGrv3UA9HVcy0iUZZr5fpOdx/o7v+ZlVgD4O7VeYirWQUDGgsdhYhIs9jbeJhKYDAw08yWAUcBUzODGkPnU69q7e4rwz+3Ag8RtJ98QlMMRE/EYuq5FpHIyjWlHBj2QQNgZp3M7Bt5iqnZaZ5rESkhjY6HcffN7t7V3avcvQp4ERjr7jVQV9k+l6x+azNLmFnX8HkZcDqQXdVuUrEY6rkWkcjKNbm+zN03ZTbcfSNwWX5Can4a0CgipSLH8TCNORZY7u5vZ+0rB6ab2WvAPIJK+L1NHHqdRCxGUovIiEhE5bqITNzMzD1ocguncmqVv7CalwY0ikgp2dt4mHr7j6u3PZOgVSR73zZgRJMG2Yh4zFBXiIhEVa7J9dMEgxd/GW5/PdxX9NLhV4tKrkVEikNCy5+LSITlmlxfR5BQXxluPwvcl5eImlkqHHGuthARkeIQ0yIyIhJhOSXX7p4G7g4fLUpmUIwGNIqIFIeEFpERkQjLdZ7rfsB/EiyVW5HZ7+6fyVNczSYzV6raQkREikPQc63kWkSiKdfZQn5FULVOAscTLAzwu3wF1Zwy1Q+1hYhIsTGza82svQXuN7O5ZnZyoePKN1WuRSTKck2uW7v7/wPM3d919x8Bp+UvrOaTGROjthARKUL/4u5bgJOBTgSrIt5a2JDyTz3XIhJluQ5o3BEuHLDYzK4mmMO0Xf7Caj67BzQWOBARkX2X+eQ6FXgwnK+6xX+aqXItIlGWa+X6WqANcA3BXKYXAZfkK6jmlNJUfCJSvOaY2TMEyfV0M6sEWvwcdfFYTD3XIhJZe61chwvGnOfu3wU+BL6a96iaUWZAo9pCRKQIfQ0YCrzt7h+ZWWda2Gd0Q1S5FpEo22vl2t1TwBeaIZaC0IBGESliRwNvuvsmM7sI+AGwucAx5V08ZiRTLb5ALyJFKtee61fNbCrwR2BbZqe7/ykvUTUjzXMtIkXsbuBzZvY54DsEi3v9FvingkaVZ3FVrkUkwnJNriuA9cDorH0OFH1yndYKjSJSvJLu7mZ2JvALd7/fzL5W6KDyLaF5rkUkwnJdobHF9vBpQKOIFLGtZnY9wRR8x4SzOpUVOKa8U+VaRKIs1xUaf0VQqf4Yd/+XJo+omWlAo4gUsfOALxPMd73azA4CflbgmPIuHjOSSq5FJKJybQv5v6znFcDZwKqmD6f5ZcbEqC1ERIpNmFD/HjjCzE4HXnb33xY6rnyLxwx3SKddhRERiZxc20Iezd42s4eBWXmJqJntbgspcCAiIvvIzM4lqFTPJFhQ5n/N7Hvu/khBA8uzRJhQp9yJoeRaRKIl18p1ff2A7k0ZSKHUtYWoci0ixeffgSPc/QMAM+sGzABadHKdqVan0k5ZvMDBiIjUk2vP9VY+3nO9GrguLxE1Mw1oFJEiFssk1qH15L7ybtHKVK7Vdy0iUZRrW0hlvgMplJQGNIpI8XrazKYDD4fb5wHTChhPs4jHgv9/0IwhIhJFOVU4zOxsM+uQtd3RzM7KX1jNRys0ikixcvfvAZOAw8PHJHdvEd8qNiaR1RYiIhI1ufZc3+juj2U2wqV2bwQez09YzUdtISJSzMIB54/u9cQWJFbXFqIl0EUkenJNrhuqcO/vYMhISac1oFFEiksD42DqDgHu7u2bOaRmpcq1iERZrglyjZndAUwMt68C5uQnpOaV6blW5VpEikVLHgeTi7iSaxGJsFxHlf8rsBP4AzAZqCVIsIue5rkWESkuqlyLSJTlOlvINmBCnmMpCM1zLSJSXOKaik9EIizX2UKeNbOOWdudwumfil7d8udqCxERKQpqCxGRKMu1GaKru2/KbLj7RlrICo0pDWgUkRJjZmPM7E0zW2Jme/xW0szGmZmbWXW4XWVm281sXvi4J+vcEWb2enjPO83y96GqthARibJck+u0mR2U2TCzKhoeqV500hrQKCIlxMziBIPTTwEGAheY2cAGzqsErgVeqndoqbsPDR9XZO2/G7gM6Bc+xuQjftAiMiISbbkm1/8OzDKzB83sd8DzwPX5C6v5aJ5rESkxI4El7v62u+8kGKR+ZgPn/Rj4KcEA9kaZWU+gvbu/6O4O/BbI20JjmQHo6rkWkSjKKbl296eBauBNgmV2vwNsz2NczUYDGkWkxPQClmdtrwj31TGz4UAfd3+ygev7mtmrZva8mR2Tdc8Vjd2zKe2uXGsRGRGJnpxmCzGzSwm+HuwNzAOOAmYDo/MXWvNQ5VpEZDcziwF3AOMbOPw+cJC7rzezEcDjZjZoH+9/OXA5wEEHHbSXsxu2u+d6vy4XEcmrXNtCrgWOAN519+OBYcCmxi8pDpnkOqHkWkRKw0qgT9Z273BfRiUwGJhpZssIiilTzaza3Xe4+3oAd58DLAUODa/v3cg967j7JHevdvfqbt267dcPENfy5yISYbkm17XuXgtgZuXu/gZwWP7Caj51bSFKrkWkNLwC9DOzvmbWCjgfmJo56O6b3b2ru1e5exXwIjDW3WvMrFs4IBIz+wzBwMW33f19YIuZHRXOEvIV4Il8/QCaik9EoizX5c9XhPNcPw48a2YbgXfzF1bzqZvnWj3XIlIC3D1pZlcD04E48IC7LzCzm4Ead5/ayOXHAjeb2S4gDVzh7hvCY98Afg20Bp4KH3mhRWREJMpyXaHx7PDpj8zsOaAD8HTeompGqbrKdYEDERFpJu4+DZhWb98Nezj3uKznjwKP7uG8GoJ2krzLtPGllVyLSATlWrmu4+7P5yOQQsl8OKtyLSJSHFS5FpEoK/l6rWYLEREpLuq5FpEoK/nkWgMaRUSKS0KVaxGJsJJPrlNqCxERKSqZRWTUcy0iUaTk2tUWIiJSTFS5FpEoK/nkOlP50PLnIiLFIVbXc61FZEQkevKaXJvZGDN708yWmNmEBo5fYWavm9k8M5tlZgPD/V3M7Dkz+9DMfpHPGOvmuVblWkSkKKhyLSJRlrfkOlzFayJwCjAQuCCTPGd5yN2HuPtQ4DbgjnB/LfBD4Lv5ii+jbp5r5dYiIkUhrnmuRSTC8lm5Hgkscfe33X0nMBk4M/sEd9+StdkW8HD/NnefRZBk51U67cQMTG0hIiJFQZVrEYmyfV5EZh/0ApZnba8Ajqx/kpldBXwbaAWMzmM8DUq5qyVERKSIxDTPtYhEWMEHNLr7RHc/BLgO+MG+XGtml5tZjZnVrF27dr9eP6hcK7kWESkWqlyLSJTlM7leCfTJ2u4d7tuTycBZ+/IC7j7J3avdvbpbt277EWJQ+VDlWkSkeGiFRhGJsnwm168A/cysr5m1As4HpmafYGb9sjZPAxbnMZ4Gpdy1gIyISBFJhIvIKLkWkSjKW8+1uyfN7GpgOhAHHnD3BWZ2M1Dj7lOBq83sRGAXsBG4JHO9mS0D2gOtzOws4GR3X9jUcabTrqXPRUSKSOYjW20hIhJF+RzQiLtPA6bV23dD1vNrG7m2Kn+R7aYBjSIixcXMiMdMi8iISCQVfEBjoaXSWp1RRKTYBMl1oaMQEfmkkk+u02knXvLvgohIcYmbKtciEk0ln1ZqQKOISPFJxEw91yISSUquNaBRRKToxOOm5c9FJJKUXGueaxGRoqPKtYhElZJrtYWIiBSdmJnmuRaRSCr55FrzXIuIFB9VrkUkqko+uU6lVbkWESk26rkWkagq+eQ67apci4gUm0Qspsq1iERSySfXKc1zLSJSdGKGeq5FJJJKPq1MOWoLEREpMkHlWovIiEj0lHxyrQGNIlJqzGyMmb1pZkvMbEIj540zMzez6nD7JDObY2avh3+Ozjp3ZnjPeeGjez5/Bi1/LiJRlSh0AIWWSjsJJdciUiLMLA5MBE4CVgCvmNlUd19Y77xK4Frgpazd64Az3H2VmQ0GpgO9so5f6O41ef0BQom4lj8XkWgq+cp1yp2Y2kJEpHSMBJa4+9vuvhOYDJzZwHk/Bn4K1GZ2uPur7r4q3FwAtDaz8nwHXGfxDJh0PGxdQ8w0FZ+IRFPJJ9dprdAoIqWlF7A8a3sFH68+Y2bDgT7u/mQj9xkHzHX3HVn7fhW2hPzQLA9Vi9QOWDUXtq4iEdMiMiISTSWfXKdcybWISIaZxYA7gO80cs4ggqr217N2X+juQ4BjwsfFe7j2cjOrMbOatWvX7ltwlT2CP7euDnuulVyLSPSUfHKdTqstRERKykqgT9Z273BfRiUwGJhpZsuAo4CpWYMaewOPAV9x96WZi9x9ZfjnVuAhgvaTT3D3Se5e7e7V3bp127fIK3sGf259P+y5VnItItFT8sm1KtciUmJeAfqZWV8zawWcD0zNHHT3ze7e1d2r3L0KeBEY6+41ZtYReBKY4O5/y1xjZgkz6xo+LwNOB+Y3eeRtuwMGW1er51pEIkvJdRpVrkWkZLh7EriaYKaPRcAUd19gZjeb2di9XH418FnghnpT7pUD083sNWAeQSX83iYPPp6Adt2DyrXaQkQkokp+Kr60VmgUkRLj7tOAafX23bCHc4/Len4LcMsebjuiqeJrVGWPsOc6puRaRCKp5NNKtYWIiBSRyp6qXItIpJV8cq0BjSIiRaSucm1a/lxEIqnkk2tVrkVEikhlT9i2ljJLqXItIpGk5DrtxFW5FhEpDuFc153TG0m5kmsRiZ6ST67TaSemyrWISHEI57rumFpPKqXkWkSip+ST65Srci0iUjTCynWn1HrNcy0ikaTkOo0q1yIixaJdkFx3SK1Tz7WIRFLJJ9dp1zzXIiJFo21XsDgdkhvUcy0ikVTyaaUGNIqIFJFYHNodQMfUej7amSKt6rWIREzJJ9ca0CgiUmQqe9CNjexMplm5aXuhoxER+ZiST641oFFEpMhU9qRjah0ASz74sMDBiIh8nJLrtBaREREpKpU9qKhdC8DiD7YWOBgRkY8r+eQ67WoLEREpKpU9iW3fwIHtYixeo8q1iERLySfXGtAoIlJkwrmuq7vs4C21hYhIxJR0cu3upF3zXIuIFJVwlcbD229nyZqtuKbkE5EIKenkOjODkyrXIiJFJKxc92v7Idt2pnh/c22BAxIR2a2kk+vM6l5aREZEpIiEleuDy7YAsFitISISISWdVmaSa7WFiIgUkTadIVbGAbYRgMVrNGOIiERHaSfXYZ+e2kJERIqIGVT2oHXtWrq0baW5rkUkUko7ua5rC1FyLSJSVCp7wNb3+Wz3drylyrWIREhJJ9fpTFuIKtciIsWlsgdsXU2/A9qx+IMPNWOIiERGSSfXdW0hqlyLiBSXygNhy0r6dWvL1tokH2zdUeiIRESAEk+u02oLEREpTgcOhZ0fcnj5agCt1CgikVHSybUq1yIiRarPkQB8dsdCABZ/oL5rEYmG0k6u05otRESkKHX+DLTpSrsPaujYpkxzXYtIZOQ1uTazMWb2ppktMbMJDRy/wsxeN7N5ZjbLzAZmHbs+vO5NM/vnfMSXTgd/ap5rEZEiYwZ9jsSWv0z/HpXULNugQY0iEgl5S67NLA5MBE4BBgIXZCfPoYfcfYi7DwVuA+4Irx0InA8MAsYAd4X3a1K720Ka+s4iItG1t8JH1nnjzMzNrDprX4OFj1zv2aT6jIQNSxnXv4K31nzI6ys3N8vLiog0Jp9p5Uhgibu/7e47gcnAmdknuPuWrM22QKbscCYw2d13uPs7wJLwfk0qpan4RKTE5Fj4wMwqgWuBl7L2NVj4yPWeTS7suz6143LKEzH+WLMi7y8pIrI3+UyuewHLs7ZXhPs+xsyuMrOlBJXra/bl2k8rrQGNIlJ69lr4CP0Y+ClQm7VvT4WPXO/ZtA4cCrEy2q6p4Z8H9eCJeSup3ZXK+8uKiDSm4A0R7j7R3Q8BrgN+sC/XmtnlZlZjZjVr167d59fWgEYRKUF7LV6Y2XCgj7s/meO1zVIQ+YSy1kGCvfxlzq3uw5baJM8sXJP3lxURaUw+k+uVQJ+s7d7hvj2ZDJy1L9e6+yR3r3b36m7duu1zgHVtIapci4gAYGYxgvEv38nT/T9VUeQT+hwJK+fy+apKenVszR9rlu/9GhGRPMpncv0K0M/M+ppZK4I+vanZJ5hZv6zN04DF4fOpwPlmVm5mfYF+wMtNHWBdW4gq1yJSOvZWvKgEBgMzzWwZcBQwNRzUuKdrcy6mfNqiyCf0GQmpHcRWv8a44b2YtWQdqzZt//T3FRHZT3lLrt09CVwNTAcWAVPcfYGZ3WxmY8PTrjazBWY2D/g2cEl47QJgCrAQeBq4yt2bvJEupRUaRaT0NFr4cPfN7t7V3avcvQp4ERjr7jXsufCx12JK3oSDGln+El8c0Qd3eGDWO83y0iIiDUnk8+buPg2YVm/fDVnPr23k2p8AP8lfdLsr12oLEZFS4e5JM8sUPuLAA5nCB1Dj7ntMisPzMoWPJFmFj4bume+fBYDKHtDxYFj6Fw76/NWcf0Qf7v/bOxx7aDeOPbQJKuMiIvsor8l11KXCRWTUFiIipWRvhY96+4+rt91g4aOhezab4RfDX26BhU9w4xmn8+p7m/jWH+Yx7dpjOKB9RUFCEpHSVfDZQgpp94DGAgciIiL7b9Q3oedQ+L9v0XrnBiZeOIyPdqa45uFXSWaqKCIizaSk00oNaBQRaQHiZXD2PbBjKzz5LT7brR23nDWYl97ZwHf/+A8l2CLSrEo6udaARhGRFqL7ADj+32HRn+HlSYwb0Zvv/fNhPD5vFddMfpWdSSXYItI8SrvnWgMaRURajs//K7z7d3jq32Dju1x18o8pT8S45clF7Ng1h//98jDatCrpf/ZEpBmU9KdMWis0ijRq165drFixgtra2r2fLDmpqKigd+/elJWVFTqUlicWh/MfgunfhxcnwvrFXDrufirKBvPDJ+Yz7u7ZTLp4BH06tyl0pCLSgpV0cq22EJHGrVixgsrKSqqqqjD9T+in5u6sX7+eFStW0Ldv30KH0zLFE3DqbdDtMJj2PfjN6Vx04aP07nQE//rwq4z9xSwmXjiczx/StdCRikgLVdI913XzXCtpEGlQbW0tXbp0UWLdRMyMLl266JuA5nDE1+CCh2HtW/DAyRzX/SOmXv0FurQr56L7XuJn099QH7aI5EVJJ9d181yrci2yR0qsm5bez2Z06D/DJVPhow1w/8n03TqHx68axRdH9Gbic0s55+6/8ebqrYWOUkRamNJOrjNT8ZX0uyASbZs2beKuu+7a5+tOPfVUNm3a1Og5N9xwAzNmzNjf0KQY9BkJ/zIdyivhN2NpN+s/uO3sgdxz0XBWbtzOKf/zAt/6wzyWrdtW6EhFpIUo6bQyM6BRbSEi0bWn5DqZTDZ63bRp0+jYsWOj59x8882ceOKJnyo+KQLd+8Plz8Owi+Cv/wX3nciYtkuY8e1/4tJjPsNT89/nhDue55uTX+X1FZsLHa2IFLmSTq41oFEk+iZMmMDSpUsZOnQoRxxxBMcccwxjx45l4MCBAJx11lmMGDGCQYMGMWnSpLrrqqqqWLduHcuWLWPAgAFcdtllDBo0iJNPPpnt27cDMH78eB555JG682+88UaGDx/OkCFDeOONNwBYu3YtJ510EoMGDeLSSy/l4IMPZt26dc38LsinVt4OzvwFfOk3sG0t/OZ0ujx+Id8fWssL/3Y84z9fxbML13DGL2Zx7j2z+WPNcrbU7ip01CJShEp7thANaBTJ2U1/XsDCVVua9J4DD2zPjWcMavScW2+9lfnz5zNv3jxmzpzJaaedxvz58+tm23jggQfo3Lkz27dv54gjjmDcuHF06dLlY/dYvHgxDz/8MPfeey/nnnsujz76KBdddNEnXqtr167MnTuXu+66i9tvv5377ruPm266idGjR3P99dfz9NNPc//99zfdGyDNb9BZQS/2y5OCKvak4+h+0Of54VFXcu3ok5gy531+O/tdvvfIa/z74/M5oX93Th3Sk9H9u9O2vKT/yRSRHJX0J0ValWuRojNy5MiPTWN355138thjjwGwfPlyFi9e/Inkum/fvgwdOhSAESNGsGzZsgbvfc4559Sd86c//QmAWbNm1d1/zJgxdOrUqUl/HimAstYw6loYMR7mPggv/xKmXEz7tt24dNDZfO38cbzqhzP1H6t58vX3eWr+asoTMY7p140TBnTn+MO606NDRaF/ChGJqJJOrncPaFRyLbI3e6swN5e2bdvWPZ85cyYzZsxg9uzZtGnThuOOO67Bae7Ky8vrnsfj8bq2kD2dF4/H99rTLS1ARQf4/NVw1JXw1nR47Q8w97fYy5MY3qYrwz97IjeMPZm55Ufyf29s5tmFa5ixaA0AA3q257jDuvFPh3ZjxMGdKNPIeBEJlXRyrQGNItFXWVnJ1q0NT5e2efNmOnXqRJs2bXjjjTd48cUXm/z1R40axZQpU7juuut45pln2LhxY5O/hhRYLA79Tw0etVuCRHvxM7D4GWKvTaa6rA3Vh53KjWeexZI2w5nxTi0z3/yAe194m7tnLqWyPMExh3bl+MO684V+XenZoXWhfyIRKaCSTq41oFEk+rp06cKoUaMYPHgwrVu35oADDqg7NmbMGO655x4GDBjAYYcdxlFHHdXkr3/jjTdywQUX8OCDD3L00UfTo0cPKisrm/x1JCIq2sPhXwoe6RS8NxtefwQWPo7Nf4R+Fqdf72quPPR4tp1wLLM+OpjnFm/gL298wLTXVwPwma5tOeqQLhzZtzMj+3ZWsi1SYszD1ohiV11d7TU1Nft0zX1/fZtbnlzEP244mQ5tyvIUmUjxWrRoEQMGDCh0GAW1Y8cO4vE4iUSC2bNnc+WVVzJv3rxPdc+G3lczm+Pu1Z/qxkVmfz63Cya5E1a8DEufg6V/gVWvAg7l7eHgUaSrvsA77YYzc1M3/vb2Jl5+ZwMf7ghai3p1bM2Anu0ZdGB7hvTqwLCDOtKlXXnjrycikdbYZ3ZJV67rlj9Xq5yI7MF7773HueeeSzqdplWrVtx7772FDkkKIdEKqr4QPE74YbDq4zsvwNvPwTsvEHvrKQ4BDmnVjq8dOIz0qBGsLO/LKx/1ZNaGtry2Zht/eWMN4RemHNylDYMP7MCAnpUM6NmegQe2p0f7Cq3gKdIClHRynVn+PKHsWkT2oF+//9/e3QdZVZ8HHP8+55z7tneXfXHlbVmFKAloBHmpkvoS1LRFYzBxVJKgFZuEqaNDsJ22ZJq2mkmadMYhxhnHRBOtMaghGKNNtLaxG1+SqkBURDDFCMblbXdhgX2/957z9I9zdr0giwTu7uXe+3xmzuw9r/x+97c8+5zf+Z1zpvLKK68UuxjmRFPVED7W78xPh/P7t8M7v4bWtdC6FufFu2gOsjQDVyIwpgl/6mTaaz7Cq+5HebrrNH67Yz+/eH3n0CEb0nHOmDCGqeOq+fC4GqaOrea0k6upT8eLU0djzDGp6OTaeq6NMcYURG0TzLgmnCAcRrL397D7DejYAp1bcfe+zfg3f8QCf4AF4kDDaWRnTqctdRpbggm80tPIrzuFR17upC/rDx26IR3n9LHVfGRcDR8eX8NpJ6f5UGM148YkrKfbmBNQRSfXQzc0WnAyxhhTSF4cxk4Pp3zZ/rB3+51fw67Xie1+nabO/6AJZT5wC4KedAr9dVPpiDexy69hW38163vH8otXxvHgwHt/r6riLpPqUzTXV9HcUMWpJ1Ux+aQ0zQ1VNNWlSMXdUa2yMSZkyTX2tBBjjDGjJJaEKReE06BMb9jL3bEFOrYg7W+San+T5h0vLTAVWgAAEzlJREFU0pzp5k+Aq4FvegkyTTPZlzqVXdLItmwDb2XqeX1vHY9tTbN/4OAHFDSk44ytSXByTYLG6gQTapNMqq+iqT5FU13KEnBjRkhFJ9eBKiLYZTVjjDHFE6+C8WeF06EyvdC9G3a9jrz7EonWdYzb/TzjuncxM28zdVyCcZPoTjezzxtLh9awK1fN9lwdW7vq+F3bGH7RVU0mOPjvXV1VjMbqBI3VcepScVJxl2TMpSEdY9yYJOPGJGlIx6mvilFXFac2FbMX5hjzASo6ufYDtSEhxpSZ6upquru72bFjB8uWLWPNmjXv22b+/PncfvvtzJ07/JPv7rjjDpYuXUpVVRUAl112GQ899BB1dXUjVnZj3ideBQ1TwumMhe8tzw3Age2w7w+w7w9I5zbczm3U7t1Kbef/cmpPBwTZgw6lyRh+7Sl0VzXT6Z1Mm5zEDr+WtmyKHX0p2g4k2ZmLsyebYHufR29w+BShOuFRm4pRk/QYk4oxJulRkwznUzGXuOeQ8BxScY+ahEd10qMhHeekdJz6dJyapEfCsx5zU74qO7lWxbEhIcaUpYkTJx42sT5ad9xxB9dee+1Qcv3kk08WqmjGHD8vAQ0fCqfDUYX+/dC1M0zC97cie7fi7X2bus6t1HVuZEpvx/DHj4N6KXKxarKxGgacNH1Omm6nhv1SQ6dWsy9Isbc3xb4DMQ5kXfZkHTSXIRH0kiDDdm1kS9DELhqAg//WJjyHmmSYmFcnPcYkY4xJhT+r4h7pRNiDnvAcYm6YrFcnPaoTHumER1XcJR0PP1cnPJIxx65CmxNGRSfXgfVcG3PCW7FiBc3Nzdx0000A3HrrrXieR0tLC52dnWSzWb7+9a9zxRVXHLTftm3buPzyy9m4cSN9fX3ccMMNvPbaa0ybNo2+vr6h7W688UbWrl1LX18fV111Fbfddht33nknO3bs4KKLLqKxsZGWlhYmT57MunXraGxsZOXKldx3330AfPGLX2T58uVs27aNSy+9lPPPP5/f/OY3NDU18fjjj5NK2dv5TBGIQKounA69qXJQth962qBvH/R1wsABGOiGTDf070f69xHr20cs003VQBf1/fuhbyv07IX+faDB+4/pRlOewEuS86rJulUMOCkGJEEfKXpJ0KsJuvvjHOhN0ukn2ZNN0OXHaM259GmcfuL0aoIsHh4+nvj0aYJ2raWdOjKEL4BzhKFkPBlzh6ZUzIl+usRcB9cVPEdIeu5Qkh73nKHe9upE2At/6PJwcknEHKriYQ+93a9lhlPRybUf2M2Mxhy1p1bArtcLe8zxZ8Gl3zriJosWLWL58uVDyfXq1at5+umnWbZsGWPGjKGjo4N58+axcOHCYXuu7r77bqqqqti8eTMbNmxg9uzZQ+u+8Y1v0NDQgO/7XHLJJWzYsIFly5axcuVKWlpaaGxsPOhY69ev5/777+ell15CVTn33HP5+Mc/Tn19PVu2bOHhhx/m3nvv5ZprruHRRx/l2muvPc4vqfBEZAHwHcI06Puq+q1D1v81cBPgA93AUlXdJCKLgb/L23QGMFtVXxWRXwETgMEzlz9X1baRrYk5LrEk1J0STn+sIIiS8H3huHB/IHz8oBeHeDW4Meh8Bzp+h7N3K/FMN/FML+lMD2R7wiQ+2wHZXsj0hMfKRb86AhzlS5N98fCdODmJkXFSUfKeJOt7ZHIe9Pgkg25SQQ8BQj8JeknSowl6gjhdQZx9mmY/aTo0TT9xBogxoDGyeGTwyBAjozEyePSRoIcE/ZrAd2KIFwc3gevFhhLxweQ75gmORMl8lOCn4mGSH3OFeN62qbiL54TLPMfBc4WYK8TccD7uCQkvPGGIuULWV3JBgOc41FXFDjsWXlVRxa7QF0FFJ9eBKvY7Z8yJbdasWbS1tbFjxw7a29upr69n/Pjx3HLLLTz33HM4jsP27dvZvXs348ePP+wxnnvuOZYtWwbAjBkzmDFjxtC61atXc88995DL5di5cyebNm06aP2hXnjhBT7zmc+QTqcBuPLKK3n++edZuHAhU6ZM4eyzzwZgzpw5bNu2rUDfQuGIiAvcBfwZ0AqsFZEnVHVT3mYPqep3o+0XAiuBBaq6ClgVLT8L+Jmq5r8LfrGqlsj7zM1xcRxIjgmn4dSdcvBTUT5ILgMDXZDrD6ds33vJd5ADxwuT9kwPdO2C7jbcbA9uLkM8109Vtg8yXVGynwE/G/bgJyeGr6kXeS+Rz/RCthfNdITDZ/r2IegHl/FwFPysSy4XJyceuR6PHC5Z8cgSCz+rS1YdsuqQwxmaz6mDj0MGjx5NsZckWTwUUBwyuEOJfYYw4c/iogiBOgQIAYLioI6D4hKIi6+KH4CPA24CJ57CiSVwXZeY6+J4HuomEDcGXgJ1EkgsjhdLkkjESXphEu+5Eib7juC6givhCYPjCHFXSERXC1wnXCciOBJ2XLqOEHfD3n9v8ITCDT970fr+rE9vxifrB9SmwptmqxMerhMep1SH+lR0cu0Haj3XxhytD+hhHklXX301a9asYdeuXSxatIhVq1bR3t7O+vXricViTJ48mf7+/j/6uFu3buX2229n7dq11NfXs2TJkmM6zqBEIjH02XXdg4afnEDOAd5S1bcBROQR4ApgKLlW1QN526fhsFnH54BHRrCcptJ4cfBOGtV/cigDGOyJzw1Eyf1A1Bs/ECbqgz+Hetp7wuQ9yEIug5vrx831kxhM6gfXDc4HOQj8cFngg59Fgywa+AR+BnJdkGlFsj2In42G3CjiZ4496c/nR9NRbeqQU3cocQ+T/zDB99VFgYDwpCBLeCIRpfcADAyeCKhHDocBHAKcg4+PSw4XP1rnq4Mr4WmCAv2aoJ/wikQgLoiLOk74UxycvH/Plxi+EydwYuEJmOMSl4AqyZCSAXwc+jVGhhjqJsFL4nheeG+A30OqqoYr/mrF8X/HeSo7uVZLro0pBYsWLeJLX/oSHR0dPPvss6xevZqxY8cSi8VoaWnhnXfeOeL+F154IQ899BAXX3wxGzduZMOGDQAcOHCAdDpNbW0tu3fv5qmnnmL+/PkA1NTU0NXV9b5hIRdccAFLlixhxYoVqCqPPfYYDz744IjUe4Q0Ae/mzbcC5x66kYjcBPwNEAcuPsxxFhEm5fnuFxEfeBT4uqq+LysQkaXAUoBTTjmG4QjGjITBnvhRJNF0xAcbqoaJ+WBy70cJOxom4EGUMQd+NJ8Lp3Dn8KQh1x8OuckN7hcd08++dwIxeDLhZ3H9DK6fCY+nigY5NDdAkB2AIBcONwkCAj9H4GfQXBZFUHFQVSTIQq4f8cOTh3AfH0VRVQh8JMghQQ6XgMH+d1/DRJ7AxwsGcIMB3CCLoz4OPlF3fsHt8poAS64L5uaLTue6eacWuxjGmA9w5pln0tXVRVNTExMmTGDx4sV86lOf4qyzzmLu3LlMmzbtiPvfeOON3HDDDUyfPp3p06czZ84cAGbOnMmsWbOYNm0azc3NnHfeeUP7LF26lAULFjBx4kRaWlqGls+ePZslS5ZwzjnnAOENjbNmzTohh4AcD1W9C7hLRD4PfBW4fnCdiJwL9KrqxrxdFqvqdhGpIUyurwN+eJjj3gPcAzB37twR+FNpTBkRCYfCuEc5CH0kisBRnASMNM07mdAAxAkn9OArC4NXB5xY+BhLLwXqH3xFItcfnlwkaiBew/hEdcGLK4fpWChJc+fO1XXrbKifMYW0efNmpk8f5kkD5pgd7nsVkfWqOvyDtwtERD4G3KqqfxHNfwVAVb85zPYO0KmqtXnLvg20q+q/DrPPEmCuqt58pLJY3DbGlKojxWx7zZIxxlSWtcBUEZkiInHgs8AT+RuIyNS82U8CW/LWOcA15I23FhFPRBqjzzHgciC/V9sYYypGRQ8LMcaYSqOqORG5GXia8FF896nqGyLyNWCdqj4B3CwinwCyQCd5Q0KAC4F3B2+IjCSAp6PE2gV+Cdw7CtUxxpgTjiXXxhhTYVT1SeDJQ5b9c97nLx9h318B8w5Z1gPMKWwpjTGmNNmwEGPMEZXLfRknCvs+jTGmvFlybYwZVjKZZM+ePZYQFoiqsmfPHpLJZLGLYowxZoTYsBBjzLAmTZpEa2sr7e3txS5K2Ugmk0yaNKnYxTDGGDNCLLk2xgwrFosxZcqUYhfDGGOMKRk2LMQYY4wxxpgCseTaGGOMMcaYArHk2hhjjDHGmAIpm9efi0g78M4x7NoIdBS4OCcSq19ps/qVtj+mfqeq6skjWZgTzTHGbfudKW1Wv9Jm9XvPsDG7bJLrYyUi64Z7N3w5sPqVNqtfaSv3+hVDuX+nVr/SZvUrbYWqnw0LMcYYY4wxpkAsuTbGGGOMMaZALLmGe4pdgBFm9SttVr/SVu71K4Zy/06tfqXN6lfaClK/ih9zbYwxxhhjTKFYz7UxxhhjjDEFUtHJtYgsEJHfichbIrKi2OU5HiLSLCItIrJJRN4QkS9HyxtE5L9FZEv0s77YZT0eIuKKyCsi8vNofoqIvBS14Y9FJF7sMh4rEakTkTUi8qaIbBaRj5VT+4nILdHv5kYReVhEkqXcfiJyn4i0icjGvGWHbS8J3RnVc4OIzC5eyUtXOcVsqIy4Xc4xG8o7bpdbzIbRi9sVm1yLiAvcBVwKnAF8TkTOKG6pjksO+FtVPQOYB9wU1WcF8IyqTgWeieZL2ZeBzXnz/wZ8W1VPBzqBLxSlVIXxHeA/VXUaMJOwnmXRfiLSBCwD5qrqRwEX+Cyl3X7/Diw4ZNlw7XUpMDWalgJ3j1IZy0YZxmyojLhdzjEbyjRul2nMhlGK2xWbXAPnAG+p6tuqmgEeAa4ocpmOmaruVNXfRp+7CP+DNxHW6YFosweATxenhMdPRCYBnwS+H80LcDGwJtqkZOsnIrXAhcAPAFQ1o6r7KKP2AzwgJSIeUAXspITbT1WfA/Yesni49roC+KGGXgTqRGTC6JS0bJRVzIbyj9vlHLOhIuJ2WcVsGL24XcnJdRPwbt58a7Ss5InIZGAW8BIwTlV3Rqt2AeOKVKxCuAP4eyCI5k8C9qlqLpov5TacArQD90eXUL8vImnKpP1UdTtwO/AHwgC9H1hP+bTfoOHaq2zjzSgq6++wTON2OcdsKOO4XUExG0Ygbldycl2WRKQaeBRYrqoH8tdp+GiYknw8jIhcDrSp6vpil2WEeMBs4G5VnQX0cMilxBJvv3rCXoApwEQgzfsvzZWVUm4vM7rKMW5XQMyGMo7blRizoXDtVcnJ9XagOW9+UrSsZIlIjDBAr1LVn0aLdw9exoh+thWrfMfpPGChiGwjvBx8MeFYt7rokhWUdhu2Aq2q+lI0v4YwaJdL+30C2Kqq7aqaBX5K2Kbl0n6Dhmuvsos3RVCW32EZx+1yj9lQ3nG7UmI2jEDcruTkei0wNbrzNU44UP+JIpfpmEVj2X4AbFbVlXmrngCujz5fDzw+2mUrBFX9iqpOUtXJhG31P6q6GGgBroo2K+X67QLeFZGPRIsuATZRJu1HeGlxnohURb+rg/Uri/bLM1x7PQH8ZXT3+Txgf95lSHN0yipmQ3nH7XKP2VD2cbtSYjaMRNxW1YqdgMuA/wN+D/xjsctznHU5n/BSxgbg1Wi6jHCM2zPAFuCXQEOxy1qAus4Hfh59/hDwMvAW8BMgUezyHUe9zgbWRW34M6C+nNoPuA14E9gIPAgkSrn9gIcJxyJmCXuwvjBcewFC+KSL3wOvE96BX/Q6lNpUTjE7qk9FxO1yjdlRfco2bpdbzI7qNCpx297QaIwxxhhjTIFU8rAQY4wxxhhjCsqSa2OMMcYYYwrEkmtjjDHGGGMKxJJrY4wxxhhjCsSSa2OMMcYYYwrEkmtjCkRE5ovIz4tdDmOMMR/MYrYZKZZcG2OMMcYYUyCWXJuKIyLXisjLIvKqiHxPRFwR6RaRb4vIGyLyjIicHG17toi8KCIbROQxEamPlp8uIr8UkddE5Lciclp0+GoRWSMib4rIqujNVsYYY46RxWxTaiy5NhVFRKYDi4DzVPVswAcWA2lgnaqeCTwL/Eu0yw+Bf1DVGYRvaBpcvgq4S1VnAn9K+MYngFnAcuAMwjdZnTfilTLGmDJlMduUIq/YBTBmlF0CzAHWRh0UKaANCIAfR9v8CPipiNQCdar6bLT8AeAnIlIDNKnqYwCq2g8QHe9lVW2N5l8FJgMvjHy1jDGmLFnMNiXHkmtTaQR4QFW/ctBCkX86ZDs9xuMP5H32sf9jxhhzPCxmm5Jjw0JMpXkGuEpExgKISIOInEr4f+GqaJvPAy+o6n6gU0QuiJZfBzyrql1Aq4h8OjpGQkSqRrUWxhhTGSxmm5JjZ2imoqjqJhH5KvBfIuIAWeAmoAc4J1rXRjjGD+B64LtRIH4buCFafh3wPRH5WnSMq0exGsYYUxEsZptSJKrHeiXFmPIhIt2qWl3schhjjPlgFrPNicyGhRhjjDHGGFMg1nNtjDHGGGNMgVjPtTHGGGOMMQViybUxxhhjjDEFYsm1McYYY4wxBWLJtTHGGGOMMQViybUxxhhjjDEFYsm1McYYY4wxBfL/ig0ks0evvQUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture --no-display \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up figure\n",
    "f = plt.figure(figsize=(12,5))\n",
    "f.add_subplot(1,2, 1)\n",
    "\n",
    "# plot accuracy as a function of epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "\n",
    "# plot loss as a function of epoch\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "file_autoencoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
