{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General File Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN connected to a Dense network that predicts file labels given the first 512 bytes of a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23529412 0.24705882 0.47058824 ... 0.28627451 0.25882353 0.32156863]\n",
      " [0.31372549 0.42352941 0.39607843 ... 0.         0.         0.        ]\n",
      " [1.         0.84705882 1.         ... 0.27843137 0.28235294 0.28627451]\n",
      " ...\n",
      " [0.23529412 0.24705882 0.47058824 ... 0.28627451 0.3254902  0.31372549]\n",
      " [0.31372549 0.42352941 0.39607843 ... 0.22745098 0.1254902  0.32941176]\n",
      " [0.23529412 0.24705882 0.47058824 ... 0.28627451 0.3254902  0.31372549]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from ml_utils import grab_labels, feature_from_file, translate_bytes\n",
    "import csv\n",
    "\n",
    "le = LabelEncoder()\n",
    "naivetruth_path = \"/Users/ryan/Documents/CS/CDAC/xtract_autoencoder/automated_training_results/xtract_results/balanced_cdiac_subset.csv\"\n",
    "\n",
    "labels, file_paths = grab_labels(naivetruth_path)\n",
    "x = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    x.append(feature_from_file(file_path))\n",
    "\n",
    "x = translate_bytes(x) / 255\n",
    "y = to_categorical(le.fit_transform(labels), 6)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['freetext' 'image' 'json/xml' 'netcdf' 'tabular' 'unknown']\n",
      "image is 3.6766102983089493 and there are 387 files\n",
      "unknown is 22.81968459053772 and there are 2402 files\n",
      "json/xml is 22.800684020520613 and there are 2400 files\n",
      "tabular is 25.470264107923235 and there are 2681 files\n",
      "netcdf is 2.4320729621888657 and there are 256 files\n",
      "freetext is 22.800684020520613 and there are 2400 files\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n",
    "for unique in set(labels):\n",
    "    print(\"{} is {} and there are {} files\".format(unique, ((labels.count(unique) / len(labels)) * 100), labels.count(unique)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, GlobalMaxPooling1D, Reshape, Flatten\n",
    "\n",
    "\n",
    "classifier_model = Sequential()\n",
    "classifier_model.add(Reshape((len(x[0]), 1), input_shape=(len(x[0]),)))\n",
    "classifier_model.add(Conv1D(50, 32, activation='relu'))\n",
    "classifier_model.add(MaxPooling1D(pool_size=2))\n",
    "classifier_model.add(Conv1D(50, 32, activation='relu'))\n",
    "classifier_model.add(GlobalMaxPooling1D())\n",
    "classifier_model.add(Dense(50, activation='relu'))\n",
    "classifier_model.add(Dense(32, activation='relu'))\n",
    "classifier_model.add(Dense(28, activation='relu'))\n",
    "classifier_model.add(Dense(24, activation='relu'))\n",
    "classifier_model.add(Dense(16, activation='relu'))\n",
    "classifier_model.add(Dense(8, activation='relu'))\n",
    "classifier_model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_model.summary()\n",
    "\n",
    "history = classifier_model.fit(x_train, y_train,\n",
    "                               epochs=20,\n",
    "                               batch_size = 16,\n",
    "                               shuffle=True,\n",
    "                               validation_data=(x_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up figure\n",
    "f = plt.figure(figsize=(12,6))\n",
    "f.add_subplot(1,2, 1)\n",
    "\n",
    "# plot accuracy as a function of epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "\n",
    "# plot loss as a function of epoch\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from ml_utils import grab_labels, feature_from_file, translate_bytes, features_from_list\n",
    "\n",
    "validation_set = \"/Users/ryan/Documents/CS/CDAC/xtract_autoencoder/automated_training_results/nist_subset.csv\"\n",
    "\n",
    "validation_labels, validation_paths = grab_labels(validation_set)\n",
    "validation_labels.pop(0) #Gets rid of headers\n",
    "validation_paths.pop(0)\n",
    "\n",
    "validation_features = features_from_list(validation_paths)\n",
    "\n",
    "validation_encoded = translate_bytes(validation_features) / 255\n",
    "validation_labels = to_categorical(le.transform(validation_labels), 6)\n",
    "validation_predictions = classifier_model.predict(validation_encoded, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils import convert_to_index, plot_confusion_matrix\n",
    "from pycm import ConfusionMatrix\n",
    "import numpy as np\n",
    "\n",
    "# apply conversion function to data\n",
    "y_test_ind = convert_to_index(validation_labels)\n",
    "y_pred_test_ind = convert_to_index(validation_predictions)\n",
    "\n",
    "# compute confusion matrix\n",
    "cm_test = ConfusionMatrix(y_test_ind, y_pred_test_ind)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# plot confusion matrix result\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_test,title='confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "validation_accuracy = accuracy_score(y_test_ind, y_pred_test_ind)\n",
    "validation_recall = recall_score(y_test_ind, y_pred_test_ind, average='micro')\n",
    "\n",
    "print(validation_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
