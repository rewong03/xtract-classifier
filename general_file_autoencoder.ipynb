{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from ml_utils import feature_from_dir, translate_bytes\n",
    "import time\n",
    "\n",
    "test_set_dir = \"/Users/ryan/Documents/CS/CDAC/official_xtract/sampler_dataset/pub8\"\n",
    "\n",
    "raw_features = feature_from_dir(test_set_dir, byte_num=512)\n",
    "untranslated_features = translate_bytes(raw_features)\n",
    "\n",
    "x = untranslated_features / 255\n",
    "x_train, x_test, _, _ = train_test_split(x, x)\n",
    "\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "input_size = len(x_train[0])\n",
    "\n",
    "input_layer = Input((input_size,))\n",
    "encoded = Dense(256, activation='relu')(input_layer)\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(256, activation='relu')(decoded) \n",
    "decoded = Dense(input_size, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "autoencoder.summary()\n",
    "\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                          epochs=100,\n",
    "                          batch_size = 256,\n",
    "                          shuffle=True,\n",
    "                          callbacks=[EarlyStopping()],\n",
    "                          validation_data=(x_test, x_test))\n",
    "\n",
    "encoder = Model(input_layer, encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up figure\n",
    "f = plt.figure(figsize=(12,5))\n",
    "f.add_subplot(1,2, 1)\n",
    "\n",
    "# plot accuracy as a function of epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "\n",
    "# plot loss as a function of epoch\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "encoder = load_model(\"encoder_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and Label Grabbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25882353 0.30980392 0.32941176 ... 0.1254902  0.1254902  0.1254902 ]\n",
      " [0.14509804 0.31372549 0.26666667 ... 0.03921569 0.18823529 0.18823529]\n",
      " [0.26666667 0.38039216 0.45490196 ... 0.38431373 0.44705882 0.38039216]\n",
      " ...\n",
      " [0.14509804 0.31372549 0.26666667 ... 0.14117647 0.03529412 0.58431373]\n",
      " [0.31372549 0.42352941 0.39607843 ... 0.         0.         0.        ]\n",
      " [0.1254902  0.1254902  0.1254902  ... 0.1254902  0.1254902  0.1254902 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from ml_utils import grab_labels, feature_from_file, translate_bytes\n",
    "import csv\n",
    "\n",
    "le = LabelEncoder()\n",
    "naivetruth_path = \"/Users/ryan/Documents/CS/CDAC/xtract_autoencoder/automated_training_results/balanced_cdiac_subset.csv\"\n",
    "\n",
    "labels, file_paths = grab_labels(naivetruth_path)\n",
    "labels.pop(0) #Gets rid of headers\n",
    "file_paths.pop(0)\n",
    "\n",
    "x = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    x.append(feature_from_file(file_path))\n",
    "\n",
    "#x = encoder.predict(translate_bytes(x))\n",
    "x = translate_bytes(x) / 255\n",
    "y = to_categorical(le.fit_transform(labels), 6)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['freetext' 'image' 'json/xml' 'netcdf' 'tabular' 'unknown']\n",
      "json/xml is 22.800684020520613 and there are 2400 files\n",
      "tabular is 25.470264107923235 and there are 2681 files\n",
      "unknown is 22.81968459053772 and there are 2402 files\n",
      "netcdf is 2.4320729621888657 and there are 256 files\n",
      "freetext is 22.800684020520613 and there are 2400 files\n",
      "image is 3.6766102983089493 and there are 387 files\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n",
    "for unique in set(labels):\n",
    "    print(\"{} is {} and there are {} files\".format(unique, ((labels.count(unique) / len(labels)) * 100), labels.count(unique)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "classifier_dim = len(x[0])\n",
    "output_dim = len(y[0])\n",
    "\n",
    "classifier_model = Sequential()\n",
    "classifier_model.add(Dense(128, activation='relu', input_shape=(classifier_dim,)))\n",
    "classifier_model.add(Dense(100, activation='relu'))\n",
    "classifier_model.add(Dense(64, activation='relu'))\n",
    "classifier_model.add(Dense(32, activation='relu'))\n",
    "classifier_model.add(Dense(16, activation='relu'))\n",
    "classifier_model.add(Dense(8, activation='relu'))\n",
    "classifier_model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_model.summary()\n",
    "\n",
    "history = classifier_model.fit(x_train, y_train,\n",
    "                               epochs=50,\n",
    "                               batch_size = 16,\n",
    "                               shuffle=True,\n",
    "                               validation_data=(x_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up figure\n",
    "f = plt.figure(figsize=(12,5))\n",
    "f.add_subplot(1,2, 1)\n",
    "\n",
    "# plot accuracy as a function of epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "\n",
    "# plot loss as a function of epoch\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Results (Binary Classifier):\n",
    " - Trained on cdiac_subset:\n",
    "     - cdiac_naivetruth: 0.87\n",
    "     - nist_subset: 0.77\n",
    "     - cdiac_subset: 0.91\n",
    " - Trained on cdiac_naivetruth:\n",
    "     - cdiac_naivetruth: 0.92\n",
    "     - nist_subset: 0.82\n",
    "     - cdiac_subset: 0.94\n",
    " - Trained on nist_subset:\n",
    "     - cdiac_naivetruth: 0.65\n",
    "     - nist_subset: 0.96\n",
    "     - cdiac_subset: 0.65 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Results (Categorical Classifier) (32,28,24,16,8,6 inputs):\n",
    " - Trained on cdiac_subset:\n",
    "     - cdiac_naivetruth: 0.80\n",
    "     - nist_subset: 0.50\n",
    "     - cdiac_subset: 0.89\n",
    " - Trained on cdiac_naivetruth (adam optimizer):\n",
    "     - cdiac_naivetruth: 0.91\n",
    "     - nist_subset: 0.50\n",
    "     - cdiac_subset: 0.87\n",
    " - Trained on nist_subset:\n",
    "     - cdiac_naivetruth: 0.40\n",
    "     - nist_subset: 0.93\n",
    "     - cdiac_subset: 0.43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1118 21:47:27.192196 4642072000 deprecation_wrapper.py:119] From /Users/ryan/anaconda3/envs/xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1118 21:47:27.204441 4642072000 deprecation_wrapper.py:119] From /Users/ryan/anaconda3/envs/xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1118 21:47:27.211512 4642072000 deprecation_wrapper.py:119] From /Users/ryan/anaconda3/envs/xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1118 21:47:27.228755 4642072000 deprecation_wrapper.py:119] From /Users/ryan/anaconda3/envs/xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1118 21:47:27.301944 4642072000 deprecation_wrapper.py:119] From /Users/ryan/anaconda3/envs/xtract/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1118 21:47:27.315569 4642072000 deprecation_wrapper.py:119] From /Users/ryan/anaconda3/envs/xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1118 21:47:27.382168 4642072000 deprecation.py:323] From /Users/ryan/anaconda3/envs/xtract/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1118 21:47:27.457269 4642072000 deprecation_wrapper.py:119] From /Users/ryan/anaconda3/envs/xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 512, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 481, 50)           1650      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 240, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 209, 50)           80050     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               6528      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 102       \n",
      "=================================================================\n",
      "Total params: 110,302\n",
      "Trainable params: 110,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7894 samples, validate on 2632 samples\n",
      "Epoch 1/20\n",
      "3424/7894 [============>.................] - ETA: 6s - loss: 0.8618 - acc: 0.6294"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, GlobalMaxPooling1D, Reshape, GlobalAveragePooling1D, Flatten\n",
    "\n",
    "\n",
    "classifier_model = Sequential()\n",
    "classifier_model.add(Reshape((len(x[0]), 1), input_shape=(len(x[0]),)))\n",
    "classifier_model.add(Conv1D(50, 32, activation='relu'))\n",
    "classifier_model.add(MaxPooling1D(pool_size=2))\n",
    "classifier_model.add(Conv1D(50, 32, activation='relu'))\n",
    "classifier_model.add(GlobalMaxPooling1D())\n",
    "classifier_model.add(Dense(128, activation='relu'))\n",
    "classifier_model.add(Dense(100, activation='relu'))\n",
    "classifier_model.add(Dense(64, activation='relu'))\n",
    "classifier_model.add(Dense(32, activation='relu'))\n",
    "classifier_model.add(Dense(16, activation='relu'))\n",
    "classifier_model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_model.summary()\n",
    "\n",
    "history = classifier_model.fit(x_train, y_train,\n",
    "                               epochs=20,\n",
    "                               batch_size = 16,\n",
    "                               shuffle=True,\n",
    "                               validation_data=(x_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up figure\n",
    "f = plt.figure(figsize=(12,6))\n",
    "f.add_subplot(1,2, 1)\n",
    "\n",
    "# plot accuracy as a function of epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "\n",
    "# plot loss as a function of epoch\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from ml_utils import grab_labels, feature_from_file, translate_bytes, features_from_list\n",
    "\n",
    "validation_set = \"/Users/ryan/Documents/CS/CDAC/xtract_autoencoder/automated_training_results/nist_subset.csv\"\n",
    "\n",
    "validation_labels, validation_paths = grab_labels(validation_set)\n",
    "validation_labels.pop(0) #Gets rid of headers\n",
    "validation_paths.pop(0)\n",
    "\n",
    "# for idx, label in enumerate(validation_labels):\n",
    "#     if not(label == \"tabular\"):\n",
    "#         validation_labels[idx] = \"blah\"\n",
    "\n",
    "validation_features = features_from_list(validation_paths)\n",
    "\n",
    "#validation_encoded = encoder.predict(translate_bytes(validation_features))\n",
    "validation_encoded = translate_bytes(validation_features) / 255\n",
    "validation_labels = to_categorical(le.transform(validation_labels), 6)\n",
    "validation_predictions = classifier_model.predict(validation_encoded, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ml_utils import convert_to_index, plot_confusion_matrix\n",
    "from pycm import ConfusionMatrix\n",
    "import numpy as np\n",
    "\n",
    "# apply conversion function to data\n",
    "y_test_ind = convert_to_index(validation_labels)\n",
    "y_pred_test_ind = convert_to_index(validation_predictions)\n",
    "\n",
    "# compute confusion matrix\n",
    "cm_test = ConfusionMatrix(y_test_ind, y_pred_test_ind)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# plot confusion matrix result\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_test,title='confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "validation_accuracy = accuracy_score(y_test_ind, y_pred_test_ind)\n",
    "validation_recall = recall_score(y_test_ind, y_pred_test_ind, average='micro')\n",
    "\n",
    "print(validation_accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "file_autoencoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
