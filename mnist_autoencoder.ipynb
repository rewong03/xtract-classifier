{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avV9z5bOcWFS"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "hK5cWVb0ciNi",
    "outputId": "bcfdb04b-6230-40c2-f8e8-675b00c349fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of features in reduced dimension\n",
    "encoding_dim = 32\n",
    "\n",
    "# input layer\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# encoded layer\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# decoded layer\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# maps input image to the encoded image\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# maps input image to its reconstructed version\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "3RiRLap2e9_P",
    "outputId": "d5f2ad2e-fa16-404a-ce19-65a1ae139cfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input layer for encoded image\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# decoder layer\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# decoder model\n",
    "decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NODByBGqf12H",
    "outputId": "ce9e7269-1c3d-4ab2-9d8c-72f00b51e826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "x_train = x_train.reshape(len(x_train), 784)\n",
    "x_test = x_test.reshape(len(x_test), 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "D0UPjS9shJfk",
    "outputId": "ed5d5da2-0773-4e21-9428-6d4e2db742ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.3505 - val_loss: 0.2709\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2633 - val_loss: 0.2518\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2421 - val_loss: 0.2304\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.2227 - val_loss: 0.2124\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2069 - val_loss: 0.1990\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1954 - val_loss: 0.1894\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1867 - val_loss: 0.1815\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1796 - val_loss: 0.1749\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1735 - val_loss: 0.1694\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1681 - val_loss: 0.1642\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1633 - val_loss: 0.1597\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1588 - val_loss: 0.1553\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1548 - val_loss: 0.1514\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1511 - val_loss: 0.1480\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1478 - val_loss: 0.1448\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1448 - val_loss: 0.1419\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1419 - val_loss: 0.1391\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1393 - val_loss: 0.1367\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1369 - val_loss: 0.1342\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1346 - val_loss: 0.1322\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1324 - val_loss: 0.1298\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1304 - val_loss: 0.1278\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1284 - val_loss: 0.1259\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1265 - val_loss: 0.1241\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1248 - val_loss: 0.1223\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1231 - val_loss: 0.1207\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1215 - val_loss: 0.1191\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1199 - val_loss: 0.1176\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1185 - val_loss: 0.1163\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1172 - val_loss: 0.1149\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1159 - val_loss: 0.1137\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1147 - val_loss: 0.1125\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1136 - val_loss: 0.1115\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1126 - val_loss: 0.1105\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1117 - val_loss: 0.1096\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1108 - val_loss: 0.1087\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1100 - val_loss: 0.1079\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1092 - val_loss: 0.1072\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1085 - val_loss: 0.1065\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1079 - val_loss: 0.1059\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1073 - val_loss: 0.1053\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1067 - val_loss: 0.1048\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1061 - val_loss: 0.1043\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1056 - val_loss: 0.1038\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1052 - val_loss: 0.1033\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1047 - val_loss: 0.1029\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1043 - val_loss: 0.1025\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1039 - val_loss: 0.1021\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1035 - val_loss: 0.1017\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1032 - val_loss: 0.1014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c49401eb8>"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "               epochs=50,\n",
    "               batch_size=256,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cAo2v9nKiQJ3",
    "outputId": "12ad2b6b-500d-428e-fa75-4840875febc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09862608340929965\n"
     ]
    }
   ],
   "source": [
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "k5S_xdMtkH8_",
    "outputId": "743bc8cf-dd24-435e-c2e8-a5a8c85f8058"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAACzCAYAAAB1lj1JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXd8VVW6/p9zUiAJRIooIFVsKCCg\n2BFER8SKbXDsBVGu452PbUYHvero6DjjjON1roO9YAdlEHSsSLGgICgjzUIRUAQBCUkIIcn5/bF/\nz7vXKTkJcFq2z/efwMk5J2vtvfZa73reskKRSARCCCGEEEEhnO0GCCGEEEKkEhk3QgghhAgUMm6E\nEEIIEShk3AghhBAiUMi4EUIIIUSgyE/2y1Ao1JRTqX6MRCLtkr0h6P0Dgt9H9S+nCfwYjUQioYbe\n05T7B43RwPcPCGYfg6zcrMh2A9JM0PsHBL+P6p/IdYJ+D9W/pk/CPgbZuBFCCCHEzxAZN0IIIYQI\nFDJuhBBCCBEoZNzsJM2aNcOcOXMwZ84c1NbWora2Fv/617+y3SwhhBDiZ4uMGyGEEEIEiqSp4Kni\nqKOOAgB89NFHAIB9990XJ598MgDgpJNOAgC89tpr9v4PP/wQAPD+++9nonk7RLNmzQAA9913H/r2\n7QsA4CGkn376adbaJURjuO2223DrrbcCAKZNmwYAOOaYY7LYotRx0EEHAQCGDx+OM888E4A35wBA\nKORlbkciEcydOxcAsGjRIgDAXXfdhcWLF2e6uSIgtGjRAgDQqVMn/Nd//VfU7x5//HF89tln2WjW\nz5a0GTelpaUAgGeffRZDhgwBAGzZsgUAUFhYaAOBDBw40P7N91VWVgIARo8ejQkTJqSrqTvEf//3\nfwMARo0ahalTpwIA/ud//gcAMGvWrKy1S+wcrVu3BgD07dsXw4YNAwDccMMNAIC6ujoAwIQJE7Bi\nhZd9+Ne//hUA8MMPP2S6qTvFoEGD7N+DBw+O+kljJ5cZNWoUAGC//fYDED1/9O/fH4BnwLjGDAA8\n/PDDAICJEyfirbfeylh7RXDhWsZ54uabb457z5VXXokXX3wRAPCb3/wGALBhw4YMtTB3eeGFFwAA\nkydPBuDZC6lCbikhhBBCBIq0KTf33HMPAN/tBABFRUUAPBl43bp1AICysrKoz4VCIfsM3//YY4/h\nyy+/BADMnz8/XU3eLtq3b2//fueddwBIsWmqFBQU4LrrrgMAXHXVVQCADh062O+p2HD3T1cHAOy6\n664AgEsvvTQjbU0VVGkSvdYUlJuxY8cC8O9JZWWluZTuv/9+AMDixYttnpk4cWIWWplaeH/OOOMM\nAN447NixIwCYi238+PEAgD/96U+Zb+DPlJtuugkAcOONN9b7nry8PJx77rkAYJ6MSy655GetHobD\nYbsWCxcuTP33p/wbhRBCCCGySMqVmwMOOAAAcNZZZ9lrq1atAgBceOGFAICvv/4aP/30EwCgvLw8\n6vPhcNhiV+i7LC0tteDHkSNHAgA2btyY6qZvFy1btgQAbNu2zZSbnwMMnr7jjjtw4oknAvDuGRAd\nkzJmzBgAwPfffw/AC1Z99913AfgxVbnCFVdcgTvvvLPe30+fPh0AcPTRR8f9jmO6qSk3ibjtttuy\n3YRG88orrwDwgoYBT6UZMGBANpuUFqgQv/LKKzjkkEMA+EHRq1atwpIlSwAAXbp0AQAbxytWrMDz\nzz+f6ebGMWzYMCuNUVBQEPd7zgWvvvqqvcZ4tvvvvx+HHnooAODHH38EkJtJJsuXL4/6fyQSwf/9\n3/8BABYsWADA6/sf/vAHAP49nTRpknk4/vznPwPw40x/DvTr18+U73SQcuOGi37btm0BeDeaN7Ax\ncnddXZ1NsoWFhQCA66+/HqeffjoAL+ociM6uyiSUgS+77DIAXmYXJeGgUlBQYAGoTzzxBADPbUOX\nQCK3DSetzp07A/Ak9YsuuggA8Mwzz2Su8UmgIX7LLbck/D1lZro5ODkxcFBkj9GjRwPwM6O6du1q\nC/y3336btXalCk76nOf69u1r/briiisAAB9//DE2bdoEwH/OJk2aBAA4++yzLYD17LPPBgDMmzcP\nX331FQD/WU03Xbt2TWjUEIYejBgxIu5311xzjX2Wc8zHH38MwNtA0ZVB44KGXqahgU3Gjx9vQcMu\nn3/+OQDfRdqmTRube3r06AHA3yRt27Ytbe1NNfvssw8A4N5778XVV18NwDdQG8t//vOflLdLbikh\nhBBCBIqUKzes/0Keeuopk+i2l9///vcAPKu+e/fuAPxgumwpN4nS/LaXww47DIC/2wJ8q56B07lE\n//798cYbb0S99v333+PXv/41gHgptWvXrqioqAAAPPDAAwCA6upqc1FlGyo2d999NwBvl8ydLHcc\np556qtU/4a6R7tKJEyeajM4d9vz589GnT58M9WDnuf32283VS6iYNgX3FAOFmdp955132r0IgnJD\ndZBu4O+++85q9VRXV8e9f+XKlQB8lWbr1q3mNn7uuefsfUxbzpRr+LHHHjMVYq+99gIQfX+aN28O\nADjttNPiPtuzZ0+0a9cOgO/6Pvzww6N+AkBVVRUA4C9/+UvcmM4EvM6cJ+pzcdOlxr7efffdVgOO\nwcbkkksuQU1NTVram2q4np188sl46qmnADROueF4AIDVq1envF1SboQQQggRKFKu3Nxxxx1R/6eP\ndGd48803ceWVVwLwrcRs4aa2A97OpDH885//tM+zUBz9zYCfEn/fffcBiL+O2YAKhxvsx6Dgm266\nqd5Yo44dO5rvv1WrVgC8XRU/m21Y5I33MhwO2274wQcfBOAHArpwB/rJJ5/gySefBABLIe/du7ep\nCCwwl8tkY4ebDrijD4VC6Nmzp/07FqpwTSFg85xzzsG1114LwC/01rNnz4SKTSzffPMNAGD//ffH\n008/HfW7SZMmmcqRKbZt29aoOZLznkuvXr3wi1/8Iuo1KhyMtQJ89ec3v/kN/va3vwGAxSJlAiaU\nMK2ZqnV9sAL/b3/7W/NAcE1g/yZPnoyXXnopLe1NNew3sH0KzKhRoyyxKB1xqyk1bvbcc08LuOXg\nSkWg0NSpU824ySbFxcXIz/cuGW8iFzmX/Px8W0AZPMYI+XA4bJI6H4r+/ftbMCQXxqeffnq7g7JS\nDYPddt11V3sIOel+/fXX9X6uV69e6NevX9RrsW6tbMLKw25ANIPdWXG4IRhszO/q1asXDj744BS3\nVNQH3RXMnoxEIiaJu1WJ+W8+h6yAmst1b/r06WNGG43s2KzShmCGqsvmzZszFkicCr744gt88cUX\nUa9xk7jHHnvYM8jkjtLSUtts0IWcCWg4u4s84fg899xz8dBDD8X9nhltscc17L333qluZsph8tCx\nxx4LAHjppZfwySefNPrzBQUF5spLhwtObikhhBBCBIqUKjfnn38+9txzTwDAyy+/DMCX4ILAyJEj\nsfvuuwPwAxldqFqNGjUqLvD4u+++AwCMGzfOXB/u7oquHwandejQIWvKzSOPPALAD06sqKiwXVIy\nxYZpmzfddJPtmFkjhj+zTdu2ba1eiMu4ceN26Pv4OZY7EOmnXbt2Np6oeM6dO9d20G4tlMsvvxyA\n78ZgQkIkErFxkGsuK6YFAzs+roYOHRrl9gbQZNwcyaBb7ZtvvrFrQ+Vm8+bNCZX0dDNnzpyo//fp\n08dcZf/4xz8ARJfTaAwjR4601Pa3334bQGZdbY1h//33B+CpaIAXgkIlJhkMVejZs6f1LR1IuRFC\nCCFEoEipcnPOOeeYdcnCZ0HCjSNhMSwXqjVXXHGF+bZ5Yvg111wDIHGgan3fly0YO8I+lJeXJz37\ng4oNg6AHDhxon2Xhu1zhoIMOQrdu3aJemzlzZkpKCzAokOdS5Urqe9DYd999LS2alYqpMsZChZVp\n4ueffz4Ar/Aa4wM4ts8++2w7nyobFBcXA4AVLAV8xbexsPDpXXfdZf9mvE5s/EpTJzZ9vGXLllYZ\nnxV/MwErMFO1mDp1qin8VJqSFTJMRJcuXawIIxXFUaNGWaJGLqiMTGMnjVXnWbCxbdu2mDFjRsrb\nRVKeLcXJIRfLZO8sdDvFwgqNbpVNunZYqbIxmQ6AHzXeFKoe00hgMByDjQF/Yf/ss88y3q5kuFkW\n5NZbb03JcR6sW9SrVy8ATc+4aQr1bQBvbsnLy9uuz7B8/9///nf7yeB9uq5mzJhhAeKffvppqpq7\n3Wxv3wB/8WRQK8MDAL+qe7YTFFLJnnvuGTdey8rKbN7NJMx0dSuv06A877zzAAC//OUv0aZNGwB+\n6EFjodH7zDPPmIHKrKr6NsvpplmzZjbvM6OvQ4cOePTRRwHAjLuSkpK4Y2vcbEa679KB3FJCCCGE\nCBQpUW5KSkoAbL/01tRo2bJlwhoaPE+DgVLPPfecnX2zPd8N+LVUGqv0pAPK9L179wbgyYfz5s2L\nex+lfipabpopa9qwjkGuUFxcHHcPUxHsHA6HGxVMJ3IHuqzo2po+fbq5J/n8ZjJlnOmwy5cvN1X0\n+OOPB+BXMI+FLtALLrgAgF912yUbQbbp5pRTTrF1hzzyyCNZP1A5Fo6n1157zRQ5zvWAr3Bw7ly7\ndq397vbbbwfgnzdVXFxsqjDr+fzud7/LijrevHlzOzWATJ482eZABukvX74c//73v6Pex9Tx5s2b\n46677gIArF+/HgDiajPtDFJuhBBCCBEoUqLc/PKXvwTgpTDSt51KTj31VPt3Ns/biEQiCYtgcffE\n3/H/jaVjx46WzshdZDZh4anS0lIAno+YKk4ieH8uvPBCAN6p4GPHjk1zK3eMAQMGpKWQWV1dXZMq\nkCZ8OGeNHj3aijiy4FrXrl0tTifdUK0dNGiQqadMdz7++OOtvAZTcFu2bImBAwcC8BUAxn/ssssu\ndoYTz50KAjyPyD2/iRWBG1stPpNQ3d5nn32sLIqrZidTthmvycDif/7zn6bcHHfccQA8pY5xYplk\n69atlgSz2267AfCC2FlI01WgYuG47NSpk3kqeNK9lBshhBBCiHpIebZUKmFmy8knn2yv8aTwXIJW\n55FHHmk/b7rpJgC+X58+xUS88sorltrX2PL/6YQnBp9yyikAgMGDB8cdLbBgwQLzpfLUd6Zhfvnl\nl3bGzc8JZkgku9cid3GzpRiHde+992ZMuSGrVq2ylPUxY8YA8LKgmAnF3e6yZcvs2BCW8Z8yZQoA\nT0Vm3BuzWZoyVEDuvfdeAIiKt+FRC9lM44+FcyfHTseOHXHOOecAgKVzNxYqPkcddZRl0TIb7vDD\nD8cJJ5wAILNH3FRVVWHAgAEAYEcSNTTOWOyPJTM+//xzXHTRRQDSk9qek8YNjRqmFrdq1QoffPAB\nAO8QzUzDgNn63E1czHie1Kuvvmo1XzjwaKBt3rzZ/s26OP369TOZddasWenowk4xbdo0m0QTwXO/\n6JaZPXu2nZ8VdOiKA/xU6qaQxj9t2jQMHjw46jW2v6mkhKcDuqhYymK//fbLSjtYsZwbCLeEAd1X\n7jhjOQrWtgGACRMmpL2dmYIV0t0QhaVLlwLIzZpqLVq0AOCvHYWFheZWZH2Y7Z3rN2/ejF/96lcA\ngI8++giA55r83e9+ByDz5/fRBdpYuBbSMJ0yZQrmz5+f8nYRuaWEEEIIEShSotwsX74cgGdZ7ix5\neXm4/vrrAfhF8VavXm2vZSOgmFVCv/rqK3Tt2hWAXyzroYceMkmNRdsGDBhg6gxT4pgm/te//tWC\nh/m5O++805SepkRspV+6ZTIt428PN954o+1wKHU//vjjlm65vfA71q1bl7NB1KLxUKkZPnw4ACSt\nzJ0J6IJqaJdPyd/l448/TkubMgldOazwTioqKuwe5WIJBroJeV/uueceK0GxI0UayYEHHggguhBe\nOtWPVEJ3FEnmDUgFUm6EEEIIEShSoty89957ADyFhenD3NE2lBrep08fAH4J//79+8cFr55//vk5\nsQu57LLLrCgTS2i/+eabVlDJLbd/6KGHAoAFFvP/oVDITntlsGAmC4WlkltuuSXq/5MnTwaQ2zEn\nn332GW644QYAfnGzs88+207vbWzbWeadKbjjx4+3c2RyGcbZxMbbNDW4k2dsl1v6fkfp2rUr/vjH\nPwLwS97Xd2ZVrsFg/iAxaNAgS8mPLbx58cUXN4mzsphQcsIJJ+CYY44B4Kc7T58+HX/6058AeEkY\n9cGU8JEjR9qJ8YmKyTY1tm7dmtbvT3lAcc+ePQH4wU0Nna9z2GGHAfCq4BIaRAyqmz17dqqbuUOs\nWrXKgqJo0B1++OEYP3581PtCoVC9NU+eeOIJCwBrylk1BxxwAM4888yo17IR7L0jMDj9ueeeA+Cd\n0zJo0CAAjTNujjnmGDvckPUccu2A0Pq49dZbs92Eneb000+3rBkuHo01btq1a2ff4X4f4G2seD8Z\nKJ5LGTiJ6NKlCwBYoCmZMWPGdgd85gp04U+ZMiWuCjEzM7k25Dq8B8OHD7cq00xMueiii6yydDLX\nGrOREjF79uwmM/dkGrmlhBBCCBEoUqrcjBkzxtKbmRbdWGi5btiwwdw8lOxyCSpRVJxGjBhhVTN5\nuvCjjz4ap9ywemau7wQbS//+/e2MFPa1KbhlAD+FlG61I4880hQN7uzdekpMs2Vdh/vuuy8qQBzI\nfuBpQyRzR1EuT3eAXyoJh719GU/2PvPMM626NyX7/fbbz1RgBp/yd5FIxP7NoP9nn33WzrpJR6X1\ndEA3xS677BL1+qRJk7JazX1H4D1l7RNXteEp7SwPwkDrpkJ5ebndK/bvnHPOsYrDTBlvCNa8oUr+\nyCOPNBkPwBFHHAEg+vlkyYV0IOVGCCGEEIEipcrNxIkTLfCXMTe0TOuDgZk8dbqppNPyTBAGvAGw\nQNWfA7vuuqspNgsWLADQ9IqGsYTBkUceaeOOge3Dhg2z1+jTduPCWAmWMR9Njdtvv73JFuubOHGi\nxb5RkQH82BmqbwsXLrQxyvtERcYN4qeamo4qqemG5/oQ9uGBBx7IRnN2Cqrh9913X9zveMZWU1Ns\nEsHzl5566im0b98egF/0j0rke++9Z0oxg43nzJlj54SlOxg3HcQq/ek+wT2U7LC/UCjUlE8C/DQS\niRyc7A1B7x+Qvj7OmzfPDtNk9VAGeaaQjN1Dyvr77rsvAM9lxVL8sUdivPzyyxZ4vJPSv8YomnYf\nI5FIg2kr6ezfSy+9BAAW3M/NJV0AKSAjY7S0tBTLli0D4NdDCYVCmDlzJgC/rlgaXG16BpGZPl53\n3XUAYIe9nnvuuanaUCTso9xSQgghhAgUOXm2lMh9Fi5caMpNENi0aRMA4JNPPgHgH3wnRC7D+jZU\n4Oneb2oce+yxcRVsZ86caSnuTS04WsRDBTxTh0NLuRFCCCFEoJByI3aIN954w1Ibc6XIohA/N5g+\n3dRZuHAh1qxZA8A7ww8AzjvvPKxevTqbzRJNGBk3YocYN24cxo0bl+1mCCECwJIlSxpd60WIxhAM\ns18IIYQQ4v/TkHLzI4AVmWhIGujaiPcEvX9A8Puo/uUuQR+jQe8foDEKBL9/QAD7mLTOjRBCCCFE\nU0NuKSGEEEIEChk3QgghhAgUMm6EEEIIEShk3AghhBAiUMi4EUIIIUSgkHEjhBBCiEAh40YIIYQQ\ngULGjRBCCCEChYwbIYQQQgQKGTdCCCGECBQyboQQQggRKGTcCCGEECJQyLgRQgghRKCQcSOEEEKI\nQCHjRgghhBCBQsaNEEIIIQKFjBshhBBCBAoZN0IIIYQIFDJuhBBCCBEoZNwIIYQQIlDIuBFCCCFE\noJBxI4QQQohAIeNGCCGEEIFCxo0QQgghAoWMGyGEEEIEChk3QgghhAgUMm6EEEIIEShk3AghhBAi\nUMi4EUIIIUSgkHEjhBBCiEAh40YIIYQQgULGjRBCCCEChYwbIYQQQgQKGTdCCCGECBQyboQQQggR\nKGTcCCGEECJQyLgRQgghRKCQcSOEEEKIQCHjRgghhBCBQsaNEEIIIQKFjBshhBBCBAoZN0IIIYQI\nFDJuhBBCCBEoZNwIIYQQIlDIuBFCCCFEoJBxI4QQQohAIeNGCCGEEIFCxo0QQgghAoWMGyGEEEIE\nChk3QgghhAgUMm6EEEIIEShk3AghhBAiUMi4EUIIIUSgkHEjhBBCiEAh40YIIYQQgULGjRBCCCEC\nhYwbIYQQQgQKGTdCCCGECBQyboQQQggRKGTcCCGEECJQyLgRQgghRKCQcSOEEEKIQCHjRgghhBCB\nQsaNEEIIIQKFjBshhBBCBAoZN0IIIYQIFDJuhBBCCBEoZNwIIYQQIlDIuBFCCCFEoJBxI4QQQohA\nkZ/sl6FQKJKphqSBHyORSLtkbwh6/4Dg91H9y2kCP0YjkUioofc05f5BYzTw/QOC2ccgKzcrst2A\nNBP0/gHB76P6J3KdoN9D9a/pk7CPQTZuhBA/Y0KhBkUVIURASeqWEg2Tn5+PSMRT9PLy8gAAkUgE\ntbW1AIC6urqstU2IoBMOe/uzurq6qH8TGjh8RoXIJBx/oVAobiy6Y1LjM/VIuRFCCCFEoJBys51Q\nneHPVq1aYa+99gIANGvWDACwZcsWLFu2DACwadMmAMDWrVsByEIXuYG7k6TiQWpra3NynLpuptjd\nb35+vvUjPz8/7n18/qSkinTB8dm8eXMbgwUFBQCAmpoabNu2DQBQXV0NIJhrQWNcwW6/G+s63pFr\nJeVGCCGEEIEibcoNlY2ioiJ07NgRANCvXz8AwNFHH42ioiIAQPv27QEAFRUVAIDvv/8eH374IQDg\ns88+AwCsW7cOZWVlADwLmGTa8g2FQqbO7L///gCAY445BieddBIAoKSkBACwceNGvPrqqwCAd955\nBwCwYoUX0L1169a43WOiHanILuFwGIWFhQCAXXbZBYB/n8rKymz3xdiqXL5v4XDY2r7rrrsCAM4+\n+2yceeaZAHw146GHHgIATJkyBZWVlVloaWKoyDRv3hyAd615vVu0aAEAGDx4MC655BIAwL777gvA\nex45r7z55psAgLfeegsAMHfuXJSXlwMAqqqqAPg7ahf3vmZT9Um0w83lMfdzgmvdwQcfDADYc889\n0a1bNwDAmjVrAHhjd968eVGvrV271hRFziONJRdjycLhsD2rbJ+rAse2NRQKmbLFubawsNC+g89j\nZWXlDs2zKTVu3MZ2794dAHD33XfjmGOOAQAUFxcD8AZDbIfdi3HxxRcDAH744QcAwJNPPol//etf\nAHwjobq6Oi5oN1032g0KizVuhg0bZn2lFLlgwQLMmTMHALB69WoA0UZZY6S4UCiUUwM36PCB6tKl\nCwDgt7/9LY477jgAQGlpKQD/YVuxYgXGjh0LAGbElpeX5+z9ikQiNm579uwJALj44ovNncrF/fDD\nDwfgGQBbtmyxz2aT/Pz8qLkB8CbAVq1aAQBGjhwJABg9ejRat25tnyE0YE444QQA3sIDeMbNpEmT\nAABfffWVvZ+ug/om5EzBPrMvhYWFtoi6/ePcx0WS80yyRSXTxC7ErrGdCLfd2W57Q+Tn5+PII48E\nAIwYMQIAcMghh9j4JNXV1bYmvPvuuwC8zfvSpUsBwDYTyYycREHJ2cK9f5w7XeMmWbA0P9usWTMT\nBGgblJSUYPPmzQD8a1FUVBQ3vhuz0ZBbSgghhBCBIuVuKbqbfv/73wPw3Da0zmjBbdy4EcuXLwfg\nuaEA2K6rS5cu9h20fs844wxTcfizuro6LiBye6W9xuLuONq0aQMAGDBgAACgTZs2FjT8/PPPAwCe\nfvppa2dj5TRXHWrM+7OFu6Ns2bIlANg1+emnnwB4QdSJ7kUiCz72tWz0Oz8/H4MHDwYAjBs3DgDQ\nrl27qNR+l06dOuGQQw4BALzwwgsAgKuuuspUglyE961t27b2k88Z+/fdd98B8HaR2R5/vPYFBQVx\nz1BBQQH22WcfAL4iU1RUZLs5urDffvttPPnkkwD8vlF9++mnn+zf3BVu27YtqQqcqWczHA6b0rb7\n7rsDAPbbbz/07dsXAHDggQcC8MYhn71Vq1YBAB555BEAwMyZM7Fx40YA0XNQpp+3UChk44yuh+Li\n4rgdfn5+vrkQ+Vp5eXncTj3b45Kw/QcffDCuueYaAECvXr0AAB07drTxy3ZXVlZaCAZV/7KyMrvP\ndFVR6U80f4ZCIfte9/fZVhdjf7pEIpG419mHdu3a4bDDDrP3AcCcOXNMSaaK6vbV/VsN9VvKjRBC\nCCECRcqVGwbzHXrooQA8vxp3tC+99BIAYMyYMbbLJ7RgO3bsiPPOOw8AcOKJJwLwAjqHDh0KABaU\ntWjRooynd4bDYVOYOnToAMCztCdOnAjAV264A0lEfX7T2EAsd5eV7fTVUChkfn7uGm+88UYMGjQI\ngH/vqMY98MAD+PTTTwEgajdNNcsNoou1+tOlviWC13zYsGF44oknAPgqVCQSsXYy/oR+4cLCQtuF\nnnbaaQCAxx57DO+//z6A7N+vRLBN/OmmTnOccffvxodlC7aztrbWxgTHSE1Njf3+22+/BeApUVOn\nTgUA/OEPfwDgqTX1+fsTPYfufcvGbthVojnPnHLKKQCAU089FXvssQcAmGIaDofjFAI+kwsXLrR5\niApVKBSya5nu54zt6tevn8V5MaGkdevW1l4+UwUFBaaAU3lbu3atrR1U3pYsWQLAUzr4fHJuys/P\nt3WFYzhd95H9O/roo9GnTx8AfrxeKBSKi4X68ssvbX3gc9a9e3fcfPPNAHyPx5gxYwAAkyZNsu9o\nTHxSOmisUsn35eXl2bxI9cX9PN9HT85BBx2E4cOHA/Bi4PiT49ZV7fjZ7ZlbU2rc5OXlmbzPh7Oy\nshIPPvggAOCPf/wjAP+GuzCgatmyZfZ+DvJLLrkEnTp1AgCTo5csWZJ2dxRxA6D4oFIuXr58uRk3\nyYwatrV169ZmDLgGHh9yDoSqqirrV7Zr5DRv3hzXXnstAN/dyPYC/iBkJs7QoUNtIqMR+PXXX+Pv\nf/87AF+CdclkqXz+Ld7LRx99NMqoAYDFixfj+uuvBwCsXLkSgJdhBADXXnutZelwUjrppJPwySef\nAIh+sHONDRs2APAkfy56lH9juGH4AAAbPElEQVR//PHHrLWrPmpqauImx6qqKjM4GZA5Y8YMjB8/\nHoDvuk72vLibh4ayFTPljnIXCQY+0/3ZuXNnczMx82vu3LlxBjiNgLKyMntGmWVWV1dn70v3nNm1\na1cA3jN2xhlnAIBlzRYVFVmQvrto0WAgdXV1URsKwJ9jly5dGhWiAHiJHM8++ywA/5lN5NJI5X3c\nf//9bQ5wg7lplD366KMAgCeeeAJr164F4CcoDB061NYz9u/CCy8E4AX1x64nkUgk7UZb7N9LBq8n\n296pUyczNFnnLRG8z0cccQR69+4NwM+MdjNRE1Ua3x4jR24pIYQQQgSKlCo3LVq0sJ0GLbiFCxfi\n4YcfBpC4jgRxrerYHcqJJ55okizdXu+++27Gd8ilpaU49thjAXjBUADw4osvmpzq4qo9gC8Xs7YI\n4KcEbty40QI9ad2vWrUqbmeSaeWGaf3nnXeeyafsT3l5ubmennvuOQD+LnqvvfYyuZH3q1mzZvbZ\nRGmCmXRHUVVkOne7du3sftHNcdJJJ9nuj2383//9XwDeLtq9j4C3U6Wax8/lknuKO77169cD8Hb6\n7DPvM9Wo+lS02NcztXtM9Hdid7BuenSiNFW3UizQ+IDMRIGL6S7TUFRUZMHDdHUsWbIEd999NwDg\niy++ABCtgMdWZ27WrJkpJdwdr169GgsWLIj6bKr7QZWIys2BBx5oKgZVo3A4bKo1x2NlZaXdIz43\nHTp0MBcc1QGy9957WykD9qFDhw7mGqaCVVdXl5aUeKoP06dPNwWY7V66dClee+01ADAvREVFRVx6\nv5suzjl+1qxZ9l25nlzC+9WjRw8AnguVbkOuBck+t8suu1jfWI5h06ZNCedNVSgWQgghxM+elCg3\nbvAY0+G4M/roo4/Mz5/oTIlEu0F+lmlxK1eutB0MFZzddtvNdgTpDoCk4nDkkUfiqKOOsnYCnt87\n1tIMh8PmV2WqKgPFunXrZrsmxnl8++23tvOi5btlyxaLOeLPTMG2cPd1ww032DXmzu/SSy/FwoUL\nAfg7Fo6DqqoqXHfddQD8OJzvv/8+LhYiGzuSZs2a4fzzzwfgxzOEQiGLO7nxxhsBePckNqCPCt1d\nd91l/nDGmHXv3h1HHHEEAL+wn1vlN5u7r0gkYn1x09XddGvAf7YSKTdu4bVEgY6p6l9jdqv5+flW\nAXbIkCEAvLghFv967733AHjX3019B7xEBMBTDBJVCk8Uh5OOHbT7nbE7+j322AMDBw4E4D+LY8eO\nNaWUY9X9ntiA/NLSUnsGqTZ/+umnuPrqqwHA1PFUw3mCJT5mzZpl44pzx+rVqy2olkrp5s2bra9U\nEAcMGGAB+yyUyni9qqoqe40/99hjD1PUeS3da5VK6DV45513LB6U92D27NkWIMs5IBKJ2HPWv39/\nALC1BEDcWlZUVBQX/O2qUMQds7x+mVCMw+GwKU/nnHMOAK9frMifyNsQ+xxVVFTYnNrYZIaMVyhm\nQNXxxx9vkwgv8Pvvv28DzH0QYzM1EtUx4AVyH8TddtsNgDdZsVpxumB72b8hQ4aYS4PS8JIlS+IG\nV3FxMYYNGwbAD8DlZFxQUGDGHieA2tpau26sgbB8+fK4TIhM4FZhvuqqqwB4bhvKvWeddRaAxMHT\n7P9ZZ52Fzp07R71vwoQJWa16y8muXbt2uPzyywH4izrgP1yvv/46gMTXnJPMypUrrb4NAwK7d+9u\n12b27NkAoqXZbEvLsX+/VatWUa4cwA+Sdw/STFbDItH37iyxxkWiyTEvL8+qR9MlUFBQYMGndMcA\nXn0YwJ9DPvroIwBeADJdIq6Lys3S4t+MNcZT3edYF/aQIUPM5cI57pNPPkm4UNdXt2bgwIHmOuX3\ntmzZ0hbldFdz59xWU1NjbloGrG/cuDGuCq1r5PF969evN3cF50JukFq0aIELLrgAgG+Uu5l1mZoz\nf/zxRzOYaaAsWLDANqR8xtq3b28bXSZndOvWLaq6L+BV2ga8mjk8DoVrTbKEFSAzc4xriDNphJu6\n77//3ozVRNc/dpy3bNnS7mdsCEAqkFtKCCGEEIFip5QbWtN0PRx00EFmqdJy//LLL5MGiybbDdH6\nW7JkiZ1Pxd12aWlp2tOH2T/uDA488EBrEwO/EgWKdevWzeRUSqxUaxYsWGBVcOmC2nvvvU06psKz\n7777mluOPzNhmYfDYey9994AvNoagOcio1st2e6B6avnn39+lEQLeDvPTKsXbi0T7hb69etnO3tX\nyn/mmWcARPevPndEdXW1yeMMdGzTpo0F1lEmX7FiRUYDpRuDG3Aa2z8GedbX5kSqSrqCHpPJ2YWF\nhaamuhVbqWysW7cOgPe88l5QSeTY7t+/vx3Qy93mqlWrTNlwxwG/N5WlJ9xrye/lAa1DhgwxlzUP\n+kyUtJAI3t/zzjvP6olQ6v/oo49MMUnXsxhbvmLDhg2m2HJ8bd26NWHl9ti1oKyszFzffD/7Ulxc\nbMo+14S8vDxTT9Lt+ua4q6mpweLFiwH4485Na+d8fvPNN9sBy1QYXYWUr3EO7dy5s9UU49w7ffr0\nuEDwwsJCey2dAcixpwG0atXK1FO65b766qs4ddEd33xmqXbvvvvu+PLLLwGkx30o5UYIIYQQgWKH\nlRu3ai199bvvvrvtfJjG7QZmup9tjGVNa72srMx2LrTMM3H+DS1OKlPhcNh2IazG61YJZRDx5Zdf\njoMPPhgAzKr/85//DMBTMGhp83PhcNj8sQyI6927t+0sM5ESyL/RvHlzXHTRRQBgZ6F8++235htN\ntGvnLpPn2rRu3dr85q+88goA775lI7XR7RfgVRSNPVOpvLwcL774YtRrDamCHJtMVS0pKbF7x3iJ\nGTNm5IxyE9sf9//cNTEIsr5UzEymRLsxebExbQUFBRZDw5OWX375ZUyYMAGAP0e8/fbbpvgyNocK\nTs+ePe3eUYX74IMPTF345ptv7LvSGZzqBppynmndurX1lQp4QzEkvJ+MCdxvv/1s7PE7nn322bQF\n2BK2k6pKYWGhqUVuZfKGCizyfZxvCfvZokUL/OIXvwDgP4ObN2/OWGkJfn9+fr6NlURKDBMODjnk\nEJuDEs2hxC2Mx/jLBx54AICn4Hz++ecA/HvavHlzWxszURqFa1bnzp3N28D5v6SkxMawe/2pSFJF\nZQBy79697X651yZVc8pOGTeU5DlhVFVVWWVCLhaxgxPwbmqi4wbqo7Ky0iYsBmrVlw+fSthGXviS\nkhJ7aOluat26td3wX/3qVwCAc8891wYag8I++OADANG1fjgAfvrpp7iMo/z8fAsyTqf7zQ3SBDy5\nkVH87HeXLl1www03AIBVGV6/fr1VH77yyisB+FkAkUgEX3/9NQBgypQpAKKj4DNZ8ZULEx86ZvMB\n/mI1ffp0c/01Fn6WC5P7tzhu8vLy0r6YNBbeX066tbW1Nua4+CSrKpppl2KyZ7uystIqv7Ltq1at\nilvQysrKzHChQUtXTfv27W1Txt/16NEj7qDGb775Jq3Zim6pfratrq7O+uJmtMVWdnclfz6rhx9+\nOADvOWamzhtvvAHA25BlKtCW42Xr1q1xBkeyKtCxv4/dbPB6nHHGGTjggAMARD+DsW6pdFNdXR0X\nmA/494PGckVFhbmtuAb88MMPtiGmgcB1JT8/3/rMcXrnnXdaFXS6K2fNmmVrbKLKvqkidpz16NHD\n2sp7csghh+Cee+4B4G/+y8rKbFxz88eNfKtWrayttCHWrl3b6MOmG2zzTn1aCCGEECLHSIlbilJo\nXV1dXGpXIsLhsH021kpzrTVahLvssotZ5/zp1u5IF7GBUAUFBSaBUm5cuXKlWbCXXXYZAM/qZsAx\nU6iTpXFWVFTEHfNeWVlpVTbTeYZWbKpvJBKJC14sLi7GpZdeCgB2RP2CBQusrUcffTQAP2h327Zt\n5o7id9W3W0u3q809KBLwdgtuwCIAPPzww1H1X/jZWNy2chdNZaB79+72Gr+refPmGa+iXV9F3dix\nXF1dbdeG98ityUGyedZSLPx7W7ZssUDE2NTtWLir5U+mf69evdrmFwZxHnrooVZPiy7ZlStXxlU3\nTjWxhywCfn9Yi2n69OlxVV9LSkpsPuLuns9pcXGxjW8Gyyc60y9dz6Cr1iTbibuVlWPvpeu24fuo\ncFx++eU2lvmeiooKU0oyNTbdw0jd1/hMzZw5E4A3jvgaFVL33DSOMdaOGTRokJ1lx+r2bjo5A+P3\n2Wcf/OMf/wDgX6PYQ6lT0UdXjQa8MUslivN+YWGhtYtKzPLlyy2dn32j+pOfn2+vMclj3rx5cWVh\ndnSMSrkRQgghRKBIaSp4fn6+7YxcP2iigEDupJPtDKmI9OnTx/x27qnGiXYiqcK1yLkDWr9+vVmd\n9JV26dLFlCsGD7/55pt46qmnAPgxAYng9SgpKbEzpRirsmDBAktHdQPVUk2sP3zLli12yjmt8A4d\nOtiu1T3Nlzsn94RwwLte06dPB4CoomHJCrSli9j+FRYW2s6AQc+LFi2Ka0teXl7SApOxJ2lXVlaa\nisMdWqJ4s3ThFgNLpGKwD9zV/fTTT7Zb2t52us9zJguHuX8/dgwl2t3l5eXFVe7ldairq7MxzVTw\nHj16WNE/VudeunSpzTnpgm1iIclp06ZZ5WWmEo8cOdJ2wHwGi4qKTClnsCaV1aKiIosj4+cS3at0\n37/6gocTnWvGcch53T1fie8bOnQoAK88hxvXAwBTp061PmcytijRfMbn0U1ASXZWIFVTzh3jxo2z\neEWezTh06FBbE3lmX01Njc3XXH9ShfvcxQaKf/7553jiiScA+KphYWGh9ZuFDefOnWvPGT0bVKJq\na2tN/XE9PalaJ6TcCCGEECJQbLdy41pzVG7oc6uqqjJFhulfbiE1UlNTE3duhmud8TtY3nnw4MGm\nmND65S45XbgxPfybCxcutF0Tf9esWTNLof33v/8NwCve5hZ0AhKfVMxsqO7du9t1YNGqWbNmmSqQ\nzrOzEhXOeumllwD4qkSfPn1s9/r2228D8BQp7ioHDBgAwO/jihUrzHJPpCJkyh9eV1dn15o7om3b\ntsUVBGvTpo2lVhI3Xif2NPtIxD//jGNj48aNtmtkpkCis2DSBZ/F+nbnsSpkZWWlXRvGbfA5bqyP\nO1F6eDqJjb8A/B19ovOWCgsL4+YZl9id/9KlS02F5ve6Zx6lA3ee4TP29NNPW3wNM4IqKipMVeK4\nXbx4sak93A0zhiEvL89UukyfTefS0PjguHXXiURxl1QqGFOUn59v5QCoXNxzzz1xsXPppr5MVrab\nZSE6duxoygqVNDceKZZIJGL9u+uuuwB4RXJZUJbrYZs2bezUd6akpwP2k0r86tWrrfRCohg9eh1q\na2vt+WH86YUXXgjAU7Xuv/9+AH5BWzcOaWfZYbdUJBKxSYEdqampsdd4Y9wJsDG1bUKhkMlcPJep\nU6dO9rDzbJitW7emXXqMrRMxf/58G6xcCNq0aWPnv3BRq6qqst9TRiSlpaUWwMgKlMXFxfbZefPm\nAfCMN1eezRS1tbU2KfIAyNdff92uRaJFhIsjH+glS5Zk1CVTH+7CQdl37dq15m5juw877DAz5LjA\n1NTUxKUFu3Dxo4FaXl5uAeCJzixKNw25HGJdw+3bt48K2Af8vqxevTrjwcLJiDVqCgsL7d5wPCaq\nGeJOlMnKKfA+bdq0yVxUpLy8PKEbIZXwe/l3Vq5ciddeew2AvyDstttutunjxm7Dhg3mEh4+fDgA\n/xkEYDVRYo3zXCC2z7W1tXHPWygUsvnzjjvuAODPmZWVlbjzzjsB+AHT5eXlGR+3iQ5YDYVCtgmm\n0Tl8+HDrK0MWpk2bZvMO11B3fmVfaAytW7cu7pDOuro6C33gfJaqQ1Hdaxk7l1VVVcVVS3ZJ5Mqn\nYcT2bdq0ycY5+5/KtU5uKSGEEEIEip0KKKal7abfUS5jgb/6UmpjX+eurEOHDpbaRit969atllLH\nE1IzURyNViRl3ZkzZ6JPnz4A/PRRwJeO2ffi4mIrFkc5lQGK7dq1MwuWwcOTJk0yxYZujm3btqXl\nTJvGEBt8uW3btjjVzS3iyGBvfm7KlCkZVZuSEet6mDt3Lvr27QvAb/fxxx+P+fPnA/DPh3HPv4kl\nFArZ/eQp9atXr7aq3PyOhiqxphL3/iQL4HRl8tj0cKZv8lo09m+mkkSBwrHng5WUlMQF6icKWHfH\nYLK2chy3b9/e3FJ8DhOlI2dCMXaLv7E9sWpHJBIxhTR2t15XV2dzZiafxe0NAuUakuhZCYfDGDly\nJABYNWIqU++9954FtGa63IKLW5CWFBQUmHLDMhm9evWydrLYa1lZmXkk+JP33b1n9GR06tQpbi2o\nqakxz0K6QzVc3GBvl0Temdgq8VS233///bjwjVQi5UYIIYQQgWK7lZtEFhn9om3btrWANlraL7zw\ngikf/GxeXp59hjtgFiwaMWKElfWnVT9r1iyz0mO/KxPQSl62bJn5d3muRmlpqZU8Z4Glvn37Wh/o\nByVbtmyxwK+pU6cC8Ap0JTqROTaNNdMki5HKy8uztEw3oBwAPv744wy1sPFwtzt+/HhLl6UK16VL\nF9tpUS1Lds1LS0sxatQoAP6J8TNnzrTAcjcOJFM0tPPhjpc74ebNm8eVtM8FtS125+9eQ96jHj16\nWKwCA2pdpS1ZfI177AvnID6/RxxxhCk2ieJVMnk/Y1Wa+o4nIJx3eQ8rKiosfigbpRd25nO8RwMG\nDMDtt98OwN/1U6m69tprs6LYxF5L95lx52vGR1FhrKmpsdeokI4YMcIU5ddffx2AH2P13XffmbJ8\n3333AfCL5QL+dZg+fbopN5lIPCH1FQtNBOMTqUBxrXML9qWDnXJLxbptSkpKrAO33norAM9omT17\nNgA/YLF3797mrok9/r2urs4GLYOHr732WnPhZGOh502sqqrCf/7zHwC+m+PYY4/FwIEDAcCyGdq2\nbWtSNycnysYffPAB/vKXvwDwg4erq6sTBj7GZlplM8gztpJxt27dcPXVVwPwZX1OpgzezSXcs4LG\njh0LwB+j4XDYFgf3kMTYIFy6oG655RYzbnlfZ8yYkZVAYtLQ2ODiQBeqO854vxjUnk2STXY0ogcN\nGoSDDjoIgF/bY/78+Vb51U1woAHD7+U97NatG44//ngAvot5yZIllq3oZns0JhEi3dT3t+lS5CaR\nC9zatWttEUnn2XSpwq2Cyw3Diy++aP0jXPxjA78zRbLAfdfgYZLJvffeC8AL/mbVa96jjh07WoAw\nn0ue6zd37lzbLLPmUV5enj2r7P+HH35oVZuTnQ2XTpKNr1AoZGs8N/x8tqqqqrbrjMntRW4pIYQQ\nQgSKnVJuqEowcO2ss86yqqcMzBs0aJCdw5TMwqM1+91339nJos8//zwATxmK3dGl+1wiF/6dbdu2\nmeXMADDXmubPkpISC+7iKa48Ffzrr7+29yXbpTYkQ2catoc7qdNPP93uMSVSBtTW1tbmhNqUiK1b\nt+LDDz8E4I+vgQMHokePHgD8k83Xrl1rOyIGBTLdtlu3biY3U06eNWtWTqbcAp7yFOt2c2tNsZo0\n6/Rk857FusjctlAZW7lyJU477TQAfkVeN6jTDbh1KzIDvvpDhRnwFavXX3/dXKp0e2XLJdwY8vPz\nrRZYz549AURXzuaOmX2ur3xGJgLDGyIcDpvb5bbbbgPgKziAf14b3aq54EIlifrIOZFei3nz5pl6\nSg/GZZddhhEjRgDwxyPnmgEDBpjqyLFbXV1t5SYeffRRAMDkyZNt7cy02zTRPY71QOTl5dncwzmT\nqlZlZaU9767nJlX9kHIjhBBCiECxwxWKQ6GQWYzc+fz617+28yMuuOACANHFp1w1gjsi+g5prb/8\n8ssJTyeOJRu7y7q6Oou14S6yrKwMn332GQDg8ccfB+DtnmILfyUKQEtELvj3E8F7x91VmzZtLN6E\nJzRPnjwZQG7tqhLBGDEWKQyFQpYizQDGdu3aWXE77oDZry+++MLOfeF3uKm6uQh3TYw3GjJkiO0C\nH3nkEQD+7jib/eC84Co4fI2++gkTJli6+nHHHQfAKxvBeBqe7O2mq3KeoYqxbt06vPHGGwD8yuIb\nNmywHXeyAo7ZIvacpU6dOmH06NEA/H6xvZs3b46rIF9TUxPXn3QXJkyWLuz+vqioyJJQGM9WW1tr\n8yiVCo7ZxpJJhT8RrupPVY3P2dixYy1OkzGojDddsWKFFRTlmFy2bBneffddALCzCGtra9N6xiJQ\n/zVsqBgv4CkyVGUYrM/+r1mzJi7mJpX3a4ezpdwGcAB+8803GDNmDADg5ptvBuAFm9KVQXmqvLzc\nyuGnM8I7HbDfycq6B41wOGwTJN2OHTp0sMqZlPJZsyBX3TPELbcPeBVDuSByYu3Rowf22WcfAH6/\nZsyYAQB47bXXbLHkYpHJmjbbi1vF9OmnnwYAPPvsszk9hhPNM261aW4o+DMUCtlizsW/uLg4zlhx\n5fLY/ufyPQSij5UAvCBiZtRwUeTPb7/91hYRvubOtZnsp1s1GYhe8Oh6Oeyww3DJJZcA8I2xyspK\ne85efvllAI1vdy4HUbMPa9aswQ033ADAz7Tl3LRo0SILbXAzaXltuLl2vy9X4WZywYIFAPzxu379\n+oRzT6pCGuSWEkIIIUSg2KmA4kTE7riqqqrizpQQTQPurgoKCiwYjjUaCgoKzN0YK4vmsnKTSHFc\ns2aN7ZLmzJljv4+tSJtLLortpakpjtt7rSORiCkT/JkL55ulEl4T9mvx4sV48MEHAfg1m+jSnzZt\nmgVuZqPuEnHvi/s8ce5w04RjD3ctLCw01wzrhVGVa2j85rqaAUSnjNM17NbKSVRHhq7ZbNRc2tHP\ncrzSLeWqarFjs74K6zuClBshhBBCBIqUKzciOHCn6KaQzpo1C4B3oivVDv5kXEeuxy4kw1UMmrJS\nI4ILVYs1a9bgrbfeAuDHg7nnwWVTsUmEW5SUag538/PmzbPYEp791rlzZyvkumjRIgCIUoH4fZk6\n8yvVJDt124X9i0QiTaaPiWLmYhV+N8A9UemHnUXGjWgUnChZC4U/hWiKZDuLJhVEIhF7LjNxkHCq\ncF0PXNTnz59vFdvdelmxx6EkWgSbyoK/ozTF/rn3l64010hzf6YLuaWEEEIIESgaUm5+BLAiEw1J\nA10b8Z6g9w8Ifh/Vv9wlZ8doinaNOdu/FJLWMZqoFAgDijOEnkGPtPQxQwpbwj6Gmro0K4QQQgjh\nIreUEEIIIQKFjBshhBBCBAoZN0IIIYQIFDJuhBBCCBEoZNwIIYQQIlD8PyC5WNXLApSQAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "BwMk8bvmlAkv",
    "outputId": "fb79b303-3a18-44c7-b166-06fbc91dc5b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "# add a Dense layer with a L1 activity regularizer\n",
    "encoded = Dense(encoding_dim, activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-ohf_plxlCYX",
    "outputId": "147a1e53-05c2-4f94-c94f-f38b83f3a606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.6729 - val_loss: 0.6485\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.6284 - val_loss: 0.6090\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.5916 - val_loss: 0.5749\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.5598 - val_loss: 0.5454\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.5323 - val_loss: 0.5198\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.5084 - val_loss: 0.4975\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.4875 - val_loss: 0.4780\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.4692 - val_loss: 0.4609\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.4531 - val_loss: 0.4457\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.4389 - val_loss: 0.4324\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.4262 - val_loss: 0.4205\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.4150 - val_loss: 0.4098\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.4049 - val_loss: 0.4003\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.3959 - val_loss: 0.3918\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.3877 - val_loss: 0.3840\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.3804 - val_loss: 0.3771\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.3737 - val_loss: 0.3707\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.3676 - val_loss: 0.3649\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.3621 - val_loss: 0.3596\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.3570 - val_loss: 0.3548\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.3524 - val_loss: 0.3503\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.3481 - val_loss: 0.3463\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.3442 - val_loss: 0.3425\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.3406 - val_loss: 0.3390\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.3372 - val_loss: 0.3357\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.3341 - val_loss: 0.3327\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.3312 - val_loss: 0.3299\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.3285 - val_loss: 0.3273\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.3259 - val_loss: 0.3249\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.3236 - val_loss: 0.3226\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.3214 - val_loss: 0.3204\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.3193 - val_loss: 0.3184\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.3173 - val_loss: 0.3165\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.3155 - val_loss: 0.3147\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.3138 - val_loss: 0.3131\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.3121 - val_loss: 0.3115\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.3106 - val_loss: 0.3100\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.3091 - val_loss: 0.3085\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.3077 - val_loss: 0.3072\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.3064 - val_loss: 0.3059\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.3052 - val_loss: 0.3047\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.3040 - val_loss: 0.3035\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.3029 - val_loss: 0.3024\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.3018 - val_loss: 0.3014\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.3008 - val_loss: 0.3004\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2998 - val_loss: 0.2994\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2989 - val_loss: 0.2985\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2980 - val_loss: 0.2976\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2971 - val_loss: 0.2968\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.2963 - val_loss: 0.2960\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.2955 - val_loss: 0.2952\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.2948 - val_loss: 0.2945\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2941 - val_loss: 0.2938\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2934 - val_loss: 0.2931\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2927 - val_loss: 0.2925\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2921 - val_loss: 0.2918\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2915 - val_loss: 0.2912\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.2909 - val_loss: 0.2906\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2903 - val_loss: 0.2901\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2898 - val_loss: 0.2895\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2892 - val_loss: 0.2890\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2887 - val_loss: 0.2885\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2882 - val_loss: 0.2880\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.2877 - val_loss: 0.2875\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2873 - val_loss: 0.2871\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2868 - val_loss: 0.2867\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.2864 - val_loss: 0.2862\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2860 - val_loss: 0.2858\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2856 - val_loss: 0.2854\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.2852 - val_loss: 0.2850\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2848 - val_loss: 0.2846\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2844 - val_loss: 0.2843\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2841 - val_loss: 0.2839\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2837 - val_loss: 0.2836\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2834 - val_loss: 0.2832\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2831 - val_loss: 0.2829\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2828 - val_loss: 0.2826\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2825 - val_loss: 0.2823\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.2822 - val_loss: 0.2820\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2819 - val_loss: 0.2817\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.2816 - val_loss: 0.2814\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2813 - val_loss: 0.2812\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2810 - val_loss: 0.2809\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2808 - val_loss: 0.2806\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2805 - val_loss: 0.2804\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2803 - val_loss: 0.2801\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2800 - val_loss: 0.2799\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2798 - val_loss: 0.2796\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2796 - val_loss: 0.2794\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2793 - val_loss: 0.2792\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2791 - val_loss: 0.2790\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2789 - val_loss: 0.2788\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.2787 - val_loss: 0.2785\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2785 - val_loss: 0.2783\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2783 - val_loss: 0.2781\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.2781 - val_loss: 0.2780\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.2779 - val_loss: 0.2778\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2777 - val_loss: 0.2776\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2775 - val_loss: 0.2774\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2774 - val_loss: 0.2772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5dbbbb8e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "               epochs=100,\n",
    "               batch_size=256,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OBZ_wu1-_DBp",
    "outputId": "1ff80eb9-83b6-4449-9aff-b43103047b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3572 - val_loss: 0.2637\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2567 - val_loss: 0.2469\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.2342 - val_loss: 0.2257\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.2230 - val_loss: 0.2180\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.2133 - val_loss: 0.2082\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.2022 - val_loss: 0.1933\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1893 - val_loss: 0.1831\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1814 - val_loss: 0.1770\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1755 - val_loss: 0.1720\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1701 - val_loss: 0.1670\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1653 - val_loss: 0.1616\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1607 - val_loss: 0.1582\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1567 - val_loss: 0.1541\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1535 - val_loss: 0.1503\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1501 - val_loss: 0.1476\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1470 - val_loss: 0.1442\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1444 - val_loss: 0.1422\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1422 - val_loss: 0.1399\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1405 - val_loss: 0.1384\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1386 - val_loss: 0.1361\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1368 - val_loss: 0.1346\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1351 - val_loss: 0.1318\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1336 - val_loss: 0.1326\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1324 - val_loss: 0.1307\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1312 - val_loss: 0.1294\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1302 - val_loss: 0.1281\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1292 - val_loss: 0.1268\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1283 - val_loss: 0.1265\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1274 - val_loss: 0.1247\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1266 - val_loss: 0.1246\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1259 - val_loss: 0.1243\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1251 - val_loss: 0.1239\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1244 - val_loss: 0.1225\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1237 - val_loss: 0.1204\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1229 - val_loss: 0.1204\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1223 - val_loss: 0.1206\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1215 - val_loss: 0.1193\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1208 - val_loss: 0.1178\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1202 - val_loss: 0.1203\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1196 - val_loss: 0.1177\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1189 - val_loss: 0.1185\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1182 - val_loss: 0.1172\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1178 - val_loss: 0.1151\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1172 - val_loss: 0.1161\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1167 - val_loss: 0.1140\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1162 - val_loss: 0.1150\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1156 - val_loss: 0.1156\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1151 - val_loss: 0.1148\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1145 - val_loss: 0.1121\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1139 - val_loss: 0.1122\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1135 - val_loss: 0.1111\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1131 - val_loss: 0.1120\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1126 - val_loss: 0.1119\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1121 - val_loss: 0.1111\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1118 - val_loss: 0.1108\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1113 - val_loss: 0.1101\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1109 - val_loss: 0.1090\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1106 - val_loss: 0.1086\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1101 - val_loss: 0.1069\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1097 - val_loss: 0.1081\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1093 - val_loss: 0.1091\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1089 - val_loss: 0.1063\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1086 - val_loss: 0.1079\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1083 - val_loss: 0.1059\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1079 - val_loss: 0.1080\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1077 - val_loss: 0.1060\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1072 - val_loss: 0.1072\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1069 - val_loss: 0.1072\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1066 - val_loss: 0.1046\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1063 - val_loss: 0.1040\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1060 - val_loss: 0.1041\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1057 - val_loss: 0.1040\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1054 - val_loss: 0.1056\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1052 - val_loss: 0.1042\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1049 - val_loss: 0.1050\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1047 - val_loss: 0.1046\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1041 - val_loss: 0.1038\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1037 - val_loss: 0.1016\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1034 - val_loss: 0.1017\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1033 - val_loss: 0.1024\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1031 - val_loss: 0.1007\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1028 - val_loss: 0.1014\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1027 - val_loss: 0.1012\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1024 - val_loss: 0.1019\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1024 - val_loss: 0.1004\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1021 - val_loss: 0.1022\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1019 - val_loss: 0.1020\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1017 - val_loss: 0.1013\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1015 - val_loss: 0.1008\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1013 - val_loss: 0.1017\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1011 - val_loss: 0.0999\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1007 - val_loss: 0.0997\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1006 - val_loss: 0.1003\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1004 - val_loss: 0.0989\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1002 - val_loss: 0.0990\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1000 - val_loss: 0.0992\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0999 - val_loss: 0.1004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1e71a2470>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "input_img = Input((784,))\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train,\n",
    "               epochs=100,\n",
    "               batch_size = 256,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_test, x_test))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "mnist_autoencoder",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
