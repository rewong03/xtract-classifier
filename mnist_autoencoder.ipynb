{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "avV9z5bOcWFS",
    "outputId": "56f19b5a-b1d0-453e-855d-9810fafb343d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "hK5cWVb0ciNi",
    "outputId": "1a6336e4-36ac-427c-f11f-57a9d2a98477"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0929 20:19:32.108077 4648940992 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0929 20:19:32.119141 4648940992 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0929 20:19:32.121424 4648940992 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of features in reduced dimension\n",
    "encoding_dim = 32\n",
    "\n",
    "# input layer\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# encoded layer\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# decoded layer\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# maps input image to the encoded image\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# maps input image to its reconstructed version\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "3RiRLap2e9_P",
    "outputId": "d5f2ad2e-fa16-404a-ce19-65a1ae139cfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input layer for encoded image\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# decoder layer\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# decoder model\n",
    "decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NODByBGqf12H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
      " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215687\n",
      " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
      " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
      " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313726\n",
      " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13725491 0.94509804\n",
      " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
      " 0.5882353  0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
      " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.5803922\n",
      " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058825\n",
      " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
      " 0.3137255  0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333336 0.99215686\n",
      " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# mnist processing\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import keras\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "x_train = x_train.reshape(len(x_train), 784)\n",
    "x_test = x_test.reshape(len(x_test), 784)\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "D0UPjS9shJfk",
    "outputId": "ed5d5da2-0773-4e21-9428-6d4e2db742ab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.3505 - val_loss: 0.2709\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2633 - val_loss: 0.2518\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2421 - val_loss: 0.2304\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.2227 - val_loss: 0.2124\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2069 - val_loss: 0.1990\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1954 - val_loss: 0.1894\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1867 - val_loss: 0.1815\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1796 - val_loss: 0.1749\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1735 - val_loss: 0.1694\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1681 - val_loss: 0.1642\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1633 - val_loss: 0.1597\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1588 - val_loss: 0.1553\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1548 - val_loss: 0.1514\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1511 - val_loss: 0.1480\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1478 - val_loss: 0.1448\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1448 - val_loss: 0.1419\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1419 - val_loss: 0.1391\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1393 - val_loss: 0.1367\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1369 - val_loss: 0.1342\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1346 - val_loss: 0.1322\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1324 - val_loss: 0.1298\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1304 - val_loss: 0.1278\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1284 - val_loss: 0.1259\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1265 - val_loss: 0.1241\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1248 - val_loss: 0.1223\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1231 - val_loss: 0.1207\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1215 - val_loss: 0.1191\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1199 - val_loss: 0.1176\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1185 - val_loss: 0.1163\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1172 - val_loss: 0.1149\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1159 - val_loss: 0.1137\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1147 - val_loss: 0.1125\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1136 - val_loss: 0.1115\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1126 - val_loss: 0.1105\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1117 - val_loss: 0.1096\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1108 - val_loss: 0.1087\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1100 - val_loss: 0.1079\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1092 - val_loss: 0.1072\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1085 - val_loss: 0.1065\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1079 - val_loss: 0.1059\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1073 - val_loss: 0.1053\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1067 - val_loss: 0.1048\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1061 - val_loss: 0.1043\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1056 - val_loss: 0.1038\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1052 - val_loss: 0.1033\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1047 - val_loss: 0.1029\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1043 - val_loss: 0.1025\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1039 - val_loss: 0.1021\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1035 - val_loss: 0.1017\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1032 - val_loss: 0.1014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c49401eb8>"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "               epochs=50,\n",
    "               batch_size=256,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cAo2v9nKiQJ3",
    "outputId": "12ad2b6b-500d-428e-fa75-4840875febc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09862608340929965\n"
     ]
    }
   ],
   "source": [
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "k5S_xdMtkH8_",
    "outputId": "743bc8cf-dd24-435e-c2e8-a5a8c85f8058"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoded_imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-634e7c7cc1c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# display reconstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoded_imgs' is not defined"
     ]
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "BwMk8bvmlAkv",
    "outputId": "fb79b303-3a18-44c7-b166-06fbc91dc5b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "# add a Dense layer with a L1 activity regularizer\n",
    "encoded = Dense(encoding_dim, activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-ohf_plxlCYX",
    "outputId": "147a1e53-05c2-4f94-c94f-f38b83f3a606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.6729 - val_loss: 0.6485\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.6284 - val_loss: 0.6090\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.5916 - val_loss: 0.5749\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.5598 - val_loss: 0.5454\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.5323 - val_loss: 0.5198\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.5084 - val_loss: 0.4975\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.4875 - val_loss: 0.4780\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.4692 - val_loss: 0.4609\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.4531 - val_loss: 0.4457\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.4389 - val_loss: 0.4324\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.4262 - val_loss: 0.4205\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.4150 - val_loss: 0.4098\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.4049 - val_loss: 0.4003\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.3959 - val_loss: 0.3918\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.3877 - val_loss: 0.3840\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.3804 - val_loss: 0.3771\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.3737 - val_loss: 0.3707\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.3676 - val_loss: 0.3649\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.3621 - val_loss: 0.3596\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.3570 - val_loss: 0.3548\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.3524 - val_loss: 0.3503\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.3481 - val_loss: 0.3463\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.3442 - val_loss: 0.3425\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.3406 - val_loss: 0.3390\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.3372 - val_loss: 0.3357\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.3341 - val_loss: 0.3327\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.3312 - val_loss: 0.3299\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.3285 - val_loss: 0.3273\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.3259 - val_loss: 0.3249\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.3236 - val_loss: 0.3226\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.3214 - val_loss: 0.3204\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.3193 - val_loss: 0.3184\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.3173 - val_loss: 0.3165\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.3155 - val_loss: 0.3147\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.3138 - val_loss: 0.3131\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.3121 - val_loss: 0.3115\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.3106 - val_loss: 0.3100\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.3091 - val_loss: 0.3085\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.3077 - val_loss: 0.3072\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.3064 - val_loss: 0.3059\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.3052 - val_loss: 0.3047\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.3040 - val_loss: 0.3035\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.3029 - val_loss: 0.3024\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.3018 - val_loss: 0.3014\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.3008 - val_loss: 0.3004\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2998 - val_loss: 0.2994\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2989 - val_loss: 0.2985\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2980 - val_loss: 0.2976\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2971 - val_loss: 0.2968\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.2963 - val_loss: 0.2960\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.2955 - val_loss: 0.2952\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.2948 - val_loss: 0.2945\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2941 - val_loss: 0.2938\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2934 - val_loss: 0.2931\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2927 - val_loss: 0.2925\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2921 - val_loss: 0.2918\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2915 - val_loss: 0.2912\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.2909 - val_loss: 0.2906\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2903 - val_loss: 0.2901\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2898 - val_loss: 0.2895\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2892 - val_loss: 0.2890\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2887 - val_loss: 0.2885\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2882 - val_loss: 0.2880\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.2877 - val_loss: 0.2875\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2873 - val_loss: 0.2871\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2868 - val_loss: 0.2867\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.2864 - val_loss: 0.2862\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2860 - val_loss: 0.2858\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2856 - val_loss: 0.2854\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.2852 - val_loss: 0.2850\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2848 - val_loss: 0.2846\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2844 - val_loss: 0.2843\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2841 - val_loss: 0.2839\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2837 - val_loss: 0.2836\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2834 - val_loss: 0.2832\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2831 - val_loss: 0.2829\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2828 - val_loss: 0.2826\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2825 - val_loss: 0.2823\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.2822 - val_loss: 0.2820\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2819 - val_loss: 0.2817\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.2816 - val_loss: 0.2814\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2813 - val_loss: 0.2812\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2810 - val_loss: 0.2809\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2808 - val_loss: 0.2806\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2805 - val_loss: 0.2804\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2803 - val_loss: 0.2801\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2800 - val_loss: 0.2799\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2798 - val_loss: 0.2796\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2796 - val_loss: 0.2794\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2793 - val_loss: 0.2792\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2791 - val_loss: 0.2790\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2789 - val_loss: 0.2788\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.2787 - val_loss: 0.2785\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2785 - val_loss: 0.2783\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2783 - val_loss: 0.2781\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.2781 - val_loss: 0.2780\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.2779 - val_loss: 0.2778\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2777 - val_loss: 0.2776\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2775 - val_loss: 0.2774\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2774 - val_loss: 0.2772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5dbbbb8e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "               epochs=100,\n",
    "               batch_size=256,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OBZ_wu1-_DBp",
    "outputId": "b713886a-44dc-4ca6-eab9-a159f4876931"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0929 20:19:51.508165 4648940992 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0929 20:19:51.517754 4648940992 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0929 20:19:51.521856 4648940992 deprecation.py:323] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0929 20:19:51.697019 4648940992 deprecation_wrapper.py:119] From /Users/ryan/.conda/envs/official_xtract/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3507 - acc: 0.7439 - val_loss: 0.2650 - val_acc: 0.7866\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2597 - acc: 0.7948 - val_loss: 0.2573 - val_acc: 0.7946\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.2524 - acc: 0.7983 - val_loss: 0.2483 - val_acc: 0.7928\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2423 - acc: 0.7952 - val_loss: 0.2368 - val_acc: 0.7920\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.2310 - acc: 0.7945 - val_loss: 0.2243 - val_acc: 0.7903\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.2174 - acc: 0.7949 - val_loss: 0.2093 - val_acc: 0.7968\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2031 - acc: 0.7954 - val_loss: 0.1954 - val_acc: 0.7936\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1905 - acc: 0.7970 - val_loss: 0.1839 - val_acc: 0.7997\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1820 - acc: 0.7989 - val_loss: 0.1778 - val_acc: 0.7965\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1766 - acc: 0.7999 - val_loss: 0.1719 - val_acc: 0.7987\n"
     ]
    }
   ],
   "source": [
    "# uses a deeper network to encode images\n",
    "\n",
    "input_img = Input((784,))\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "autoencoder.fit(x_train, x_train,\n",
    "               epochs=10,\n",
    "               batch_size = 256,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_test, x_test))\n",
    "\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dd6YgQ4Slfdv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "[[-0.00886185  0.05947391  0.0362878  ...  0.0643462  -0.0302072\n",
      "   0.06987628]\n",
      " [-0.05570446  0.05679414 -0.00288785 ... -0.02871788  0.01562922\n",
      "  -0.06326306]\n",
      " [-0.04213982  0.00370272 -0.01871976 ...  0.01576682  0.01789319\n",
      "  -0.06476408]\n",
      " ...\n",
      " [-0.06354181 -0.06226511  0.01426818 ...  0.04273959  0.07934282\n",
      "   0.07521937]\n",
      " [-0.02018193 -0.00297017  0.04712795 ... -0.01389293 -0.04841049\n",
      "   0.00758218]\n",
      " [-0.07920329 -0.00101845  0.0739345  ... -0.07975397 -0.03917845\n",
      "  -0.01054192]]\n",
      "LAYER 1\n",
      "[ 1.63487345e-01  6.87050596e-02  3.19820940e-01 -1.67464048e-01\n",
      "  1.07701115e-01  2.27858588e-01  1.94374949e-01  9.55232605e-02\n",
      "  1.64251581e-01 -2.09099539e-02  1.85118601e-01  1.85812250e-01\n",
      " -1.25695691e-02  5.39480269e-01  3.06804515e-02  4.04111862e-01\n",
      "  9.15872082e-02  6.76475316e-02 -2.83332579e-02  8.15361142e-02\n",
      "  2.71564811e-01 -1.06186643e-02  2.79324315e-02  9.17478949e-02\n",
      " -3.39823291e-02  9.61285308e-02  7.00145811e-02  1.04808070e-01\n",
      "  7.92038590e-02 -1.29371345e-01  1.24740548e-01  8.82682577e-02\n",
      "  1.10820204e-01  9.72718820e-02 -6.44359589e-02  4.08816896e-02\n",
      "  8.22374672e-02  1.68663010e-01  2.94384714e-02  1.79805383e-01\n",
      " -7.53307389e-03  3.33910316e-01  1.18648015e-01 -4.56545409e-03\n",
      "  1.80711392e-02  8.11850727e-02 -1.59116816e-02  1.54786929e-01\n",
      " -1.39352918e-01  3.19325894e-01  6.74104244e-02 -3.45981345e-02\n",
      "  1.65190116e-01  1.90585390e-01  8.49273354e-02 -1.30468523e-02\n",
      " -4.54775058e-03  2.55498677e-01  2.58371383e-01 -2.34411620e-02\n",
      "  2.33468726e-01  4.99667794e-01  2.62800138e-02  2.43797544e-02\n",
      "  1.64296195e-01 -1.29373232e-02 -3.87190958e-03  4.49334472e-01\n",
      "  1.43226162e-01  3.97736579e-02  5.35769900e-03  5.53404260e-03\n",
      "  1.85059085e-01  9.48142782e-02 -2.31223982e-02  1.31122813e-01\n",
      " -2.63072420e-02  1.52180761e-01 -1.03422612e-01  1.27458677e-01\n",
      " -2.00666953e-02 -1.87922575e-04  5.29592574e-01  2.62906641e-01\n",
      " -1.20919175e-03 -4.18746565e-03  3.18347692e-01  1.02145903e-01\n",
      "  4.30016555e-02  2.01053366e-01  1.81976914e-01  1.49061501e-01\n",
      "  2.35637099e-01  1.41791031e-01  9.16795954e-02  6.09669030e-01\n",
      "  4.86891456e-02  1.57924682e-01  1.66628007e-02 -1.17431050e-02\n",
      " -3.35998721e-02  6.28658682e-02  3.23321857e-02  2.87532687e-01\n",
      "  2.36997958e-02  1.82821691e-01 -2.67463066e-02  7.23118708e-02\n",
      "  1.55868784e-01  9.81067643e-02  7.62658715e-02  1.10261947e-01\n",
      "  1.84187576e-01 -2.97376998e-02  7.24194869e-02  9.08406898e-02\n",
      " -6.38674572e-03  1.96388721e-01  2.58519173e-01  1.73360258e-01\n",
      "  1.74229906e-03  5.07405698e-02  1.11176811e-01  9.95218083e-02\n",
      "  8.50592852e-02  4.01825815e-01  4.01097275e-02 -5.43398969e-03]\n",
      "LAYER 2\n",
      "[[ 0.10100605 -0.11051609 -0.14179677 ...  0.00516719 -0.06174311\n",
      "  -0.13159196]\n",
      " [ 0.00215988  0.11854798  0.09014743 ...  0.15622967 -0.05041417\n",
      "  -0.12188738]\n",
      " [ 0.00266519 -0.00292754  0.10543036 ...  0.02547515 -0.00512289\n",
      "  -0.00595225]\n",
      " ...\n",
      " [-0.06680334  0.13529333  0.0938296  ...  0.13318077 -0.07408251\n",
      "  -0.02911176]\n",
      " [-0.04128407 -0.04805582 -0.08998898 ...  0.21811786 -0.07049699\n",
      "  -0.14034416]\n",
      " [ 0.12730058 -0.16166183 -0.09752665 ... -0.12546559 -0.16050944\n",
      "   0.06048939]]\n",
      "LAYER 3\n",
      "[ 4.4463566e-03 -2.0821344e-02 -2.8921843e-02 -5.0595533e-03\n",
      "  9.0964809e-02 -8.0760844e-02  1.9950680e-02  3.1849194e-01\n",
      " -7.3418603e-03 -3.3812118e-03  2.0610164e-01 -1.2959791e-03\n",
      "  3.0859875e-02 -1.2722597e-02  1.3743196e-01  3.3773807e-01\n",
      "  3.6392030e-01  5.8346982e-03  4.3658161e-01 -1.8316202e-03\n",
      "  1.2883126e-05  7.7715769e-02  3.2458273e-01 -7.6552883e-02\n",
      "  4.9783796e-02  4.2179653e-01  3.9147979e-01  7.0902288e-02\n",
      "  1.8169504e-02  9.6486621e-02  2.4686851e-01  3.4306559e-04\n",
      "  3.5045031e-01  4.9637429e-02  2.7713743e-01  1.5446505e-01\n",
      " -5.3653039e-02  2.8413451e-01  2.6158732e-01 -2.2903277e-02\n",
      " -1.5812075e-01 -4.4463955e-02 -4.2500965e-02  1.2873068e-01\n",
      " -2.1993045e-03  2.2722834e-01  9.6734235e-05  7.1374378e-03\n",
      "  1.0811923e-01 -2.5812208e-04  2.6128164e-01  6.6018231e-02\n",
      "  4.4163555e-02 -5.7500643e-03  3.7252381e-01 -1.1269633e-02\n",
      "  2.7154356e-01 -5.9261904e-03 -5.4063131e-03  1.9046900e-01\n",
      " -8.5982364e-03  1.3922504e-01  2.9876985e-02 -6.4870017e-03]\n",
      "LAYER 4\n",
      "[[-0.05193209 -0.13344866 -0.09862942 ...  0.13183658 -0.23332739\n",
      "   0.20950669]\n",
      " [-0.02581599  0.01312664 -0.18652183 ...  0.24615484  0.12935893\n",
      "  -0.16372599]\n",
      " [-0.22705862  0.07959621  0.20920695 ... -0.24833764 -0.05234802\n",
      "  -0.14663878]\n",
      " ...\n",
      " [ 0.35851604 -0.23761083  0.16052459 ...  0.02819853  0.17469963\n",
      "   0.04751166]\n",
      " [ 0.1177587  -0.09798122  0.20207436 ... -0.10335363  0.2279476\n",
      "   0.16350476]\n",
      " [-0.14192496 -0.12122857  0.10369152 ... -0.22135405  0.01636153\n",
      "  -0.14913097]]\n",
      "LAYER 5\n",
      "[ 1.9668505e-01 -1.5259668e-04  8.5005030e-02 -3.5424311e-02\n",
      "  4.0473893e-01  8.4016852e-02  1.1879885e-02  2.8268108e-01\n",
      " -1.7472437e-02  2.6320500e-02 -6.8070199e-03 -1.9377755e-03\n",
      "  1.4523733e-02  5.7016063e-04  5.9764890e-05  7.7707313e-02\n",
      "  1.9782962e-01  1.2170896e-01 -3.8269050e-03  1.4185561e-01\n",
      "  3.0360314e-01  6.7423604e-02 -4.0584024e-02  1.8336651e-01\n",
      "  2.0957521e-01  1.1303178e-01 -4.8897217e-04  3.1263286e-03\n",
      "  2.0898113e-02  4.4276114e-03 -2.3804152e-02 -5.9912208e-02]\n",
      "LAYER 6\n",
      "[[-0.17614238 -0.18688546  0.12098621 ... -0.18619642  0.07864058\n",
      "   0.02213368]\n",
      " [ 0.09180359 -0.12207097 -0.00204525 ... -0.14636506  0.2287304\n",
      "   0.15396234]\n",
      " [ 0.00159927  0.30089656 -0.19522887 ... -0.15043068 -0.14296405\n",
      "  -0.09898376]\n",
      " ...\n",
      " [-0.25863856  0.21034485  0.22224584 ...  0.11536682  0.0570967\n",
      "   0.20754129]\n",
      " [-0.22216389  0.01747456  0.0739391  ...  0.0092615   0.06022181\n",
      "  -0.13465714]\n",
      " [-0.20937906 -0.09152567  0.3456035  ...  0.19408944  0.10684236\n",
      "   0.15575892]]\n",
      "LAYER 7\n",
      "[ 1.39967114e-01  4.42534387e-02  2.53743045e-02  2.54367082e-03\n",
      "  1.22395754e-01  3.63499746e-02 -6.62274361e-02  1.38950851e-02\n",
      " -2.41299230e-03  1.03544933e-03  6.63810642e-04  4.35087904e-02\n",
      " -7.24214478e-04  3.93221118e-02 -1.61468063e-03 -7.41376192e-04\n",
      "  5.64957261e-02  8.20154622e-02 -6.34513097e-03 -2.96720639e-02\n",
      "  1.91025406e-01  7.52684008e-03  7.96432048e-02  4.74625714e-02\n",
      "  4.66168467e-06  8.47913325e-02  1.51750194e-02  1.93085343e-01\n",
      "  2.52991013e-04  1.10241454e-05  1.74769148e-01  1.16258293e-01\n",
      " -2.46495096e-04 -5.59746462e-04 -8.03588983e-03 -3.86199169e-02\n",
      "  2.70746257e-02  2.99992580e-02  4.48658988e-02 -5.39548462e-03\n",
      "  7.03444406e-02  1.09456114e-01  8.71482417e-02  9.27000344e-02\n",
      "  3.87277938e-02  5.63406311e-02  6.67775944e-02 -9.81753226e-04\n",
      "  9.14298669e-02 -9.88053624e-03  7.14209005e-02  6.95632100e-02\n",
      "  2.37974711e-02 -5.83603696e-06  3.41762898e-05  2.72645548e-06\n",
      "  4.13323753e-03  7.36251846e-02  5.78289740e-02 -8.93289573e-04\n",
      " -4.60330993e-02 -6.87523861e-04  3.24786152e-03  4.71672509e-03]\n",
      "LAYER 8\n",
      "[[ 0.1077948   0.01007557 -0.13671868 ... -0.0158531   0.19003718\n",
      "   0.23954226]\n",
      " [-0.11252923  0.13551508  0.16207619 ...  0.16707037 -0.09871535\n",
      "   0.11163675]\n",
      " [-0.00636725 -0.01235358  0.03616476 ...  0.1065385  -0.01738628\n",
      "  -0.13552049]\n",
      " ...\n",
      " [-0.14116284 -0.09797584 -0.02351576 ...  0.00045554  0.05113921\n",
      "   0.10482328]\n",
      " [ 0.02236529 -0.03549653 -0.09859923 ... -0.17870216  0.03821412\n",
      "  -0.08880283]\n",
      " [ 0.10875274 -0.05191839  0.06088283 ...  0.18562654  0.14903493\n",
      "  -0.02049926]]\n",
      "LAYER 9\n",
      "[ 6.51359335e-02 -1.51358661e-03 -1.24089676e-03 -1.45961437e-03\n",
      "  6.30202005e-03 -5.06267436e-02 -3.89309484e-04 -3.01465834e-03\n",
      " -3.61353322e-03 -1.74003828e-03  5.98149784e-02 -4.38615330e-04\n",
      "  1.62056014e-02  1.38839096e-01  1.09801488e-02  4.08086373e-04\n",
      "  5.30465432e-02  3.65808718e-02  2.61409916e-02  2.64522765e-04\n",
      "  1.91450783e-03 -1.61508918e-02  6.64585503e-04  1.36797670e-02\n",
      " -3.73949436e-03 -4.12853900e-03  4.34621097e-06 -3.61633231e-03\n",
      "  2.87649897e-03  5.21771312e-02  1.66382699e-04  2.40266253e-03\n",
      " -6.90614209e-02  3.93467862e-03  3.98750417e-02  1.19866374e-04\n",
      " -1.19730175e-05  8.49645138e-02  3.99726294e-02 -8.74748081e-03\n",
      " -8.92351847e-04  2.82025188e-02  3.43055874e-02  4.38360684e-03\n",
      "  1.00722641e-01 -4.80372971e-03 -2.76648207e-03 -3.79226985e-04\n",
      " -4.14176699e-04 -6.36885641e-04  4.93529290e-02 -9.63580236e-03\n",
      "  2.65055243e-03 -4.69626673e-03  1.71391696e-01 -5.36989886e-03\n",
      " -2.22830367e-05  1.26909062e-01  3.40938829e-02  9.62581893e-04\n",
      " -1.38360541e-03  2.12961659e-02  8.30762908e-02  3.08889803e-02\n",
      "  5.50434552e-02 -1.71633932e-04  4.53712083e-02 -5.63942129e-04\n",
      "  1.19232526e-02 -7.40972534e-03  1.79440132e-03  1.56128220e-02\n",
      "  4.35464755e-02 -1.54873906e-05  1.13153271e-02 -1.58289019e-02\n",
      " -6.32977718e-03 -6.40731230e-02 -1.17670521e-02 -4.43626707e-03\n",
      " -1.17853167e-03 -2.41280795e-04  1.53506262e-04  2.66151447e-02\n",
      "  3.33446078e-04  1.23801408e-02  1.10368372e-03  5.76111081e-04\n",
      "  1.29467566e-02 -1.76358066e-04 -2.33019404e-02  5.59047097e-03\n",
      " -2.56414860e-02 -6.76419772e-03  1.32728778e-02  9.75150093e-02\n",
      "  6.32065523e-04  1.09979764e-01  9.74336490e-02 -9.05158697e-04\n",
      " -5.19032124e-03  1.04508556e-01  8.86810594e-05  3.95291783e-02\n",
      "  4.61823912e-03 -2.54468806e-02 -3.38828249e-04  1.84455304e-04\n",
      "  5.75542683e-03  9.90025699e-03  3.60495076e-02  4.66669537e-02\n",
      " -5.61362207e-02  1.11044951e-01 -6.26630057e-03  2.95121083e-03\n",
      " -1.80939939e-02  3.32472217e-03 -2.29594341e-04 -1.81511696e-02\n",
      " -6.40377104e-02  4.65521356e-04  7.16927201e-02  9.10736620e-03\n",
      "  7.54946917e-02  2.55949832e-02  3.30833755e-02  9.19031650e-02]\n",
      "LAYER 10\n",
      "[[-0.05519436 -0.0234701   0.03377417 ... -0.01996812 -0.10339618\n",
      "  -0.0784824 ]\n",
      " [ 0.05355314  0.03508412  0.04310593 ... -0.01879042  0.00302851\n",
      "   0.04085165]\n",
      " [ 0.04790672 -0.00128384  0.02209823 ...  0.07427032  0.03524648\n",
      "  -0.01479845]\n",
      " ...\n",
      " [-0.0656804   0.02065646 -0.04504934 ... -0.05953941  0.03595776\n",
      "  -0.06873666]\n",
      " [-0.06797108 -0.06160394  0.05247896 ... -0.01496235 -0.06528195\n",
      "  -0.02330351]\n",
      " [-0.04894172 -0.02866508 -0.07953919 ...  0.02962877 -0.10270141\n",
      "   0.01323738]]\n",
      "LAYER 11\n",
      "[-2.80066114e-02 -3.15109864e-02 -3.46752182e-02 -2.95638889e-02\n",
      " -2.75243279e-02 -3.04252189e-02 -2.61052866e-02 -3.00229434e-02\n",
      " -3.38228568e-02 -2.63685789e-02 -2.84077749e-02 -3.64943855e-02\n",
      " -3.17569003e-02 -3.16150039e-02 -3.52705754e-02 -3.44636850e-02\n",
      " -3.25649790e-02 -3.26687954e-02 -2.92775054e-02 -3.24264355e-02\n",
      " -3.34703103e-02 -3.21173072e-02 -3.49224992e-02 -2.98748724e-02\n",
      " -2.86609661e-02 -3.23659144e-02 -3.31514627e-02 -3.10575105e-02\n",
      " -3.29288915e-02 -2.87769120e-02 -3.59398872e-02 -3.56102251e-02\n",
      " -3.04787029e-02 -3.42267118e-02 -2.87024640e-02 -3.09283044e-02\n",
      " -3.07588447e-02 -2.93135755e-02 -3.20092440e-02 -2.89364979e-02\n",
      " -2.74177846e-02 -3.17336544e-02 -3.04242652e-02 -3.18105258e-02\n",
      " -3.65816467e-02 -2.92226095e-02 -3.18515524e-02 -3.26713696e-02\n",
      " -2.79278755e-02 -2.79385131e-02 -3.21268402e-02 -3.35942395e-02\n",
      " -2.89981030e-02 -3.01747452e-02 -2.69649457e-02 -3.30548957e-02\n",
      " -3.24583612e-02 -2.66380850e-02 -2.97834836e-02 -3.82997803e-02\n",
      " -2.69675497e-02 -3.13290283e-02 -3.74268293e-02 -3.09195537e-02\n",
      " -2.84543540e-02 -3.14567052e-02 -2.94620562e-02 -2.87776068e-02\n",
      " -2.45300923e-02 -3.13948356e-02 -3.19412276e-02 -3.91070731e-02\n",
      " -4.22189943e-02 -3.88655588e-02 -3.85505930e-02 -3.43229324e-02\n",
      " -2.91043743e-02 -3.43586840e-02 -3.04883607e-02 -3.18577550e-02\n",
      " -3.35255228e-02 -2.91023739e-02 -2.93747429e-02 -2.76836250e-02\n",
      " -2.94507872e-02 -2.98763867e-02 -3.59179638e-02 -3.09995357e-02\n",
      " -3.21176276e-02 -2.97638252e-02 -2.75198966e-02 -2.82204058e-02\n",
      " -3.38988602e-02 -3.08031328e-02 -3.51427682e-02 -3.01769134e-02\n",
      " -3.26529443e-02 -3.34766880e-02 -4.73568626e-02 -4.65070419e-02\n",
      " -5.18715754e-02 -5.75238690e-02 -5.04172333e-02 -4.42724228e-02\n",
      " -4.34731580e-02 -3.81519087e-02 -3.73636633e-02 -3.22961695e-02\n",
      " -3.15020755e-02 -3.20063196e-02 -3.83980237e-02 -3.26188616e-02\n",
      " -2.97850743e-02 -3.33553143e-02 -3.18060629e-02 -3.12222596e-02\n",
      " -3.15059721e-02 -3.45704257e-02 -3.14302631e-02 -3.31875719e-02\n",
      " -3.18146236e-02 -2.91511081e-02 -3.62082273e-02 -3.38715687e-02\n",
      " -4.24770787e-02 -5.03852256e-02 -6.76239952e-02 -6.36780709e-02\n",
      " -7.78847858e-02 -7.93132335e-02 -7.07857311e-02 -5.93027025e-02\n",
      " -4.94530052e-02 -4.63795289e-02 -3.99170481e-02 -3.14320587e-02\n",
      " -3.28979231e-02 -3.48480605e-02 -3.03127877e-02 -3.20255682e-02\n",
      " -2.75782365e-02 -3.03906798e-02 -3.20092440e-02 -2.80350056e-02\n",
      " -3.43871489e-02 -3.24652866e-02 -2.97828969e-02 -2.67645959e-02\n",
      " -2.07729936e-02 -2.01240443e-02 -1.44876968e-02 -1.06365718e-02\n",
      " -2.01273263e-02 -3.58063653e-02 -4.55419384e-02 -5.31025007e-02\n",
      " -5.77465110e-02 -5.68087585e-02 -4.88479212e-02 -5.58562949e-02\n",
      " -4.85963523e-02 -4.28118929e-02 -4.74106483e-02 -4.35215123e-02\n",
      " -3.79939750e-02 -3.24612558e-02 -3.17462161e-02 -3.49783301e-02\n",
      " -2.79377382e-02 -3.53402793e-02 -2.99789123e-02 -3.26362662e-02\n",
      " -3.39995436e-02 -2.45320369e-02 -2.22250838e-02 -2.44306624e-02\n",
      " -3.43108503e-03  2.15985405e-04  7.34491693e-03  7.29558151e-03\n",
      " -7.77372252e-03 -3.43616195e-02 -4.39706407e-02 -5.42760342e-02\n",
      " -4.22887020e-02 -4.82311361e-02 -3.85960154e-02 -3.47841382e-02\n",
      " -2.95983478e-02 -3.34858336e-02 -3.31691392e-02 -4.22898009e-02\n",
      " -4.04636674e-02 -3.52894776e-02 -2.68304106e-02 -2.88568903e-02\n",
      " -3.56703252e-02 -3.06796283e-02 -2.37660110e-02 -2.72512659e-02\n",
      " -3.01962290e-02 -2.53827199e-02 -2.11487878e-02 -5.77847706e-03\n",
      "  1.62565103e-03  7.65918894e-03  7.23052537e-03 -9.34073422e-03\n",
      " -2.99236160e-02 -4.66352627e-02 -4.55094539e-02 -5.96488640e-02\n",
      " -4.18732651e-02 -3.01053580e-02 -3.79283987e-02 -2.13792603e-02\n",
      " -8.83390568e-03 -1.10980375e-02 -2.62009520e-02 -3.72547954e-02\n",
      " -4.01629210e-02 -3.07152048e-02 -3.55308242e-02 -2.91786175e-02\n",
      " -3.28575782e-02 -2.80038342e-02 -3.03661488e-02 -2.83445958e-02\n",
      " -2.96634454e-02 -2.64130048e-02 -1.48347691e-02 -7.37695396e-03\n",
      "  1.10808769e-02  1.20899139e-03 -5.31173823e-03 -4.02666591e-02\n",
      " -5.79603314e-02 -6.18237965e-02 -5.44692501e-02 -5.65498061e-02\n",
      " -4.95425314e-02 -3.22741941e-02 -2.78406385e-02 -1.03863943e-02\n",
      "  7.38513051e-03 -7.15671945e-03 -1.47126010e-02 -2.81401481e-02\n",
      " -3.80996168e-02 -3.55068669e-02 -2.93101743e-02 -2.95776147e-02\n",
      " -3.16153802e-02 -2.97008492e-02 -3.10646761e-02 -3.39719057e-02\n",
      " -2.79167406e-02 -3.06719132e-02 -1.46798640e-02  5.56596695e-03\n",
      "  7.90603552e-03 -1.34181501e-02 -3.55460048e-02 -7.58095980e-02\n",
      " -6.08549975e-02 -6.28130808e-02 -6.07785359e-02 -6.26790375e-02\n",
      " -4.47587855e-02 -4.46090102e-02 -2.76007764e-02 -1.76259298e-02\n",
      "  2.74605671e-04  6.45161141e-03 -5.59474947e-03 -1.83761809e-02\n",
      " -3.88824493e-02 -3.73593867e-02 -2.92011797e-02 -2.99041513e-02\n",
      " -3.06776334e-02 -3.34314555e-02 -3.22887674e-02 -3.05815153e-02\n",
      " -3.02216187e-02 -2.00372115e-02 -8.24961718e-03  1.28593529e-03\n",
      "  2.86432821e-03 -2.26544514e-02 -6.05425350e-02 -7.78438300e-02\n",
      " -7.39379302e-02 -7.42886588e-02 -8.07464197e-02 -7.19246268e-02\n",
      " -4.95185144e-02 -3.98156531e-02 -2.82896236e-02 -5.00528095e-03\n",
      "  6.03172649e-03  1.00585064e-02 -2.84512830e-03 -2.46332530e-02\n",
      " -3.02783921e-02 -3.49830128e-02 -2.93829329e-02 -3.24057937e-02\n",
      " -3.41303125e-02 -2.67278384e-02 -2.72805654e-02 -3.48910727e-02\n",
      " -3.01724020e-02 -1.96924135e-02 -6.89369068e-03 -2.17282912e-03\n",
      " -7.45357666e-03 -4.46401238e-02 -6.75661042e-02 -6.84824437e-02\n",
      " -5.57861179e-02 -5.70374466e-02 -7.44893700e-02 -7.86845759e-02\n",
      " -4.95128110e-02 -4.82569262e-02 -2.68828180e-02 -1.21553689e-02\n",
      " -2.59822118e-03  2.86718458e-03 -7.04446761e-03 -9.86657105e-03\n",
      " -3.01737003e-02 -3.43736634e-02 -3.22117917e-02 -3.28393430e-02\n",
      " -3.01718730e-02 -2.80940253e-02 -2.78749242e-02 -2.94593554e-02\n",
      " -2.95725763e-02 -1.75625440e-02 -6.06024126e-03 -1.45019614e-03\n",
      " -2.58725863e-02 -5.33714108e-02 -6.94270357e-02 -5.46744689e-02\n",
      " -4.32494767e-02 -6.58643991e-02 -8.69178399e-02 -6.95002452e-02\n",
      " -3.40680927e-02 -2.38651019e-02 -2.12421808e-02  4.51184157e-03\n",
      "  9.15590860e-03  2.00464390e-03  3.54287121e-03 -5.82255796e-03\n",
      " -2.51278598e-02 -3.12754251e-02 -3.46089303e-02 -3.60241570e-02\n",
      " -3.41832973e-02 -3.22001763e-02 -3.06644756e-02 -2.51490399e-02\n",
      " -2.94036288e-02 -1.35599123e-02  5.45852259e-03 -4.36123926e-03\n",
      " -2.50882730e-02 -4.37048897e-02 -5.27972467e-02 -4.01419550e-02\n",
      " -4.09399942e-02 -6.97499514e-02 -8.27123076e-02 -5.03104590e-02\n",
      " -4.02608588e-02 -4.25862521e-03  7.43264752e-03  1.15139910e-03\n",
      "  2.83547724e-03  6.45043701e-03 -3.39490434e-05 -1.00643979e-02\n",
      " -1.97217762e-02 -2.58011371e-02 -2.99322065e-02 -2.87195463e-02\n",
      " -3.50507088e-02 -3.21596973e-02 -3.34100686e-02 -2.97935493e-02\n",
      " -2.38474570e-02 -1.41541418e-02 -1.43664004e-03  7.07356166e-03\n",
      " -2.15880480e-02 -4.92290258e-02 -5.11283241e-02 -4.24933881e-02\n",
      " -4.64116223e-02 -6.65761381e-02 -7.44733810e-02 -6.30684942e-02\n",
      " -1.82834212e-02 -1.07069900e-02  1.70871709e-03  3.78909754e-03\n",
      " -1.61964004e-03 -1.76537223e-03 -2.85280845e-03 -1.26361744e-02\n",
      " -2.01951154e-02 -2.73395982e-02 -2.67801397e-02 -2.58168466e-02\n",
      " -3.38660628e-02 -3.11362054e-02 -2.89386790e-02 -3.36524285e-02\n",
      " -2.74909269e-02 -1.16999419e-02  9.68867168e-03  7.70447135e-04\n",
      " -1.94161776e-02 -3.44700664e-02 -5.18301539e-02 -5.00686876e-02\n",
      " -4.78272401e-02 -6.65036142e-02 -6.71087056e-02 -5.72295338e-02\n",
      " -2.28939746e-02 -1.68583281e-02  5.23347734e-03  2.59393523e-03\n",
      "  2.57336278e-03  1.18431710e-02  1.36309734e-03 -1.37715619e-02\n",
      " -2.23831143e-02 -2.67400593e-02 -2.64785234e-02 -3.67550515e-02\n",
      " -3.17441262e-02 -3.12302057e-02 -3.31516378e-02 -2.75896564e-02\n",
      " -2.76849773e-02 -4.89396881e-03  9.78606101e-03  1.17834145e-02\n",
      " -7.36123370e-03 -2.17370018e-02 -3.99893746e-02 -4.20197211e-02\n",
      " -4.61675934e-02 -6.36593103e-02 -6.89197108e-02 -6.69841617e-02\n",
      " -3.96371000e-02 -2.54006907e-02  1.53510179e-03  6.16280921e-03\n",
      "  8.98674596e-03  9.45813116e-03  9.16368980e-03 -7.79925520e-03\n",
      " -2.43974030e-02 -3.74058709e-02 -3.22005562e-02 -2.90109664e-02\n",
      " -2.99134813e-02 -3.19970921e-02 -3.39252204e-02 -2.88091041e-02\n",
      " -2.49236934e-02 -2.70722172e-04  1.35521609e-02  2.01701410e-02\n",
      "  1.64384511e-03 -1.58389900e-02 -1.86259933e-02 -2.21608263e-02\n",
      " -4.27322835e-02 -6.51104674e-02 -7.44784772e-02 -6.28930330e-02\n",
      " -4.80543785e-02 -2.48675812e-02 -3.68495565e-03  6.49081264e-03\n",
      "  2.12391987e-02  3.09464466e-02  8.95944238e-03 -1.12058464e-02\n",
      " -2.16883011e-02 -3.08813658e-02 -3.03087980e-02 -3.26098725e-02\n",
      " -2.96867471e-02 -3.11796311e-02 -2.80834306e-02 -3.17114070e-02\n",
      " -2.10942253e-02 -5.59366308e-03  1.58692170e-02  1.62055753e-02\n",
      "  1.16391396e-02 -3.37871164e-03 -2.21927781e-02 -2.06158664e-02\n",
      " -4.17310558e-02 -7.08897784e-02 -6.94181621e-02 -6.75113499e-02\n",
      " -4.81951311e-02 -6.05793530e-03  2.12442465e-02  2.40707621e-02\n",
      "  3.09965257e-02  2.60635074e-02  9.49778780e-03 -1.40242958e-02\n",
      " -2.56471522e-02 -3.11846267e-02 -2.96399221e-02 -3.65775190e-02\n",
      " -3.02526187e-02 -3.16940472e-02 -2.98350267e-02 -2.93242391e-02\n",
      " -3.09573561e-02 -2.10567680e-03  1.00739328e-02  3.05935200e-02\n",
      "  2.43656766e-02  1.58458557e-02  2.17357976e-03 -4.88813641e-03\n",
      " -1.34573523e-02 -4.58965562e-02 -5.91467582e-02 -6.15649261e-02\n",
      " -2.84265373e-02 -7.34613882e-03  2.21088082e-02  2.94688828e-02\n",
      "  3.10005210e-02  2.02160068e-02 -9.64810781e-04 -2.05780957e-02\n",
      " -3.04681323e-02 -3.31347175e-02 -2.50304677e-02 -2.97559518e-02\n",
      " -3.22374962e-02 -2.83722505e-02 -2.85393149e-02 -3.45065780e-02\n",
      " -2.64332928e-02 -8.27905815e-03  7.35822367e-03  2.87182480e-02\n",
      "  2.12180745e-02  2.26063170e-02  1.45322001e-02 -4.32923995e-03\n",
      " -2.86441576e-02 -4.77423780e-02 -5.73370084e-02 -5.42448610e-02\n",
      " -1.39789367e-02  8.81246815e-05  1.59795061e-02  3.36411931e-02\n",
      "  2.27955543e-02  1.31443013e-02 -1.21646803e-02 -1.97148249e-02\n",
      " -2.66555306e-02 -2.90790703e-02 -2.79714763e-02 -3.49355713e-02\n",
      " -2.98748482e-02 -3.31628770e-02 -2.72243321e-02 -2.83902660e-02\n",
      " -3.21557708e-02 -1.48574635e-02 -1.94889505e-03  1.10374605e-02\n",
      "  2.21720356e-02  1.51085565e-02  5.16786380e-03 -3.48347332e-03\n",
      " -3.25268060e-02 -4.87551130e-02 -5.17139919e-02 -5.93543909e-02\n",
      " -4.20170799e-02 -6.15167059e-03  9.16528888e-03  9.53369774e-03\n",
      "  4.77926107e-03 -1.18242372e-02 -2.11090557e-02 -2.76977178e-02\n",
      " -3.12923789e-02 -3.08463946e-02 -2.91343778e-02 -3.15987282e-02\n",
      " -2.89303903e-02 -2.86548752e-02 -2.74002589e-02 -3.14063393e-02\n",
      " -2.70922091e-02 -2.16021482e-02 -1.74814407e-02 -4.89668129e-03\n",
      " -1.86050264e-03 -6.29283627e-03 -1.35726025e-02 -3.55881117e-02\n",
      " -5.81057370e-02 -7.22450987e-02 -7.62233809e-02 -7.00562820e-02\n",
      " -6.28829971e-02 -2.93792840e-02 -1.37611385e-02 -1.20820412e-02\n",
      " -8.63545295e-03 -1.39259761e-02 -2.40377374e-02 -2.92235631e-02\n",
      " -3.09016500e-02 -3.18294242e-02 -2.75583118e-02 -3.11117209e-02\n",
      " -2.86549088e-02 -2.87280753e-02 -3.05497702e-02 -3.13143171e-02\n",
      " -2.82262750e-02 -3.38689126e-02 -3.33253443e-02 -3.46582718e-02\n",
      " -2.94868834e-02 -3.69042754e-02 -6.04608171e-02 -8.56851861e-02\n",
      " -9.99078602e-02 -1.02114350e-01 -1.04021512e-01 -8.26165602e-02\n",
      " -7.08164796e-02 -4.84208614e-02 -3.17459702e-02 -1.69373322e-02\n",
      " -2.28196774e-02 -2.31213868e-02 -2.69324910e-02 -2.28638519e-02\n",
      " -3.43809277e-02 -3.20447050e-02 -2.99917180e-02 -3.07529215e-02\n",
      " -3.42625007e-02 -3.44957858e-02 -3.01130507e-02 -3.15964334e-02\n",
      " -2.94283796e-02 -3.55013311e-02 -3.29124033e-02 -3.73795107e-02\n",
      " -4.76396233e-02 -5.85813932e-02 -7.99997151e-02 -9.77469459e-02\n",
      " -1.11827686e-01 -1.16964951e-01 -1.06724039e-01 -7.94430226e-02\n",
      " -7.04326779e-02 -5.48184812e-02 -4.04298529e-02 -2.98000369e-02\n",
      " -2.87762880e-02 -2.41273511e-02 -2.73425095e-02 -3.12048402e-02\n",
      " -2.77901739e-02 -2.56667230e-02 -2.94316523e-02 -3.01999319e-02\n",
      " -3.39202508e-02 -2.86850203e-02 -3.07753664e-02 -3.14195938e-02\n",
      " -3.01163103e-02 -3.05719376e-02 -3.11529543e-02 -3.45476195e-02\n",
      " -4.64636385e-02 -5.32256216e-02 -7.01523647e-02 -9.19776410e-02\n",
      " -8.60663950e-02 -9.00570676e-02 -8.47450942e-02 -7.36715570e-02\n",
      " -5.93021140e-02 -4.65515964e-02 -3.93124484e-02 -3.11780944e-02\n",
      " -3.44245844e-02 -2.78042797e-02 -2.89262328e-02 -3.19228359e-02\n",
      " -3.38942111e-02 -2.88668126e-02 -3.21080424e-02 -2.94241980e-02\n",
      " -3.05925645e-02 -3.04161236e-02 -2.98691466e-02 -2.70300414e-02\n",
      " -3.41735743e-02 -2.73054130e-02 -3.16267312e-02 -2.98261847e-02\n",
      " -3.86016853e-02 -4.55556028e-02 -4.94657904e-02 -5.90224825e-02\n",
      " -5.81432693e-02 -5.25457971e-02 -5.01789115e-02 -5.15173897e-02\n",
      " -3.90438959e-02 -3.49327587e-02 -3.59858312e-02 -3.08926087e-02\n",
      " -2.65168212e-02 -3.25377248e-02 -2.73341835e-02 -2.87118871e-02\n",
      " -3.51469517e-02 -2.92161331e-02 -3.30051109e-02 -2.52099317e-02\n",
      " -2.94085499e-02 -3.54640186e-02 -2.95219012e-02 -2.79651359e-02\n",
      " -3.14051136e-02 -3.23978402e-02 -3.32785398e-02 -3.10722310e-02\n",
      " -2.85598151e-02 -2.63893660e-02 -3.29682641e-02 -3.12378332e-02\n",
      " -3.30273621e-02 -2.98780855e-02 -3.44907306e-02 -3.28321755e-02\n",
      " -3.41625251e-02 -2.79587936e-02 -2.51560770e-02 -3.32410187e-02\n",
      " -2.93414220e-02 -2.51034740e-02 -2.66253911e-02 -2.90489215e-02\n",
      " -3.20641622e-02 -3.08825113e-02 -3.04231476e-02 -3.32180634e-02]\n"
     ]
    }
   ],
   "source": [
    "weights = autoencoder.get_weights()\n",
    "for idx, weight in enumerate(weights):\n",
    "    print(\"LAYER {}\".format(idx))\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VBVj0pLwiyD0",
    "outputId": "1ece8e38-c85d-4e98-8351-b810aab61f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 4,522\n",
      "Trainable params: 4,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.8474 - acc: 0.7455 - val_loss: 0.4517 - val_acc: 0.8600\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.4135 - acc: 0.8734 - val_loss: 0.3850 - val_acc: 0.8764\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.3598 - acc: 0.8884 - val_loss: 0.3315 - val_acc: 0.8977\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.3317 - acc: 0.8975 - val_loss: 0.3117 - val_acc: 0.9036\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.3133 - acc: 0.9034 - val_loss: 0.2946 - val_acc: 0.9100\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2990 - acc: 0.9080 - val_loss: 0.2862 - val_acc: 0.9095\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.9118 - val_loss: 0.2761 - val_acc: 0.9152\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2765 - acc: 0.9141 - val_loss: 0.2651 - val_acc: 0.9188\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2674 - acc: 0.9179 - val_loss: 0.2580 - val_acc: 0.9202\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2591 - acc: 0.9204 - val_loss: 0.2467 - val_acc: 0.9250\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2509 - acc: 0.9236 - val_loss: 0.2432 - val_acc: 0.9257\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2442 - acc: 0.9249 - val_loss: 0.2363 - val_acc: 0.9281\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.2380 - acc: 0.9278 - val_loss: 0.2306 - val_acc: 0.9306\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2316 - acc: 0.9290 - val_loss: 0.2237 - val_acc: 0.9312\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.2262 - acc: 0.9313 - val_loss: 0.2235 - val_acc: 0.9328\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2213 - acc: 0.9324 - val_loss: 0.2252 - val_acc: 0.9302\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2171 - acc: 0.9338 - val_loss: 0.2127 - val_acc: 0.9360\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2125 - acc: 0.9352 - val_loss: 0.2133 - val_acc: 0.9347\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2086 - acc: 0.9363 - val_loss: 0.2056 - val_acc: 0.9362\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.2049 - acc: 0.9373 - val_loss: 0.2017 - val_acc: 0.9374\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2011 - acc: 0.9389 - val_loss: 0.1962 - val_acc: 0.9416\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1977 - acc: 0.9401 - val_loss: 0.1994 - val_acc: 0.9376\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1942 - acc: 0.9410 - val_loss: 0.1914 - val_acc: 0.9430\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1913 - acc: 0.9413 - val_loss: 0.1874 - val_acc: 0.9425\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1884 - acc: 0.9427 - val_loss: 0.2014 - val_acc: 0.9371\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1854 - acc: 0.9432 - val_loss: 0.1825 - val_acc: 0.9441\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1826 - acc: 0.9440 - val_loss: 0.1921 - val_acc: 0.9412\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1806 - acc: 0.9450 - val_loss: 0.1783 - val_acc: 0.9469\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1780 - acc: 0.9456 - val_loss: 0.1889 - val_acc: 0.9434\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1762 - acc: 0.9460 - val_loss: 0.1724 - val_acc: 0.9475\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1733 - acc: 0.9473 - val_loss: 0.1799 - val_acc: 0.9440\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1713 - acc: 0.9472 - val_loss: 0.1719 - val_acc: 0.9457\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1690 - acc: 0.9481 - val_loss: 0.1763 - val_acc: 0.9482\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1669 - acc: 0.9487 - val_loss: 0.1672 - val_acc: 0.9501\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1655 - acc: 0.9495 - val_loss: 0.1676 - val_acc: 0.9491\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1633 - acc: 0.9500 - val_loss: 0.1674 - val_acc: 0.9485\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1612 - acc: 0.9508 - val_loss: 0.1627 - val_acc: 0.9500\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1594 - acc: 0.9511 - val_loss: 0.1685 - val_acc: 0.9482\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1579 - acc: 0.9515 - val_loss: 0.1571 - val_acc: 0.9513\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1559 - acc: 0.9519 - val_loss: 0.1682 - val_acc: 0.9488\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1548 - acc: 0.9520 - val_loss: 0.1538 - val_acc: 0.9534\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1530 - acc: 0.9524 - val_loss: 0.1554 - val_acc: 0.9511\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1514 - acc: 0.9532 - val_loss: 0.1587 - val_acc: 0.9517\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1507 - acc: 0.9532 - val_loss: 0.1605 - val_acc: 0.9502\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1486 - acc: 0.9540 - val_loss: 0.1512 - val_acc: 0.9533\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1477 - acc: 0.9546 - val_loss: 0.1497 - val_acc: 0.9550\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1459 - acc: 0.9547 - val_loss: 0.1557 - val_acc: 0.9542\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1449 - acc: 0.9551 - val_loss: 0.1512 - val_acc: 0.9520\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1431 - acc: 0.9561 - val_loss: 0.1442 - val_acc: 0.9565\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1422 - acc: 0.9561 - val_loss: 0.1482 - val_acc: 0.9549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4dff5a2ef0>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "num_classes = 10\n",
    "img_size = len(x_train[0])\n",
    "encoding_dim = len(x_train_encoded[0])\n",
    "\n",
    "mnist_encoded_classifier = Sequential()\n",
    "mnist_encoded_classifier.add(Dense(units=64, activation='selu', input_shape=(encoding_dim,)))\n",
    "mnist_encoded_classifier.add(Dense(units=32, activation='selu', input_shape=(encoding_dim,)))\n",
    "mnist_encoded_classifier.add(Dense(units=num_classes, activation='softmax'))\n",
    "mnist_encoded_classifier.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "mnist_encoded_classifier.summary()\n",
    "\n",
    "mnist_encoded_classifier.fit(x_train_encoded, y_train,\n",
    "                            epochs=50,\n",
    "                            batch_size=256,\n",
    "                            validation_data=(x_test_encoded, y_test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mnist_autoencoder",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
